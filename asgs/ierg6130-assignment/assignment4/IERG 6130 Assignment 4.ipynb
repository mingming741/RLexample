{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IERG 6130 Assignment 4: Reinforcement Learning Pipeline in Practice\n",
    "\n",
    "*2019-2020 2nd term, IERG 6130: Reinforcement Learning and Beyond. Department of Information Engineering, The Chinese University of Hong Kong. Course Instructor: Professor ZHOU Bolei. Assignment author: PENG Zhenghao.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You do not need to submit this file.*\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to assignment 4 of our RL course! \n",
    "\n",
    "Please refer to the `README.md` file for logistics of this assignment. We discuss the files you need to finish, deliverable and credits policy of this assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Get familiar with the environment\n",
    "\n",
    "The environment we used in this assignment is developed by *Billy Ho* (1155079385 AT link.cuhk.edu.hk) and *Joe Lam* (joelam1227 AT gmail.com), undergraduate students at CUHK, as their Final Year Project (FYP) with Prof. Zhou. The system is still being developed, and we only take part of the functionalities and aggregate them into this assignment. We appreciate Billy and Joe for sharing this excellent platform for this course!\n",
    "\n",
    "The RL platform contains environments similar to Pong that we used in previous assignments. However, not like the Pong environment, which provides a rule-based AI as the opponent, this platform allows the user to choose different opponents with varied capability in playing Pong and also allows the user to evaluate the performance of two agents. \n",
    "\n",
    "This provides an excellent open-ended competitive platform for this course! After implementing the algorithms in this assignment, you will have the opportunity to train your own stronger agent using WHATEVER algorithms and tricks you might have. \n",
    "\n",
    "*You will submit the agent with trained weight to us, and we will evaluate them pairwisely. Your agent will be ranked, and you will see how well it performs against other agents. So it is like a tournament! We will have our champion :) See section 4 for more information!*\n",
    "\n",
    "\n",
    "\n",
    "### Section 1.1: Build and run the environment\n",
    "\n",
    "We provide a helper function to get the environment:\n",
    "\n",
    "```\n",
    "from make_envs import make_envs\n",
    "envs = make_envs(num_envs=5)\n",
    "```\n",
    "\n",
    "The `envs` is a vectorized environment with batch size 5. Following the codes below to get familiar with it and discover what \"vectorized environment\" means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "from core.utils.notebook_utils import *\n",
    "import cv2\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "from competitive_pong import make_envs\n",
    "\n",
    "envs = make_envs(\n",
    "    env_id=\"CompetitivePong-v0\", \n",
    "    seed=0,\n",
    "    log_dir=\"demo\",  # this will create a \"demo\" directory\n",
    "    num_envs=5,\n",
    "    asynchronous=False,\n",
    "    resized_dim=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "observations = envs.reset()\n",
    "\n",
    "print(\"The observations shape is: {}.\\nMeaning: batch size {}, \"\n",
    "      \"number of channels {}, width {}, height {}\".format(\n",
    "      observations.shape, *observations.shape\n",
    "))\n",
    "\n",
    "plt.imshow(envs.envs[0].render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "print(\"This is the processed observation that are then fed to the neural network.\")\n",
    "print(\"We will discuss this later.\")\n",
    "plt.imshow(observations[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "print(\"The single environment has action space: {}.\\nYou are controlling the left pad.\"\n",
    "      \"action=0 means moving the pad down, action=1 means doing nothing, action=2 \"\n",
    "      \"means moving the pad up.\".format(envs.action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "print(\"Note that the vectorized environment take a list of interger as input.\")\n",
    "obs, reward, done, info = envs.step([0, 1, 0, 1, 0])\n",
    "print(\"\\n Observation Shape: \", obs.shape)\n",
    "print(\"\\n Reward: \", reward)\n",
    "print(\"\\n Done: \", done)\n",
    "print(\"\\n Info: \", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the original CompetitivePong-v0 may have a different observation structure. We wrap a series of wrappers on the raw environment to make the training easier. We conclude those measures here:\n",
    "\n",
    "\n",
    "1. We repeat an action for four times. During these four steps, we merge the raw four frames (210, 160, 3)\\*4 into one frame (210, 160, 3)\n",
    "2. Then, we merge three channels to one and resize the frame (210, 160, 3) to a small frame (42, 42, 1)\n",
    "3. Then, we change the order of dimension (42, 42, 1) to fit the frame into pytorch (1, 42, 42)\n",
    "\n",
    "Besides, we do another stacking when running the environment. We stack the latest 4 processed frames and make it looks like a frame with 4 channel, i.e. from (1, 42, 42)\\*4 to (4, 42, 42). By doing this, in reality, the neural network can access the latest 16 raw frames information.\n",
    "\n",
    "The following cells generate animation for you to have the first-person view as your agent does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "envs = make_envs(\"CompetitivePong-v0\", num_envs=1, resized_dim=42)\n",
    "frames = []\n",
    "observations = []\n",
    "obs = envs.reset()\n",
    "print(\"The observation shape returned by the vectorized environment is: \"\n",
    "      \"{}\".format(obs.shape))\n",
    "while True:\n",
    "    action = [envs.action_space.sample()]\n",
    "    obs, _, done, _ = envs.step(action)\n",
    "    frame = envs.envs[0].render(\"rgb_array\")\n",
    "    observations.append(obs)\n",
    "    frames.append(frame)\n",
    "    if done:\n",
    "        break\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "print(\"Raw observation from the environment \"\n",
    "      \"(observation is identical to the return of env.render, so we show you \"\n",
    "      \"the latter.)\")\n",
    "animate(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "print(\"Processed observation that are fed to the neural network.\"\n",
    "      \" (We only resize it to see more clear in this cell.)\")\n",
    "obs = [\n",
    "    cv2.resize(o.reshape((42, 42)), (42 * 8, 42 * 8), interpolation=cv2.INTER_NEAREST)\n",
    "    for o in observations\n",
    "]\n",
    "animate(obs)\n",
    "\n",
    "# Is this resolution enough? The answer is yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2: Two agents to play against each other\n",
    "\n",
    "In the platform, we provide another environment called `CompetitivePongDouble-v0`, whose observation space and action space are both a tuple of size 2. This environment allows us to run two agents and beat against each other easily.\n",
    "\n",
    "An important contribution of this environment is that this environment converts observations of both agents into the same form --- the agent controls the left pad. The environment mirrors the observation of the right-side agent in order to run an agent that train to play in the left-side.\n",
    "\n",
    "In the next few cells, we take a look at what the observations look like. Then we introduce more high-level tools to evaluate your agents, with different level experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "env = gym.make(\"CompetitivePongDouble-v0\")  # , num_envs=1, resized_dim=42)\n",
    "frames = []\n",
    "observations = []\n",
    "obs = env.reset()\n",
    "print(\"The observation shape returned by the vectorized environment is: \"\n",
    "      \"{}\".format(obs[0].shape))\n",
    "while True:\n",
    "    action = [0, 2]  # We let the first agent moving up, and second agent does nothing.\n",
    "    obs, _, done, _ = env.step(action)\n",
    "    frame = env.render(\"rgb_array\")\n",
    "    observations.append(obs)\n",
    "    frames.append(frame)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "print(\"This is what happening in environment.\")\n",
    "animate(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is what the first agent (left agent) raw observation is.\")\n",
    "obs1 = [o for o, _ in observations]\n",
    "animate(obs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is what the second agent (right agent) raw observation is.\")\n",
    "obs2 = [o for _, o in observations]\n",
    "animate(obs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: A2C and PPO Trainer\n",
    "\n",
    "### Section 2.1: Finish the base trainer\n",
    "\n",
    "You should finish `evaluate_actions` and `compute_action` in `core/base_trainer.py`.\n",
    "\n",
    "### Section 2.2: Finish A2C trainer\n",
    "\n",
    "You need to implement `update` and `compute_loss` function in `core/a2c_trainer.py`.\n",
    "\n",
    "To compute the expected return which is then used to calculate the advantages, you need to implement `A2CRolloutStorage` in `core/buffer.py`\n",
    "\n",
    "### Section 2.3: Finish PPO trainer\n",
    "\n",
    "You need to implment `update` and `compute_loss` function in `core/ppo_trainer.py`.\n",
    "\n",
    "To compute the expected return which is then used to compute the advantages, you need to implement `PPORolloutStorage` in `core/buffer.py`\n",
    "\n",
    "Here we use GAE [1] (Generalized Advantage Estimation) to estimate the advantage. This technique is implemented in `PPORolloutStorage.compute_returns`\n",
    "\n",
    "Concretely, we estimate the advantage of current state $t$ via:\n",
    "\n",
    "$$\\delta_t = r_t + \\gamma V(s_t+1) - V(s_t)$$\n",
    "\n",
    "$$A^{GAE}_t = \\sum_{l=0}^{\\infty} (\\gamma \\lambda) ^ l \\delta_{t+l}$$\n",
    "\n",
    "wherein $V$ is the state value function, $\\lambda$ is a hyper-parameter defined in `core.ppo_trainer.ppo_config[\"gae_lambda\"]`.\n",
    "\n",
    "You can find that when $\\lambda = 0$, the GAE estimates advantage in a one-step TD manner. When $\\lambda = 1$, the GAE reduces to Monte Carlo Estimation of the state value. Therefore by varying the $\\lambda$, we can make the trade-off between the accuracy of the estimation (bias) and the variance of advantage.\n",
    "\n",
    "[1]: Schulman, John , et al. \"High-Dimensional Continuous Control Using Generalized Advantage Estimation.\" (2015).\n",
    "\n",
    "### Section 2.4: Test algorithms and train your agent\n",
    "\n",
    "After implementing your algorithms, you can test both A2C and PPO algorithm using the same scripts:\n",
    "\n",
    "```bash\n",
    "# Blackbox test to make sure your A2C and PPO can run both in CartPole and Pong, with single or multiple environments.\n",
    "python blackbox_tests.py\n",
    "\n",
    "python train.py --algo PPO --env-id CartPole-v0  # Test your codes\n",
    "\n",
    "python train.py --algo PPO --log-dir data/YOUR-LOG-DIR  # A formal training of PPO\n",
    "\n",
    "python train.py --algo A2C --env-id CartPole-v0  # Test your codes\n",
    "\n",
    "python train.py --algo A2C --log-dir data/YOUR-LOG-DIR  # A formal training of A2C\n",
    "```\n",
    "\n",
    "Notes: In our testing, PPO can solve CartPole-v0 in 200 iterations, A2C in 500 iterations.\n",
    "\n",
    "### Section 2.5: Policy Serving API\n",
    "\n",
    "You need to finish the file `this_is_my_agent.py`. Run the next cell to verify your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from this_is_my_agent import test, student_compute_action_function\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Play with builtin agents\n",
    "\n",
    "We provide a group of trained agents to serve as opponents. We prepared 6 level builtin agents, including:\n",
    "\n",
    "1. `RANDOM` - A random agent\n",
    "2. `RULE_BASED` - Rule-based agent that serves as the right-side agent in `CompetitivePong-v0`\n",
    "3. `WEAK` - Weak-level agent with a smaller neural network than yours\n",
    "4. `MEDIUM` - Medium-level agent with a small neural network\n",
    "5. `STRONG` - Strong-level agent with a large neural network as yours\n",
    "6. `ALPHA_PONG` - Boss-level agent that is well trained and may hard to beat down, use a large neural network as yours\n",
    "\n",
    "### Section 3.1: Visualize games between builtin agents and yours\n",
    "\n",
    "You can run `this_is_my_agent.py` to visualize the game between those builtin agents and your agent. See the following cells.\n",
    "\n",
    "Note: You can run the next cell even if you don't implement API, in which case the left agent would be random. You can change the flags in next cells to watch the game between different builtin agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run this_is_my_agent.py --left MY_AGENT --right ALPHA_PONG --num-episodes 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run this_is_my_agent.py --left STRONG --right MEDIUM --num-episodes 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.2: Train your agent using builtin agents (Optional)\n",
    "\n",
    "In the previous section, we train agents using A2C or PPO algorithm on the `CompetitivePong-v0` environment, which means the opponent is a rule-based agent. \n",
    "\n",
    "We can train an agent competing with a set of diverse agents.\n",
    "\n",
    "To do this, simply set the environment name to `CompetitivePongTournament-v0` and then run the same script above. For example, you can train the agent via:\n",
    "\n",
    "```bash\n",
    "python train.py --log-dir data/LOG-DIR-NAME --env-id CompetitivePongTournament-v0\n",
    "```\n",
    "\n",
    "Notes:\n",
    "\n",
    "1. We do not implement the restore mechanism. So if you wish to finetune a trained agent with this competitive environment, please implement this by yourself.\n",
    "2. We implement a function call `reset_opponent` for this tournament environemnt to allow user to reset the opponent if necessary. Please refer to `competitive_pong/competitive_pong_env.py` for more detail.\n",
    "3. Currently, we randomly choose an opponent in all builtin agents and fix it for one training iteration, via calling `reset_opponent` in `train.py` (see codebolck below). In future, we can investigate the opponent selection strategy.\n",
    "\n",
    "```python\n",
    "# ===== Reset opponent if in tournament mode =====\n",
    "if tournament and iteration % config.num_steps == 0:\n",
    "    # Randomly choose one agent in each iteration\n",
    "    envs.reset_opponent()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Evaluate your agent\n",
    "\n",
    "\n",
    "The next cells run `evaluate.py`, the entrance to evaluate your agent. You need to finish the file `this_is_my_agent.py` first before running this file. This file would load your agent via the API in `this_is_my_agent.py` and evaluates your agent via launching games against other builtin agents.\n",
    "\n",
    "But now, just the run the next cell and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run evaluate.py --num-episodes 2 --num-envs 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can run this file without bugs, then you are well prepared to get a formal record of your agent. Now run\n",
    "\n",
    "`python evaluate.py`\n",
    "\n",
    "to test your agent against builtin agents for 100 episodes each and include the generated `data/evaluate_result.csv` into your submission.\n",
    "\n",
    "\n",
    "### Section 4.1 (Optional): What we will do after receiving your submission\n",
    "\n",
    "After the deadline, we will collect all submissions and hold a tournament on all students' agents.\n",
    "\n",
    "First, we merge all students' submitted checkpoints into \"data\" directory. (So you need to make sure your submitted checkpoint, i.e. pkl file, have a unique name.)\n",
    "\n",
    "Then, we rename the function `student_compute_action_function` in `this_is_my_agent.py` to `student_YOUR-STUDENT-ID`. But we do not change the content in your function. So you need to make sure we can get your agent bug-free by directly call the function in `this_is_my_agent.py` when the working directory is `assignment4/`.\n",
    "\n",
    "Third, we run this file `this_is_what_we_will_do.py`, which automatically gathers all functions in \"this_is_my_agent.py\" and launch a series of matches.\n",
    "\n",
    "Finally, we summarize the match results of all your agents against others (including builtin agents) to a winning-rate matrix.\n",
    "\n",
    "In conclusion, what you need to do for joining this tournament are:\n",
    "\n",
    "1. Finish `this_is_my_agent.py` (do not need to change the function name)\n",
    "2. Provide checkpoint file to us (it should have a unique name)\n",
    "3. Check everything work wells (i.e., run the next cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run this_is_what_we_will_do.py --num-episodes 1 --num-envs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Conclusion and Discussion\n",
    "\n",
    "In this assignment, we implement an RL pipeline that can be used in research.\n",
    "\n",
    "Please follow the submission instruction in the assignment to submit your assignment to our staff. As a reminder, you don't need to submit this jupyter notebook to us.\n",
    "\n",
    "This is the last assignment of this course. We appreciate you for joining this course and pay efforts to finish assignments. Hope this course can provide insights to you! Thanks!\n",
    "\n",
    "------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
