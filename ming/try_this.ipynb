{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from random import randint, choice\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAVEnvironment():\n",
    "    \"\"\"\n",
    "    Game environment for UAV test\n",
    "    \n",
    "    ---Map---\n",
    "    \n",
    "    y-axis(length)\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "     _______________________ x-axis(width)\n",
    "     \n",
    "    Hight is a fixed value\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        # Game config\n",
    "        self.action_space = (0, 1, 2, 3) # up, right, down, left, total 4 actions\n",
    "        self.total_steps = config[\"total_steps\"] # when the game end\n",
    "        self.current_step = 0\n",
    "        if config[\"is_random_env\"] == False:\n",
    "            self.random_seed = config[\"random_seed\"]\n",
    "            random.seed(self.random_seed)\n",
    "        \n",
    "        # Map config\n",
    "        self.map = dict(width=config[\"map\"][\"width\"], length=config[\"map\"][\"length\"], height=config[\"map\"][\"height\"])\n",
    "        self.UAV_speed = config[\"UAV_speed\"]\n",
    "        self.UAV_initial_pos = config[\"UAV_initial_pos\"] \n",
    "        self.inital_state=config[\"initial_state\"]\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        self.number_of_user = config[\"number_of_user\"]\n",
    "        self.users_pos = list()\n",
    "        self.UAV_path = [] # record the path of UAV\n",
    "        for i in range(0, self.number_of_user):\n",
    "            self.users_pos.append((randint(0, self.map[\"width\"]), randint(0, self.map[\"length\"])))\n",
    "        \n",
    "        # Wireless config\n",
    "        self.g0 = config[\"wireless_parameter\"][\"g0\"]\n",
    "        self.B = config[\"wireless_parameter\"][\"B\"]\n",
    "        self.Pk = config[\"wireless_parameter\"][\"Pk\"]\n",
    "        self.noise = config[\"wireless_parameter\"][\"noise\"]\n",
    "        \n",
    "    def get_reward(self,prev_pos, UAV_pos):\n",
    "        # One step Reward is define as the summation of all user's utility\n",
    "        \n",
    "        \n",
    "        #prev_reward =0\n",
    "        #for user_index in range(0, self.number_of_user):\n",
    "        #    gkm = self.g0 / (self.map[\"height\"] ** 2 + (prev_pos[0] - self.users_pos[user_index][0]) ** 2 + (prev_pos[1] - self.users_pos[user_index][1]) ** 2)\n",
    "        #    user_utility = self.B * math.log(1 + self.Pk * gkm / self.noise, 2)\n",
    "        #    prev_reward = prev_reward + user_utility\n",
    "        reward = 0\n",
    "        for user_index in range(0, self.number_of_user):\n",
    "            gkm = self.g0 / (self.map[\"height\"] ** 2 + (UAV_pos[0] - self.users_pos[user_index][0]) ** 2 + (UAV_pos[1] - self.users_pos[user_index][1]) ** 2)\n",
    "            user_utility = (self.B/self.number_of_user)* math.log(1 + self.Pk * gkm / self.noise, 2)\n",
    "            reward = reward + user_utility\n",
    "        return (reward) / (10 ** 6) # Use Mkbps as signal basic unit\n",
    "    \n",
    " \n",
    "    def transition_dynamics(self, action, speed, state):\n",
    "        # given the action (direction), calculate the next state (UAV current position)\n",
    "        assert action in self.action_space\n",
    "        next_UAV_pos = list(state)\n",
    "        if action == 0:\n",
    "            # move up\n",
    "            next_UAV_pos[1] = min(next_UAV_pos[1] + speed, self.map[\"length\"])\n",
    "        if action == 1:\n",
    "            # move right\n",
    "            next_UAV_pos[0] = min(next_UAV_pos[0] + speed, self.map[\"width\"])\n",
    "        if action == 2:\n",
    "            # move down\n",
    "            next_UAV_pos[1] = max(next_UAV_pos[1] - speed, 0)\n",
    "        if action == 3:\n",
    "            # move left\n",
    "            next_UAV_pos[0] = max(next_UAV_pos[0] - speed, 0)\n",
    "        return np.array(next_UAV_pos)\n",
    "                    \n",
    "    def step(self, action, speed=-1):\n",
    "        # assume we use the max speed as the default speed, when come near to the opt-position, we can slow down the speed\n",
    "        if speed < 0 or speed >= self.UAV_speed:\n",
    "            speed = self.UAV_speed\n",
    "        \n",
    "        prev_pos=self.UAV_current_pos\n",
    "        #update pos\n",
    "        self.UAV_path.append(prev_pos)\n",
    "        self.UAV_current_pos = self.transition_dynamics(action, speed, self.UAV_current_pos)\n",
    "        self.current_step = self.current_step + 1\n",
    "        done = False\n",
    "        if self.current_step == self.total_steps:\n",
    "            done =  True\n",
    "        state=self.UAV_current_pos/1000\n",
    "        return state, self.get_reward(prev_pos, self.UAV_current_pos), done\n",
    "    \n",
    "    def action_sample(self):\n",
    "        return choice(self.action_space)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        return self.UAV_current_pos\n",
    "        \n",
    "    def print_attribute(self):\n",
    "        attrs = vars(self)\n",
    "        print(', '.join(\"%s: %s\" % item for item in attrs.items()))\n",
    "        \n",
    "    def print_locations(self):\n",
    "        print(\"UAV position is: {}\".format(self.UAV_current_pos))\n",
    "        print(\"Users position are: {}\".format(self.users_pos))\n",
    "        \n",
    "    def print_map(self):\n",
    "        x_list = [pos[0] for pos in self.users_pos]\n",
    "        y_list = [pos[1] for pos in self.users_pos]\n",
    "        x_list.append(self.UAV_current_pos[0])\n",
    "        y_list.append(self.UAV_current_pos[1])\n",
    "        for i in range(0, len(self.UAV_path)):\n",
    "            x_list.append(self.UAV_path[i][0])\n",
    "            y_list.append(self.UAV_path[i][1])\n",
    "        \n",
    "        colors = np.array([\"red\", \"green\", \"blue\"])\n",
    "        sizes = []\n",
    "        colors_map = []\n",
    "        for i in range(0, self.number_of_user):\n",
    "            sizes.append(25)\n",
    "            colors_map.append(1)\n",
    "        sizes.append(50)\n",
    "        colors_map.append(0)\n",
    "        for i in range(0, len(self.UAV_path)):\n",
    "            sizes.append(10)\n",
    "            colors_map.append(2)\n",
    "        for i in range(0, len(self.UAV_path) - 1):\n",
    "            x_values = [self.UAV_path[i][0], self.UAV_path[i+1][0]]\n",
    "            y_values = [self.UAV_path[i][1], self.UAV_path[i+1][1]]\n",
    "            plt.plot(x_values, y_values, 'b')\n",
    "        plt.scatter(x_list, y_list, c=colors[colors_map], s=sizes) \n",
    "        plt.axis([0, self.map[\"width\"], 0, self.map[\"length\"]])\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_config = dict(\n",
    "    total_steps = 50,\n",
    "    random_seed = 21,\n",
    "    is_random_env = False,\n",
    "    map=dict(\n",
    "        width=1000,\n",
    "        length=1000,\n",
    "        height=100\n",
    "    ),\n",
    "    number_of_user = 10,\n",
    "    UAV_speed = 50,\n",
    "    UAV_initial_pos = np.array([0, 0]),\n",
    "    initial_state=0,\n",
    "    wireless_parameter = dict(\n",
    "        g0 = 10 ** (-5),\n",
    "        B = 10 ** (6),\n",
    "        Pk = 0.1,\n",
    "        noise = 10 ** (-9)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(policy, num_episodes=1, render=False):\n",
    "    global environment_config\n",
    "    env = UAVEnvironment(environment_config)\n",
    "    rewards = []\n",
    "    if render: num_episodes = 1\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        act = policy(obs)\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            obs, reward, done = env.step(act)\n",
    "            act = policy(obs)\n",
    "            ep_reward += reward\n",
    "            if render:\n",
    "                clear_output(wait=True)\n",
    "                env.print_attribute()\n",
    "                print(\"Current Step: {}\".format(env.current_step))\n",
    "                print(\"Action: {}\".format(act))\n",
    "                print(\"UAV current position x: {}, y: {}\".format(env.UAV_current_pos[0], env.UAV_current_pos[1]))\n",
    "                print(\"Current step reward: {}, episodes rewards: {}\".format(reward, ep_reward))\n",
    "                env.print_map()\n",
    "                wait(sleep=0.2)\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "    return np.mean(rewards)\n",
    "\n",
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    assert inspect.isclass(trainer_cls)\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    trainer = trainer_cls(config)\n",
    "    config = trainer.config\n",
    "    start = now = time.time()\n",
    "    stats = []\n",
    "    for i in range(config['max_iteration'] + 1):\n",
    "        stat = trainer.train()\n",
    "        stats.append(stat or {})\n",
    "        if i % config['evaluate_interval'] == 0 or \\\n",
    "                i == config[\"max_iteration\"]:\n",
    "            reward = trainer.evaluate(config.get(\"evaluate_num_episodes\", 50))\n",
    "            print(\"({:.1f}s,+{:.1f}s)\\tIteration {}, current mean episode \"\n",
    "                  \"reward is {}. {}\".format(\n",
    "                time.time() - start, time.time() - now, i, reward,\n",
    "                {k: round(np.mean(v), 4) for k, v in\n",
    "                 stat.items()} if stat else \"\"))\n",
    "            now = time.time()\n",
    "        if reward_threshold is not None and reward > reward_threshold:\n",
    "            print(\"In {} iteration, current mean episode reward {:.3f} is \"\n",
    "                  \"greater than reward threshold {}. Congratulation! Now we \"\n",
    "                  \"exit the training process.\".format(\n",
    "                i, reward, reward_threshold))\n",
    "            break\n",
    "    return trainer, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config = dict(\n",
    "    max_iteration=1000,\n",
    "    max_episode_length=1000,\n",
    "    evaluate_interval=100,\n",
    "    gamma=0.99,\n",
    "    eps=0.3,\n",
    ")\n",
    "\n",
    "class AbstractTrainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = merge_config(config, default_config)\n",
    "        self.env = UAVEnvironment(config)\n",
    "        self.act_dim=4\n",
    "        self.obs_dim=2 \n",
    "        self.eps = self.config['eps']\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        self.parameters = None\n",
    "        raise NotImplementedError(\n",
    "            \"You need to override the \"\n",
    "            \"Trainer._initialize_parameters() function.\")\n",
    "\n",
    "    def process_state(self, state):\n",
    "        \"\"\"Preprocess the state (observation) if necessary\"\"\"\n",
    "        processed_state = state\n",
    "        return processed_state\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        raise NotImplementedError(\"You need to override the \"\n",
    "                                  \"Trainer.compute_values() function.\")\n",
    "\n",
    "    def compute_action(self, processed_state, eps=None):\n",
    "        \"\"\"Compute the action given the state. Note that the input\n",
    "        is the processed state.\"\"\"\n",
    "\n",
    "        values = self.compute_values(processed_state)\n",
    "        assert values.ndim == 1, values.shape\n",
    "        if eps is None:\n",
    "            eps = self.eps\n",
    "        if np.random.uniform(0,1)< eps:\n",
    "            action=self.env.action_sample()\n",
    "        else:\n",
    "            action=np.argmax(values)\n",
    "        return action\n",
    "\n",
    "    def evaluate(self, num_episodes=50, *args, **kwargs):\n",
    "        policy = lambda raw_state: self.compute_action(\n",
    "            self.process_state(raw_state), eps=0.0)\n",
    "        result = evaluate(policy, num_episodes, *args, **kwargs)\n",
    "        return result\n",
    "\n",
    "    def compute_gradient(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\n",
    "            \"You need to override the Trainer.compute_gradient() function.\")\n",
    "\n",
    "    def apply_gradient(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\n",
    "            \"You need to override the Trainer.apply_gradient() function.\")\n",
    "\n",
    "    def train(self):\n",
    "        raise NotImplementedError(\"You need to override the \"\n",
    "                                  \"Trainer.train() function.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_trainer_config = merge_config(dict(\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "    hidden_dim=100,\n",
    "    n=3,\n",
    "    clip_norm=1.0,\n",
    "    clip_gradient=True\n",
    "), default_config)\n",
    "\n",
    "\n",
    "class MLPTrainer(AbstractTrainer):\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, mlp_trainer_config)\n",
    "        self.hidden_dim = config[\"hidden_dim\"]\n",
    "        super().__init__(config)\n",
    "        self.max_episode_length = self.config[\"max_episode_length\"]\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.gamma = self.config[\"gamma\"]\n",
    "        self.n = self.config[\"n\"]\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        std = self.config[\"parameter_std\"]\n",
    "        self.hidden_parameters = np.random.normal(0,std,(self.obs_dim, self.hidden_dim))\n",
    "        self.output_parameters = np.random.normal(0,std,(self.hidden_dim, self.act_dim))\n",
    "\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        assert processed_state.ndim == 1, processed_state.shape\n",
    "        activation = self.compute_activation(processed_state)\n",
    "        values = np.dot(self.output_parameters.T,activation)\n",
    "        return values\n",
    "\n",
    "    def compute_activation(self, processed_state):\n",
    "        activation = np.dot(self.hidden_parameters.T,processed_state)\n",
    "        return activation\n",
    "\n",
    "    def compute_gradient(self, processed_states, actions, rewards, tau, T):\n",
    "        n = self.n\n",
    "        G = 0\n",
    "        for i in range(tau+1,min(T,tau+n)+1):\n",
    "            G+=np.power(self.gamma,i-tau-1)*rewards[i]\n",
    "        if tau + n < T:\n",
    "            Q_tau_plus_n = self.compute_values(processed_states[tau+n])\n",
    "            act=actions[tau+n]  \n",
    "            G = G + (self.gamma ** n) * Q_tau_plus_n[act]\n",
    "        cur_state = processed_states[tau]\n",
    "        loss_grad = np.zeros((self.act_dim, 1))  # [act_dim, 1]\n",
    "        q=self.compute_values(cur_state)\n",
    "        act=actions[tau]\n",
    "        loss_grad[act]=-(G-q[act])\n",
    "\n",
    "        activation=self.compute_activation(cur_state)\n",
    "        activation=np.array([activation])\n",
    "        output_gradient = np.dot(activation.T,loss_grad.T)\n",
    "        \n",
    "        \n",
    "        cur_state=np.array([cur_state])\n",
    "        hidden_gradient = cur_state.T@loss_grad.T@self.output_parameters.T\n",
    "  \n",
    "        assert np.all(np.isfinite(output_gradient)), \\\n",
    "            \"Invalid value occurs in output_gradient! {}\".format(\n",
    "                output_gradient)\n",
    "        assert np.all(np.isfinite(hidden_gradient)), \\\n",
    "            \"Invalid value occurs in hidden_gradient! {}\".format(\n",
    "                hidden_gradient)\n",
    "        return [hidden_gradient, output_gradient]\n",
    "\n",
    "    def apply_gradient(self, gradients):\n",
    "        \"\"\"Apply the gradientss to the two layers' parameters.\"\"\"\n",
    "        assert len(gradients) == 2\n",
    "        hidden_gradient, output_gradient = gradients\n",
    "\n",
    "        assert output_gradient.shape == (self.hidden_dim, self.act_dim)\n",
    "        assert hidden_gradient.shape == (self.obs_dim, self.hidden_dim)\n",
    "        \n",
    "        if self.config[\"clip_gradient\"]:\n",
    "            clip_norm = self.config[\"clip_norm\"]\n",
    "            output_gradient=output_gradient*clip_norm/max(clip_norm, np.linalg.norm(output_gradient))\n",
    "            hidden_gradient=hidden_gradient*clip_norm/max(clip_norm, np.linalg.norm(hidden_gradient))\n",
    "\n",
    "        self.output_parameters-= self.learning_rate*output_gradient\n",
    "        self.hidden_parameters-= self.learning_rate*hidden_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ExperienceReplayMemory:\n",
    "    \"\"\"Store and sample the transitions\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class PytorchModel(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(PytorchModel, self).__init__()\n",
    "        self.action_value = torch.nn.Sequential(\n",
    "            torch.nn.Linear(obs_dim,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,act_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.action_value(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_config = merge_config(dict(\n",
    "    memory_size=50000,\n",
    "    learn_start=5000,\n",
    "    batch_size=32,\n",
    "    target_update_freq=500,  # in steps\n",
    "    learn_freq=1,  # in steps\n",
    "    n=1\n",
    "), mlp_trainer_config)\n",
    "\n",
    "\n",
    "def to_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.float32)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    if x.dim() == 3 or x.dim() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    assert x.dim() == 2 or x.dim() == 4, x.shape\n",
    "    return x\n",
    "\n",
    "def to_long_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.long)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    if x.dim() == 3 or x.dim() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    assert x.dim() == 2 or x.dim() == 4, x.shape\n",
    "    return x\n",
    "\n",
    "\n",
    "class DQNTrainer(MLPTrainer):\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, pytorch_config)\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.memory = ExperienceReplayMemory(config[\"memory_size\"])\n",
    "        self.learn_start = config[\"learn_start\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.target_update_freq = config[\"target_update_freq\"]\n",
    "        self.clip_norm = config[\"clip_norm\"]\n",
    "        self.step_since_update = 0\n",
    "        self.total_step = 0\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"Initialize the pytorch model as the Q network and the target network\"\"\"\n",
    "        self.network = PytorchModel(self.obs_dim,self.act_dim)\n",
    "        self.network.eval()\n",
    "        self.network.share_memory()\n",
    "\n",
    "        self.target_network = PytorchModel(self.obs_dim,self.act_dim)\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.target_network.eval()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.network.parameters(), lr=self.learning_rate\n",
    "        )\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        values=self.network(processed_state)\n",
    "        return values.data.numpy()\n",
    "\n",
    "    def train(self):\n",
    "        s = self.env.reset()\n",
    "        processed_s = self.process_state(s)\n",
    "        act = self.compute_action(processed_s)\n",
    "        stat = {\"loss\": []}\n",
    "\n",
    "        for t in range(self.max_episode_length):\n",
    "            next_state, reward, done = self.env.step(act)\n",
    "            next_processed_s = self.process_state(next_state)\n",
    "\n",
    "            # Push the transition into memory.\n",
    "            self.memory.push(\n",
    "                (processed_s, act, reward, next_processed_s, done)\n",
    "            )\n",
    "\n",
    "            processed_s = next_processed_s\n",
    "            act = self.compute_action(next_processed_s)\n",
    "            self.step_since_update += 1\n",
    "            self.total_step += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            if t % self.config[\"learn_freq\"] != 0:\n",
    "                # It's not necessary to update in each step.\n",
    "                continue\n",
    "\n",
    "            if len(self.memory) < self.learn_start:\n",
    "                continue\n",
    "            elif len(self.memory) == self.learn_start:\n",
    "                print(\"Current memory contains {} transitions, \"\n",
    "                      \"start learning!\".format(self.learn_start))\n",
    "\n",
    "            batch = self.memory.sample(self.batch_size)\n",
    "\n",
    "            # Transform a batch of state / action / .. into a tensor.\n",
    "            state_batch = to_tensor(\n",
    "                np.stack([transition[0] for transition in batch])\n",
    "            )\n",
    "            action_batch = to_long_tensor(\n",
    "                np.stack([transition[1] for transition in batch])\n",
    "            )\n",
    "            reward_batch = to_tensor(\n",
    "                np.stack([transition[2] for transition in batch])\n",
    "            )\n",
    "            next_state_batch = torch.stack(\n",
    "                [transition[3] for transition in batch]\n",
    "            )\n",
    "            done_batch = to_tensor(\n",
    "                np.stack([transition[4] for transition in batch])\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_state_values=self.target_network(next_state_batch)\n",
    "                Q_t_plus_one = next_state_values.max(1)[0].detach()             \n",
    "                assert isinstance(Q_t_plus_one, torch.Tensor)\n",
    "                assert Q_t_plus_one.dim() == 1\n",
    "                \n",
    "                Q_target = reward_batch+(1-done_batch)*Q_t_plus_one\n",
    "            self.network.train()\n",
    "            \n",
    "            state_action_values=self.network(state_batch)\n",
    "            Q_t = torch.t(state_action_values).gather(0,action_batch)\n",
    "    \n",
    "            assert Q_t.shape == Q_target.shape\n",
    "\n",
    "            # Update the network\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss(input=Q_t, target=Q_target)\n",
    "            loss_value = loss.item()\n",
    "            stat['loss'].append(loss_value)\n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(self.network.parameters(), self.clip_norm)   \n",
    "            self.optimizer.step()\n",
    "            self.network.eval()\n",
    "\n",
    "        if len(self.memory) >= self.learn_start and self.step_since_update > self.target_update_freq:\n",
    "            self.step_since_update = 0\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "            self.target_network.eval()\n",
    "        return {\"loss\": np.mean(stat[\"loss\"]), \"episode_len\": t}\n",
    "\n",
    "    def process_state(self, state):\n",
    "        return torch.from_numpy(state).type(torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5s,+0.5s)\tIteration 0, current mean episode reward is 0.2543874541249863. {'loss': nan, 'episode_len': 49.0}\n",
      "(13.9s,+13.4s)\tIteration 100, current mean episode reward is 0.857794874672707. {'loss': 0.0007, 'episode_len': 49.0}\n",
      "(30.1s,+16.2s)\tIteration 200, current mean episode reward is 0.5905150618304572. {'loss': 0.0013, 'episode_len': 49.0}\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "config=merge_config(environment_config, dict(\n",
    "    max_iteration=200,\n",
    "    evaluate_interval=100, \n",
    "    learning_rate=0.0001,\n",
    "    clip_norm=10.0,\n",
    "    memory_size=50000,\n",
    "    learn_start=1000,\n",
    "    eps=0.03,\n",
    "    target_update_freq=1000,\n",
    "    batch_size=32,\n",
    "))\n",
    "pytorch_trainer, pytorch_stat = run(DQNTrainer, config,reward_threshold=5) #,reward_threshold=2\n",
    "\n",
    "reward = pytorch_trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space: (0, 1, 2, 3), total_steps: 50, current_step: 50, random_seed: 21, map: {'width': 1000, 'length': 1000, 'height': 100}, UAV_speed: 50, UAV_initial_pos: [0 0], inital_state: 0, UAV_current_pos: [1000  400], number_of_user: 10, users_pos: [(168, 428), (706, 428), (650, 288), (490, 863), (221, 811), (486, 827), (986, 524), (187, 517), (540, 241), (807, 3)], UAV_path: [array([0, 0]), array([ 0, 50]), array([  0, 100]), array([ 50, 100]), array([100, 100]), array([150, 100]), array([150, 150]), array([200, 150]), array([200, 200]), array([200, 250]), array([200, 300]), array([200, 350]), array([200, 400]), array([250, 400]), array([300, 400]), array([350, 400]), array([400, 400]), array([450, 400]), array([500, 400]), array([550, 400]), array([600, 400]), array([650, 400]), array([700, 400]), array([750, 400]), array([800, 400]), array([850, 400]), array([900, 400]), array([950, 400]), array([1000,  400]), array([1000,  400]), array([1000,  400]), array([1000,  400]), array([1000,  400]), array([1000,  400]), array([1000,  400]), array([1000,  400]), array([1000,  400]), array([1000,  400]), array([1000,  400]), array([1000,  400]), array([1000,  400]), array([1000,  400]), array([1000,  400]), array([1000,  400]), array([1000,  400]), array([1000,  400]), array([1000,  400]), array([1000,  400]), array([1000,  400]), array([1000,  400])], g0: 1e-05, B: 1000000, Pk: 0.1, noise: 1e-09\n",
      "Current Step: 50\n",
      "Action: 1\n",
      "UAV current position x: 1000, y: 400\n",
      "Current step reward: 0.01049979094418637, episodes rewards: 0.5905150618304572\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode reward for your Pytorch agent:  0.5905150618304572\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Average episode reward for your Pytorch agent: \",\n",
    "      pytorch_trainer.evaluate(1, render=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
