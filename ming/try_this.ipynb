{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from random import randint, choice\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAVEnvironment():\n",
    "    \"\"\"\n",
    "    Game environment for UAV test\n",
    "    \n",
    "    ---Map---\n",
    "    \n",
    "    y-axis(length)\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "     _______________________ x-axis(width)\n",
    "     \n",
    "    Hight is a fixed value\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        # Game config\n",
    "        self.action_space = (0, 1, 2, 3) # up, right, down, left, total 4 actions\n",
    "        self.total_steps = config[\"total_steps\"] # when the game end\n",
    "        self.current_step = 0\n",
    "        if config[\"is_random_env\"] == False:\n",
    "            self.random_seed = config[\"random_seed\"]\n",
    "            random.seed(self.random_seed)\n",
    "        \n",
    "        # Map config\n",
    "        self.map = dict(width=config[\"map\"][\"width\"], length=config[\"map\"][\"length\"], height=config[\"map\"][\"height\"])\n",
    "        self.UAV_speed = config[\"UAV_speed\"]\n",
    "        self.UAV_initial_pos = config[\"UAV_initial_pos\"] \n",
    "        self.inital_state=config[\"initial_state\"]\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        self.number_of_user = config[\"number_of_user\"]\n",
    "        self.users_pos = list()\n",
    "        self.UAV_path = [] # record the path of UAV\n",
    "        for i in range(0, self.number_of_user):\n",
    "            self.users_pos.append((randint(0, self.map[\"width\"]), randint(0, self.map[\"length\"])))\n",
    "        \n",
    "        # Wireless config\n",
    "        self.g0 = config[\"wireless_parameter\"][\"g0\"]\n",
    "        self.B = config[\"wireless_parameter\"][\"B\"]\n",
    "        self.Pk = config[\"wireless_parameter\"][\"Pk\"]\n",
    "        self.noise = config[\"wireless_parameter\"][\"noise\"]\n",
    "        \n",
    "    def get_reward(self,prev_pos, UAV_pos):\n",
    "        # One step Reward is define as the summation of all user's utility\n",
    "        \n",
    "        \n",
    "        #prev_reward =0\n",
    "        #for user_index in range(0, self.number_of_user):\n",
    "        #    gkm = self.g0 / (self.map[\"height\"] ** 2 + (prev_pos[0] - self.users_pos[user_index][0]) ** 2 + (prev_pos[1] - self.users_pos[user_index][1]) ** 2)\n",
    "        #    user_utility = self.B * math.log(1 + self.Pk * gkm / self.noise, 2)\n",
    "        #    prev_reward = prev_reward + user_utility\n",
    "        reward = 0\n",
    "        for user_index in range(0, self.number_of_user):\n",
    "            gkm = self.g0 / (self.map[\"height\"] ** 2 + (UAV_pos[0] - self.users_pos[user_index][0]) ** 2 + (UAV_pos[1] - self.users_pos[user_index][1]) ** 2)\n",
    "            user_utility = (self.B/self.number_of_user)* math.log(1 + self.Pk * gkm / self.noise, 2)\n",
    "            reward = reward + user_utility\n",
    "        return (reward) / (10 ** 6) # Use Mkbps as signal basic unit\n",
    "    \n",
    " \n",
    "    def transition_dynamics(self, action, speed, state):\n",
    "        # given the action (direction), calculate the next state (UAV current position)\n",
    "        assert action in self.action_space\n",
    "        next_UAV_pos = list(state)\n",
    "        if action == 0:\n",
    "            # move up\n",
    "            next_UAV_pos[1] = min(next_UAV_pos[1] + speed, self.map[\"length\"])\n",
    "        if action == 1:\n",
    "            # move right\n",
    "            next_UAV_pos[0] = min(next_UAV_pos[0] + speed, self.map[\"width\"])\n",
    "        if action == 2:\n",
    "            # move down\n",
    "            next_UAV_pos[1] = max(next_UAV_pos[1] - speed, 0)\n",
    "        if action == 3:\n",
    "            # move left\n",
    "            next_UAV_pos[0] = max(next_UAV_pos[0] - speed, 0)\n",
    "        return np.array(next_UAV_pos)\n",
    "                    \n",
    "    def step(self, action, speed=-1):\n",
    "        # assume we use the max speed as the default speed, when come near to the opt-position, we can slow down the speed\n",
    "        if speed < 0 or speed >= self.UAV_speed:\n",
    "            speed = self.UAV_speed\n",
    "        \n",
    "        prev_pos=self.UAV_current_pos\n",
    "        #update pos\n",
    "        self.UAV_path.append(prev_pos)\n",
    "        self.UAV_current_pos = self.transition_dynamics(action, speed, self.UAV_current_pos)\n",
    "        self.current_step = self.current_step + 1\n",
    "        done = False\n",
    "        if self.current_step == self.total_steps:\n",
    "            done =  True\n",
    "        state=self.UAV_current_pos/1000\n",
    "        return state, self.get_reward(prev_pos, self.UAV_current_pos), done\n",
    "    \n",
    "    def action_sample(self):\n",
    "        return choice(self.action_space)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        return self.UAV_current_pos\n",
    "        \n",
    "    def print_attribute(self):\n",
    "        attrs = vars(self)\n",
    "        print(', '.join(\"%s: %s\" % item for item in attrs.items()))\n",
    "        \n",
    "    def print_locations(self):\n",
    "        print(\"UAV position is: {}\".format(self.UAV_current_pos))\n",
    "        print(\"Users position are: {}\".format(self.users_pos))\n",
    "        \n",
    "    def print_map(self):\n",
    "        x_list = [pos[0] for pos in self.users_pos]\n",
    "        y_list = [pos[1] for pos in self.users_pos]\n",
    "        x_list.append(self.UAV_current_pos[0])\n",
    "        y_list.append(self.UAV_current_pos[1])\n",
    "        for i in range(0, len(self.UAV_path)):\n",
    "            x_list.append(self.UAV_path[i][0])\n",
    "            y_list.append(self.UAV_path[i][1])\n",
    "        \n",
    "        colors = np.array([\"red\", \"green\", \"blue\"])\n",
    "        sizes = []\n",
    "        colors_map = []\n",
    "        for i in range(0, self.number_of_user):\n",
    "            sizes.append(25)\n",
    "            colors_map.append(1)\n",
    "        sizes.append(50)\n",
    "        colors_map.append(0)\n",
    "        for i in range(0, len(self.UAV_path)):\n",
    "            sizes.append(10)\n",
    "            colors_map.append(2)\n",
    "        for i in range(0, len(self.UAV_path) - 1):\n",
    "            x_values = [self.UAV_path[i][0], self.UAV_path[i+1][0]]\n",
    "            y_values = [self.UAV_path[i][1], self.UAV_path[i+1][1]]\n",
    "            plt.plot(x_values, y_values, 'b')\n",
    "        plt.scatter(x_list, y_list, c=colors[colors_map], s=sizes) \n",
    "        plt.axis([0, self.map[\"width\"], 0, self.map[\"length\"]])\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_config = dict(\n",
    "    total_steps = 50,\n",
    "    random_seed = 10,\n",
    "    is_random_env = False,\n",
    "    map=dict(\n",
    "        width=1000,\n",
    "        length=1000,\n",
    "        height=100\n",
    "    ),\n",
    "    number_of_user = 10,\n",
    "    UAV_speed = 50,\n",
    "    UAV_initial_pos = np.array([0, 0]),\n",
    "    initial_state=0,\n",
    "    wireless_parameter = dict(\n",
    "        g0 = 10 ** (-5),\n",
    "        B = 10 ** (6),\n",
    "        Pk = 0.1,\n",
    "        noise = 10 ** (-9)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "def evaluate(policy, num_episodes=1, render=False):\n",
    "    global environment_config\n",
    "    env = UAVEnvironment(environment_config)\n",
    "    rewards = []\n",
    "    if render: num_episodes = 1\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        act = policy(obs)\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            obs, reward, done = env.step(act)\n",
    "            act = policy(obs)\n",
    "            ep_reward += reward\n",
    "            if render:\n",
    "                clear_output(wait=True)\n",
    "                env.print_attribute()\n",
    "                print(\"Current Step: {}\".format(env.current_step))\n",
    "                print(\"Action: {}\".format(act))\n",
    "                print(\"UAV current position x: {}, y: {}\".format(env.UAV_current_pos[0], env.UAV_current_pos[1]))\n",
    "                print(\"Current step reward: {}, episodes rewards: {}\".format(reward, ep_reward))\n",
    "                env.print_map()\n",
    "                wait(sleep=0.2)\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "    return np.mean(rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    assert inspect.isclass(trainer_cls)\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    trainer = trainer_cls(config)\n",
    "    config = trainer.config\n",
    "    start = now = time.time()\n",
    "    stats = []\n",
    "    for i in range(config['max_iteration'] + 1):\n",
    "        stat = trainer.train()\n",
    "        stats.append(stat or {})\n",
    "        if i % config['evaluate_interval'] == 0 or \\\n",
    "                i == config[\"max_iteration\"]:\n",
    "            reward = trainer.evaluate(config.get(\"evaluate_num_episodes\", 50))\n",
    "            print(\"({:.1f}s,+{:.1f}s)\\tIteration {}, current mean episode \"\n",
    "                  \"reward is {}. {}\".format(\n",
    "                time.time() - start, time.time() - now, i, reward,\n",
    "                {k: round(np.mean(v), 4) for k, v in\n",
    "                 stat.items()} if stat else \"\"))\n",
    "            now = time.time()\n",
    "        if reward_threshold is not None and reward > reward_threshold:\n",
    "            print(\"In {} iteration, current mean episode reward {:.3f} is \"\n",
    "                  \"greater than reward threshold {}. Congratulation! Now we \"\n",
    "                  \"exit the training process.\".format(\n",
    "                i, reward, reward_threshold))\n",
    "            break\n",
    "    return trainer, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve TODOs and remove \"pass\"\n",
    "\n",
    "default_config = dict(\n",
    "    max_iteration=1000,\n",
    "    max_episode_length=1000,\n",
    "    evaluate_interval=100,\n",
    "    gamma=0.99,\n",
    "    eps=0.3,\n",
    ")\n",
    "\n",
    "\n",
    "class AbstractTrainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = merge_config(config, default_config)\n",
    "\n",
    "        # Create the environment\n",
    "        self.env = UAVEnvironment(config)\n",
    "        self.act_dim=4\n",
    "        self.obs_dim=2 \n",
    "        self.eps = self.config['eps']\n",
    "\n",
    "        # You need to setup the parameter for your function approximator.\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        self.parameters = None\n",
    "        raise NotImplementedError(\n",
    "            \"You need to override the \"\n",
    "            \"Trainer._initialize_parameters() function.\")\n",
    "\n",
    "    def process_state(self, state):\n",
    "        \"\"\"Preprocess the state (observation) if necessary\"\"\"\n",
    "        processed_state = state\n",
    "        return processed_state\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        \"\"\"Approximate the state value of given state.\n",
    "        This is a private function.\n",
    "        Note that you should NOT preprocess the state here.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"You need to override the \"\n",
    "                                  \"Trainer.compute_values() function.\")\n",
    "\n",
    "    def compute_action(self, processed_state, eps=None):\n",
    "        \"\"\"Compute the action given the state. Note that the input\n",
    "        is the processed state.\"\"\"\n",
    "\n",
    "        values = self.compute_values(processed_state)\n",
    "        assert values.ndim == 1, values.shape\n",
    "        if eps is None:\n",
    "            eps = self.eps\n",
    "        if np.random.uniform(0,1)< eps:\n",
    "            action=self.env.action_sample()\n",
    "        else:\n",
    "            action=np.argmax(values)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def evaluate(self, num_episodes=50, *args, **kwargs):\n",
    "        \"\"\"Use the function you write to evaluate current policy.\n",
    "        Return the mean episode reward of 50 episodes.\"\"\"\n",
    "        policy = lambda raw_state: self.compute_action(\n",
    "            self.process_state(raw_state), eps=0.0)\n",
    "        result = evaluate(policy, num_episodes, *args, **kwargs)\n",
    "        return result\n",
    "\n",
    "    def compute_gradient(self, *args, **kwargs):\n",
    "        \"\"\"Compute the gradient.\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"You need to override the Trainer.compute_gradient() function.\")\n",
    "\n",
    "    def apply_gradient(self, *args, **kwargs):\n",
    "        \"\"\"Compute the gradient\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"You need to override the Trainer.apply_gradient() function.\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        raise NotImplementedError(\"You need to override the \"\n",
    "                                  \"Trainer.train() function.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_approximator_config = merge_config(dict(\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "    n=3,\n",
    "), default_config)\n",
    "\n",
    "\n",
    "class LinearTrainer(AbstractTrainer):\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, linear_approximator_config)\n",
    "\n",
    "        # Initialize the abstract class.\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.max_episode_length = self.config[\"max_episode_length\"]\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.gamma = self.config[\"gamma\"]\n",
    "        self.n = self.config[\"n\"]\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        std = self.config[\"parameter_std\"]\n",
    "        self.parameters = np.random.normal(0,std,(self.obs_dim, self.act_dim))\n",
    "        print(\"Initialize parameters with shape: {}.\".format(\n",
    "            self.parameters.shape))\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        assert processed_state.ndim == 1, processed_state.shape\n",
    "        values = np.dot(self.parameters.T,processed_state)\n",
    "        return values\n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        s = self.env.reset()\n",
    "        processed_s = self.process_state(s)\n",
    "        processed_states = [processed_s]\n",
    "        rewards = [0.0]\n",
    "        action=self.compute_action(processed_s)\n",
    "        actions = [action]\n",
    "        T = float(\"inf\")\n",
    "\n",
    "        for t in range(self.max_episode_length):\n",
    "            if t < T:\n",
    "                next_state,reward,done=self.env.step(action)\n",
    "                processed_s=self.process_state(next_state)\n",
    "                processed_states.append(processed_s)\n",
    "                rewards.append(reward)           \n",
    "            \n",
    "                if done:\n",
    "                    T=t+1\n",
    "                else:\n",
    "                    action=self.compute_action(processed_s)\n",
    "                    actions.append(action)\n",
    "\n",
    "            tau = t - self.n + 1\n",
    "            if tau >= 0:\n",
    "                gradient = self.compute_gradient(\n",
    "                    processed_states, actions, rewards, tau, T\n",
    "                )\n",
    "                self.apply_gradient(gradient)\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "\n",
    "    def compute_gradient(self, processed_states, actions, rewards, tau, T):\n",
    "        \"\"\"Compute the gradient\"\"\"\n",
    "        n = self.n\n",
    "        G = 0\n",
    "        for i in range(tau+1,min(T,tau+n)+1):\n",
    "            G+=np.power(self.gamma,i-tau-1)*rewards[i]\n",
    "    \n",
    "        if tau + n < T:   \n",
    "            q_values=self.compute_values(processed_states[tau+n])\n",
    "            G+=np.power(self.gamma,n)*q_values[actions[tau+n]]\n",
    "        q=self.compute_values(processed_states[tau])\n",
    "        act=actions[tau]\n",
    "        loss_grad = np.zeros((self.act_dim, 1))\n",
    "        loss_grad[act]=-(G-q[act])\n",
    "        value_grad = np.zeros((self.obs_dim, 1))\n",
    "        for i in range(self.obs_dim):\n",
    "             value_grad[i]=processed_states[tau][i]\n",
    "        \n",
    "        assert loss_grad.shape == (self.act_dim, 1)\n",
    "        assert value_grad.shape == (self.obs_dim, 1)\n",
    "\n",
    "        gradient = np.dot(value_grad,loss_grad.T)\n",
    "        return gradient\n",
    "\n",
    "    def apply_gradient(self, gradient):\n",
    "        \"\"\"Apply the gradient to the parameter.\"\"\"\n",
    "        assert gradient.shape == self.parameters.shape, (\n",
    "            gradient.shape, self.parameters.shape)\n",
    "        self.parameters-= self.learning_rate*gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_fc_config = merge_config(dict(\n",
    "    polynomial_order=1,\n",
    "), linear_approximator_config)\n",
    "\n",
    "\n",
    "def polynomial_feature(sequence, order=1):\n",
    "    s=sequence[0]\n",
    "    features=[s**n for n in range(order+1)]  \n",
    "    for i in range(len(sequence)-1):\n",
    "        s=sequence[i+1]\n",
    "        s_l=[s**n for n in range(order+1)]\n",
    "        tmp=np.array([features])\n",
    "        s_l=np.array([s_l])\n",
    "        features=np.dot(tmp.T,s_l)         \n",
    "        features=np.hstack(features).tolist()\n",
    "        \n",
    "    features= np.array(features).squeeze()\n",
    "    \n",
    "    return features\n",
    "    \n",
    "    \n",
    "\n",
    "assert sorted(polynomial_feature([2, 3, 4])) == [1, 2, 3, 4, 6, 8, 12, 24]\n",
    "assert len(polynomial_feature([2, 3, 4], 2)) == 27\n",
    "assert len(polynomial_feature([2, 3, 4], 3)) == 64\n",
    "\n",
    "class LinearTrainerWithFeatureConstruction(LinearTrainer):\n",
    "    \"\"\"In this class, we will expand the dimension of the state.\n",
    "    This procedure is done at self.process_state function.\n",
    "    The modification of self.obs_dim and the shape of parameters\n",
    "    is also needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, linear_fc_config)\n",
    "        # Initialize the abstract class.\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.polynomial_order = self.config[\"polynomial_order\"]\n",
    "\n",
    "        # Expand the size of observation\n",
    "        self.obs_dim = (self.polynomial_order + 1) ** self.obs_dim\n",
    "\n",
    "        # Since we change self.obs_dim, reset the parameters.\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def process_state(self, state):\n",
    "        \"\"\"Please finish the polynomial function.\"\"\"\n",
    "        processed = polynomial_feature(state, self.polynomial_order)\n",
    "        processed = np.asarray(processed)\n",
    "        assert len(processed) == self.obs_dim, processed.shape\n",
    "        return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_trainer_config = merge_config(dict(\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "    hidden_dim=100,\n",
    "    n=3,\n",
    "    clip_norm=1.0,\n",
    "    clip_gradient=True\n",
    "), default_config)\n",
    "\n",
    "\n",
    "class MLPTrainer(LinearTrainer):\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, mlp_trainer_config)\n",
    "        self.hidden_dim = config[\"hidden_dim\"]\n",
    "        super().__init__(config)\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        std = self.config[\"parameter_std\"]\n",
    "        self.hidden_parameters = np.random.normal(0,std,(self.obs_dim, self.hidden_dim))\n",
    "        self.output_parameters = np.random.normal(0,std,(self.hidden_dim, self.act_dim))\n",
    "\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        assert processed_state.ndim == 1, processed_state.shape\n",
    "        activation = self.compute_activation(processed_state)\n",
    "        values = np.dot(self.output_parameters.T,activation)\n",
    "        return values\n",
    "\n",
    "    def compute_activation(self, processed_state):\n",
    "        activation = np.dot(self.hidden_parameters.T,processed_state)\n",
    "        return activation\n",
    "\n",
    "    def compute_gradient(self, processed_states, actions, rewards, tau, T):\n",
    "        n = self.n\n",
    "        G = 0\n",
    "        for i in range(tau+1,min(T,tau+n)+1):\n",
    "            G+=np.power(self.gamma,i-tau-1)*rewards[i]\n",
    "    \n",
    "    \n",
    "        if tau + n < T:\n",
    "            Q_tau_plus_n = self.compute_values(processed_states[tau+n])\n",
    "            act=actions[tau+n]  \n",
    "            G = G + (self.gamma ** n) * Q_tau_plus_n[act]\n",
    "        cur_state = processed_states[tau]\n",
    "        \n",
    "\n",
    "        loss_grad = np.zeros((self.act_dim, 1))  # [act_dim, 1]\n",
    "        # compute loss_grad\n",
    "        q=self.compute_values(cur_state)\n",
    "        act=actions[tau]\n",
    "        loss_grad[act]=-(G-q[act])\n",
    "\n",
    "        activation=self.compute_activation(cur_state)\n",
    "        activation=np.array([activation])\n",
    "        output_gradient = np.dot(activation.T,loss_grad.T)\n",
    "        \n",
    "        \n",
    "        cur_state=np.array([cur_state])\n",
    "        hidden_gradient = cur_state.T@loss_grad.T@self.output_parameters.T\n",
    "  \n",
    "        assert np.all(np.isfinite(output_gradient)), \\\n",
    "            \"Invalid value occurs in output_gradient! {}\".format(\n",
    "                output_gradient)\n",
    "        assert np.all(np.isfinite(hidden_gradient)), \\\n",
    "            \"Invalid value occurs in hidden_gradient! {}\".format(\n",
    "                hidden_gradient)\n",
    "        return [hidden_gradient, output_gradient]\n",
    "\n",
    "    def apply_gradient(self, gradients):\n",
    "        \"\"\"Apply the gradientss to the two layers' parameters.\"\"\"\n",
    "        assert len(gradients) == 2\n",
    "        hidden_gradient, output_gradient = gradients\n",
    "\n",
    "        assert output_gradient.shape == (self.hidden_dim, self.act_dim)\n",
    "        assert hidden_gradient.shape == (self.obs_dim, self.hidden_dim)\n",
    "        \n",
    "        if self.config[\"clip_gradient\"]:\n",
    "            clip_norm = self.config[\"clip_norm\"]\n",
    "            output_gradient=output_gradient*clip_norm/max(clip_norm, np.linalg.norm(output_gradient))\n",
    "            hidden_gradient=hidden_gradient*clip_norm/max(clip_norm, np.linalg.norm(hidden_gradient))\n",
    "\n",
    "        self.output_parameters-= self.learning_rate*output_gradient\n",
    "        self.hidden_parameters-= self.learning_rate*hidden_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ExperienceReplayMemory:\n",
    "    \"\"\"Store and sample the transitions\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchModel(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(PytorchModel, self).__init__()\n",
    "        self.action_value = torch.nn.Sequential(\n",
    "            torch.nn.Linear(obs_dim,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,act_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.action_value(obs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_config = merge_config(dict(\n",
    "    memory_size=50000,\n",
    "    learn_start=5000,\n",
    "    batch_size=32,\n",
    "    target_update_freq=500,  # in steps\n",
    "    learn_freq=1,  # in steps\n",
    "    n=1\n",
    "), mlp_trainer_config)\n",
    "\n",
    "\n",
    "def to_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.float32)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    if x.dim() == 3 or x.dim() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    assert x.dim() == 2 or x.dim() == 4, x.shape\n",
    "    return x\n",
    "\n",
    "def to_long_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.long)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    if x.dim() == 3 or x.dim() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    assert x.dim() == 2 or x.dim() == 4, x.shape\n",
    "    return x\n",
    "\n",
    "\n",
    "class DQNTrainer(MLPTrainer):\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, pytorch_config)\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.memory = ExperienceReplayMemory(config[\"memory_size\"])\n",
    "        self.learn_start = config[\"learn_start\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.target_update_freq = config[\"target_update_freq\"]\n",
    "        self.clip_norm = config[\"clip_norm\"]\n",
    "        self.step_since_update = 0\n",
    "        self.total_step = 0\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"Initialize the pytorch model as the Q network and the target network\"\"\"\n",
    "        self.network = PytorchModel(self.obs_dim,self.act_dim)\n",
    "        self.network.eval()\n",
    "        self.network.share_memory()\n",
    "\n",
    "        self.target_network = PytorchModel(self.obs_dim,self.act_dim)\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.target_network.eval()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.network.parameters(), lr=self.learning_rate\n",
    "        )\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        values=self.network(processed_state)\n",
    "        return values.data.numpy()\n",
    "\n",
    "    def train(self):\n",
    "        s = self.env.reset()\n",
    "        processed_s = self.process_state(s)\n",
    "        act = self.compute_action(processed_s)\n",
    "        stat = {\"loss\": []}\n",
    "\n",
    "        for t in range(self.max_episode_length):\n",
    "            next_state, reward, done = self.env.step(act)\n",
    "            next_processed_s = self.process_state(next_state)\n",
    "\n",
    "            # Push the transition into memory.\n",
    "            self.memory.push(\n",
    "                (processed_s, act, reward, next_processed_s, done)\n",
    "            )\n",
    "\n",
    "            processed_s = next_processed_s\n",
    "            act = self.compute_action(next_processed_s)\n",
    "            self.step_since_update += 1\n",
    "            self.total_step += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            if t % self.config[\"learn_freq\"] != 0:\n",
    "                # It's not necessary to update in each step.\n",
    "                continue\n",
    "\n",
    "            if len(self.memory) < self.learn_start:\n",
    "                continue\n",
    "            elif len(self.memory) == self.learn_start:\n",
    "                print(\"Current memory contains {} transitions, \"\n",
    "                      \"start learning!\".format(self.learn_start))\n",
    "\n",
    "            batch = self.memory.sample(self.batch_size)\n",
    "\n",
    "            # Transform a batch of state / action / .. into a tensor.\n",
    "            state_batch = to_tensor(\n",
    "                np.stack([transition[0] for transition in batch])\n",
    "            )\n",
    "            action_batch = to_long_tensor(\n",
    "                np.stack([transition[1] for transition in batch])\n",
    "            )\n",
    "            reward_batch = to_tensor(\n",
    "                np.stack([transition[2] for transition in batch])\n",
    "            )\n",
    "            next_state_batch = torch.stack(\n",
    "                [transition[3] for transition in batch]\n",
    "            )\n",
    "            done_batch = to_tensor(\n",
    "                np.stack([transition[4] for transition in batch])\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_state_values=self.target_network(next_state_batch)\n",
    "                Q_t_plus_one = next_state_values.max(1)[0].detach()             \n",
    "                assert isinstance(Q_t_plus_one, torch.Tensor)\n",
    "                assert Q_t_plus_one.dim() == 1\n",
    "                \n",
    "                Q_target = reward_batch+(1-done_batch)*Q_t_plus_one\n",
    "            self.network.train()\n",
    "            \n",
    "            state_action_values=self.network(state_batch)\n",
    "            Q_t = torch.t(state_action_values).gather(0,action_batch)\n",
    "    \n",
    "            assert Q_t.shape == Q_target.shape\n",
    "\n",
    "            # Update the network\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss(input=Q_t, target=Q_target)\n",
    "            loss_value = loss.item()\n",
    "            stat['loss'].append(loss_value)\n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(self.network.parameters(), self.clip_norm)   \n",
    "            self.optimizer.step()\n",
    "            self.network.eval()\n",
    "\n",
    "        if len(self.memory) >= self.learn_start and self.step_since_update > self.target_update_freq:\n",
    "            self.step_since_update = 0\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "            self.target_network.eval()\n",
    "        return {\"loss\": np.mean(stat[\"loss\"]), \"episode_len\": t}\n",
    "\n",
    "    def process_state(self, state):\n",
    "        return torch.from_numpy(state).type(torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5s,+0.5s)\tIteration 0, current mean episode reward is 0.3613413752857501. {'loss': nan, 'episode_len': 49.0}\n",
      "(14.4s,+13.9s)\tIteration 100, current mean episode reward is 0.3696408423388967. {'loss': 0.0016, 'episode_len': 49.0}\n",
      "(29.8s,+15.5s)\tIteration 200, current mean episode reward is 0.9043933193042503. {'loss': 0.0036, 'episode_len': 49.0}\n",
      "(47.6s,+17.7s)\tIteration 300, current mean episode reward is 1.2864917953822916. {'loss': 0.0066, 'episode_len': 49.0}\n",
      "(63.0s,+15.5s)\tIteration 400, current mean episode reward is 0.9515775445988578. {'loss': 0.0079, 'episode_len': 49.0}\n",
      "(78.1s,+15.1s)\tIteration 500, current mean episode reward is 1.3770677379226637. {'loss': 0.0093, 'episode_len': 49.0}\n",
      "(96.9s,+18.7s)\tIteration 600, current mean episode reward is 1.0011321533288113. {'loss': 0.0105, 'episode_len': 49.0}\n",
      "(117.7s,+20.9s)\tIteration 700, current mean episode reward is 1.2864917953822916. {'loss': 0.0159, 'episode_len': 49.0}\n",
      "(140.1s,+22.4s)\tIteration 800, current mean episode reward is 0.9013007145374016. {'loss': 0.0218, 'episode_len': 49.0}\n",
      "(158.0s,+17.9s)\tIteration 900, current mean episode reward is 0.36001797958307014. {'loss': 0.0171, 'episode_len': 49.0}\n",
      "(175.2s,+17.1s)\tIteration 1000, current mean episode reward is 1.0064179895962715. {'loss': 0.0317, 'episode_len': 49.0}\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "config=merge_config(environment_config, dict(\n",
    "    max_iteration=1000,\n",
    "    evaluate_interval=100, \n",
    "    learning_rate=0.0001,\n",
    "    clip_norm=10.0,\n",
    "    memory_size=50000,\n",
    "    learn_start=1000,\n",
    "    eps=0.03,\n",
    "    target_update_freq=1000,\n",
    "    batch_size=32,\n",
    "))\n",
    "pytorch_trainer, pytorch_stat = run(DQNTrainer, config,reward_threshold=5) #,reward_threshold=2\n",
    "\n",
    "reward = pytorch_trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space: (0, 1, 2, 3), total_steps: 50, current_step: 50, random_seed: 10, map: {'width': 1000, 'length': 1000, 'height': 100}, UAV_speed: 50, UAV_initial_pos: [0 0], inital_state: 0, UAV_current_pos: [600 100], number_of_user: 10, users_pos: [(585, 33), (439, 494), (591, 15), (211, 473), (832, 503), (843, 284), (669, 830), (164, 35), (533, 501), (335, 77)], UAV_path: [array([0, 0]), array([ 0, 50]), array([50, 50]), array([100,  50]), array([150,  50]), array([200,  50]), array([200, 100]), array([200, 150]), array([250, 150]), array([300, 150]), array([350, 150]), array([400, 150]), array([450, 150]), array([500, 150]), array([550, 150]), array([600, 150]), array([600, 100]), array([600, 150]), array([600, 100]), array([600, 150]), array([600, 100]), array([600, 150]), array([600, 100]), array([600, 150]), array([600, 100]), array([600, 150]), array([600, 100]), array([600, 150]), array([600, 100]), array([600, 150]), array([600, 100]), array([600, 150]), array([600, 100]), array([600, 150]), array([600, 100]), array([600, 150]), array([600, 100]), array([600, 150]), array([600, 100]), array([600, 150]), array([600, 100]), array([600, 150]), array([600, 100]), array([600, 150]), array([600, 100]), array([600, 150]), array([600, 100]), array([600, 150]), array([600, 100]), array([600, 150])], g0: 1e-05, B: 1000000, Pk: 0.1, noise: 1e-09\n",
      "Current Step: 50\n",
      "Action: 0\n",
      "UAV current position x: 600, y: 100\n",
      "Current step reward: 0.024417965828110004, episodes rewards: 1.0064179895962713\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD8CAYAAACCRVh7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAbT0lEQVR4nO3de3RV9Zn/8feTKyTcNQICFSgI1TqixAo641hpvWAR6tClrVMYZRbj1LF2dHVG+/ujq3XZma7lbTrjqCwV8TJqpVoYhtYi3parigYviCAlIkKEQhC5BshJzvP7Y39DEshGzyU5Jzmf11pnnbO/57v3fs7OTj7Zl7O3uTsiIiIdKcp1ASIikr8UEiIiEkshISIisRQSIiISSyEhIiKxFBIiIhLrc0PCzB4ys+1mtrpN2yAzW2Zm68PzwNBuZvYrM6s1s1VmdmabcWaH/uvNbHbnfBwREcmmL7Il8TBw8RFtNwPL3X0ssDwMA1wCjA2PucC9EIUK8FPgbOBrwE9bgkVERPLX54aEu78C7DyieTqwILxeAMxo0/6IR14HBpjZUOAiYJm773T3z4BlHB08IiKSZ0rSHG+wu28FcPetZnZCaB8GbG7Try60xbUfxczmEm2FUFlZOXH8+PFpligiUphWrly5w92rsjGtdEMijnXQ5sdoP7rRfR4wD6C6utpramqyV52ISAEws4+zNa10z27aFnYjEZ63h/Y6YESbfsOBLcdoFxGRPJZuSCwGWs5Qmg0satM+K5zlNAnYHXZLPQdcaGYDwwHrC0ObiIjksc/d3WRmTwDnA8ebWR3RWUr/DvzazOYAm4DvhO5LgalALdAAXA3g7jvN7FbgzdDv5+5+5MFwERHJM5bPlwrXMQkRkdSZ2Up3r87GtPSNaxERiaWQEBGRWAoJERGJpZAQEZFYCgkREYmlkBARkVgKCRERiaWQEBGRWAoJERGJpZAQEZFYCgkREYmlkBARkVgKCRERiaWQEBGRWAoJERGJpZAQEZFYCgkREYmlkBARkVgKCRERiaWQEBGRWAoJERGJpZAQEZFYCgkREYmlkBARkVgKCRERiaWQEBGRWAoJERGJpZAQEZFYCgkRaefjXR/zo9//iEv/51Lur7mfRHMi1yVJDpXkugARyR8bd21kwn0TaEg0kEgmeGnjS/zf+v9j8XcX57o0yRFtSYjIYbf/8Xb2J/aTSEZbDw2JBp7f8Dxr6tfkuDLJFYWEiBy2bsc6mpJN7dpKi0vZuGtjbgqSnFNIiMhhl427jIqSinZtjc2NTBo+KUcVSa4pJETksLkT53LOiHOoKK2gX3k/epX04v5L72dQ70G5Lk1yRAeuReSw8pJyls1axjt/fodNuzdxzohzOL7i+FyXJTmU0ZaEmf2zmb1vZqvN7Akz62Vmo8xshZmtN7OnzKws9C0Pw7Xh/ZHZ+AAikn0ThkzgsnGXKSAk/ZAws2HAD4Fqd/8qUAxcCfwSuMvdxwKfAXPCKHOAz9x9DHBX6CciInks02MSJUBvMysBKoCtwAXAwvD+AmBGeD09DBPen2JmluH8RUSkE6UdEu7+CXA7sIkoHHYDK4Fd7t5yDl0dMCy8HgZsDuM2hf7HHTldM5trZjVmVlNfX59ueSIikgWZ7G4aSLR1MAo4EagELumgq7eMcoz3Whvc57l7tbtXV1VVpVueiIhkQSa7m74BfOTu9e6eAJ4BzgEGhN1PAMOBLeF1HTACILzfH9iZwfxFRKSTZRISm4BJZlYRji1MAdYALwIzQ5/ZwKLwenEYJrz/grsftSUhIiL5I5NjEiuIDkC/BbwXpjUP+FfgRjOrJTrm8GAY5UHguNB+I3BzBnWLiEgXsHz+Z766utprampyXYaISLdiZivdvTob09JlOUREJJZCQkREYikkREQklkJCRERiKSRERCSWQkJERGIpJEREJJZCQkREYikkREQklkJCRERiKSRERCSWQkJERGIpJEREJJZCQkREYikkREQklkJCRERiKSRERCSWQkJERGIpJEREJJZCQkREYikkREQklkJCRERiKSRERCSWQkJERGIpJEREJJZCQkREYikkREQklkJCRERiKSRERCSWQkJERGIpJEREJJZCQkREYikkREQklkJCRERiZRQSZjbAzBaa2QdmttbMJpvZIDNbZmbrw/PA0NfM7FdmVmtmq8zszOx8BBER6SyZbkn8B/B7dx8PnA6sBW4Glrv7WGB5GAa4BBgbHnOBezOct4iIdLK0Q8LM+gHnAQ8CuHuju+8CpgMLQrcFwIzwejrwiEdeBwaY2dC0KxcRkU6XyZbEaKAemG9mb5vZA2ZWCQx2960A4fmE0H8YsLnN+HWhrR0zm2tmNWZWU19fn0F5IiKSqUxCogQ4E7jX3c8A9tO6a6kj1kGbH9XgPs/dq929uqqqKoPyREQkU5mERB1Q5+4rwvBCotDY1rIbKTxvb9N/RJvxhwNbMpi/iIh0srRDwt3/DGw2s3GhaQqwBlgMzA5ts4FF4fViYFY4y2kSsLtlt5SIiOSnkgzHvx543MzKgA3A1UTB82szmwNsAr4T+i4FpgK1QEPoKyIieSyjkHD3d4DqDt6a0kFfB67LZH4iItK19I1rERGJpZAQEZFYCgkREYmlkBARkVgKCRERiaWQEBGRWAoJERGJpZAQEZFYCgkREYmlkBARkVgKCRERiaWQEBGRWJleBVZEjsHdqdtTR/9e/elX3i/X5RS8Nz55g6dWP0Xf8r5cc8Y1fKn/l3JdUt5TSIh0kve2vceMJ2ewdd9Wkp7k7yb8HfdMvYfiouJcl1aQ7nvzPm5adhMHEgcoLS7ljtfu4KXZLzHxxIm5Li2vaXeT9Gjb9m3j5y//nFnPzuLp958m6ckumW9zspmLHruIDbs2cKDpAIeaD/Hoqke5r+a+Lpm/tNfY3MiPn/8xDYkGHKexuZF9jfu46Q835bq0vKctCemx6vbUMeG+Cexr3Meh5kM8s/YZFq1bxGOXP9bp8161bRV7G/e2a2tINDD/nflc9zXdVqWr1e+vpznZfFT72h1rc1BN96ItCemx7vjjHew5tIdDzYcA2J/YzzNrn2H9p+s7fd59y/t2+EdpQK8BnT5vOdqQPkOoLKts11ZkRUwePjlHFXUfCgnpsVZtW0UimWjXVlZcRu3O2k6f95hBYzjrxLMoLy4/3FZRWsEtf3lLp89bjlZcVMyCGQvoXdKbytJK+pb15bjex3HnRXfmurS8p91N3VCiOfrDV1pcmuNK8tvFYy7mtbrXONB04HDbwaaDVJ/Y0R13s2/J95bwkxd+wrNrn+WEyhO49eu3MmX0UXf2lS4ydexUNtywgSV/WkLfsr5MGzeNitKKXJeV9yy69XR+qq6u9pqamlyXkTf2HtrL1YuuZtG6RRjGlV+9kvu/dT+9S3vnurS81JBo4K8f/ms+2PEBAE3JJm7/5u06JiA9npmtdPes/DekLYluZM7iOSz50xKakk0APL3maSpKK7jvWzpjpiMVpRWs+PsVvPLxK2zavYnzR56v8+JFUqQtiW6iOdlMr9t6HQ6IFn3K+rD3lr0xY4lIIcrmloQOXHcTZkaRHf3jKjFtDIpI51FIdBNFVsTs02fTu6T1+ENFaQXXVl+bw6pEpKfTv6HdyH9e8p9UlFbw8DsPU1xUzLUTr+VnX/9ZrssSkR5MxyRERHoYHZMQEZEuoZAQEZFYCgkREYmlkBARkVgKCRERiaWQEBGRWAoJERGJpZAQEZFYCgkREYmVcUiYWbGZvW1mS8LwKDNbYWbrzewpMysL7eVhuDa8PzLTeYuISOfKxpbEDUDbu4n/ErjL3ccCnwFzQvsc4DN3HwPcFfqJiEgeyygkzGw4cCnwQBg24AJgYeiyAJgRXk8Pw4T3p4T+IiKSpzLdkrgb+BcgGYaPA3a5e8udceqAYeH1MGAzQHh/d+jfjpnNNbMaM6upr6/PsDwREclE2iFhZt8Ctrv7yrbNHXT1L/Bea4P7PHevdvfqqqqqdMsTEZEsyOR+EucCl5nZVKAX0I9oy2KAmZWErYXhwJbQvw4YAdSZWQnQH9iZwfxFRKSTpb0l4e63uPtwdx8JXAm84O5XAS8CM0O32cCi8HpxGCa8/4Ln880sRESkU74n8a/AjWZWS3TM4cHQ/iBwXGi/Ebi5E+YtIiJZlJXbl7r7S8BL4fUG4Gsd9DkIfCcb8xMRka6hb1yLiEgshYSIiMRSSIiISCyFhIiIxFJIiIhILIWEiIjEUkiIiEgshYSIiMRSSIiISCyFhIiIxFJIiIhILIWEiIjEUkiIiEgshYSIiMRSSIiISCyFhIiIxFJIiEhBWrVtFZMfnEzv23pz2n+fxssbX851SXlJISEiBWf3wd2cN/88Xq97nYNNB1ldv5qp/zOVD3d+mOvS8o5CQkQKzm8/+C3N3tyuLdGc4JF3H8lRRflLISEiBSeRTODu7dqSniSRTOSoovylkBCRgjN93PSj2sqKy7jqtKtyUE1+U0iISMGpqqxi6VVLGTVgFEVWxODKwTz67Uc59YRTc11a3inJdQEiIrlw3knn8eEPP+RQ8yHKi8sxs1yXlJcUEiJSsMyMXiW9cl1GXtPuJhERiaWQEBGRWAoJERGJpZAQEZFYCgkREYmlkBARkVgKCRERiaWQEBGRWAoJERGJpZAQEZFYaYeEmY0wsxfNbK2ZvW9mN4T2QWa2zMzWh+eBod3M7FdmVmtmq8zszGx9CBER6RyZbEk0ATe5+1eAScB1ZnYKcDOw3N3HAsvDMMAlwNjwmAvcm8G8RUSkC6QdEu6+1d3fCq/3AmuBYcB0YEHotgCYEV5PBx7xyOvAADMbmnblIiLS6bJyTMLMRgJnACuAwe6+FaIgAU4I3YYBm9uMVhfajpzWXDOrMbOa+vr6bJQnIiJpyjgkzKwP8BvgR+6+51hdO2jzoxrc57l7tbtXV1VVZVqeiIhkIKOQMLNSooB43N2fCc3bWnYjheftob0OGNFm9OHAlkzmLyIinSuTs5sMeBBY6+53tnlrMTA7vJ4NLGrTPiuc5TQJ2N2yW0pERPJTJnemOxf4PvCemb0T2n4C/DvwazObA2wCvhPeWwpMBWqBBuDqDOYtIiJdIO2QcPdX6fg4A8CUDvo7cF268xMRka6nb1yLiEgshYSIiMRSSIiISCyFRJ7bsgXOOQcGD4Zbb01vGq+8AqNHw0knwR/+kN407rwThgyBs86CjRtTH//QIbjySqiqgiuuiIZT9fHH0fyHDIHbb099fIBly2DkyGh5vPRSetO47bbo5zF5MnzySerjNzTA5ZdHy2LWLEgkUp9Gv35g1voQ6SwWHU/OT9XV1V5TU5PrMnLq0kth6dLodVER/MVfQP/+qU3j1Vehubl1GueeGz1/Ufv2wdtvQzIZDffrB2eckVoNmzfDRx9By+o2ejSMGHHscY70zjuwe3f0uqgoqqFPny8+vnu0LFo+R1ER/NVfpVbDnj3w7rut0xg4MPqZpGLjxijwWmr48pfhxBNTm8bLL7cf7tULDhxIbRrSc5nZSnevzsa0tCWR5478T7WxMbXx3VsDomW45Q/cF5VItP9vNdUaWsZp+/9IOlsSbedrlnodyWT7GpLJ1JfFkfNMd1m0rSGdZXGkgwczn4ZIh9w9bx8TJ070QrdokXtRkXtxsfvJJ7vv3Zv6NK6/3r2iwr2y0n327NTHP3DA/fTT3fv0ce/d2/3xx1Ofxrp17v37tz4++CD1aTz5ZDT/vn3dTzstqitV11wTLYeKCvfrrkt9/H373MePj2qoqHB/9tnUp/Hee9H4/fq5DxzovmFD6tOI4q71MX9+6tOQnguo8Sz9Hdbupm5g8uToP8XXXot2K6Tj3XejLYozzkhvH3ZjI9TUwNChMGpUejXs3Anvvw+nngqDBqU3jY0bo+M01dVQVpb6+O7RbquWXXfpLIuDB2HlShg+PDrOk44dO2DtWjjtNBgwIL1ptNS+cCH8zd+kNw3pmbK5u0kh0Q2cf370nO6BVumZWkIij3+FJUd0TEJERLqEQkJERGIpJEREJJZCQqS72bEDfvADont2OUyaBC+8kOuqpIdSSIh0J7t2wcSJ8MADrW0rVsC0afCb3+SuLumxFBJ5bs2a6NTXl1+OLuGQ6pe/pIf5r/+C7dspTuw93GQko2t9/OM/tv/mpEgWKCTy3LXXtn5D99lno2sPSQF77DE4eJAkZUS3c4nOgy2lIboux6pVOS1Peh6FRJ478pIN6VwGQnqQmC2FZkqiL040NXVxQdLTKSTy3N13R98ONot2RV98ca4rkpz69rfDV82THD5wDdQyOFpRTj89l9VJD6SQyHOTJ0dXbZ00CV58EUpLc13R51tRt4KJ8yZS+YtKzn7gbN7a+lauS+o5brwR+vbFi8q4iZ/xFd5lJ0WMrjgE//Zv6V2rROQYFBLdQFFR9LvfHe4b8MmeT5jyyBTe2voWDYkG3vjkDc5/+Hzq99fnurSeYcgQeOMNuPBCbi/9BWvKzmbg8OFw333RgWuRLFNISFY9ufpJmpLt94s3ezML1yzMUUU90OjR8LvfRafDbt0KmzbB97+f66qkhyrJdQHSsySSCZLe/jzdZDJJY7OOuGddRUX0EOlE2pKQrJp5ykxKitr/72FmXP6Vy3NUkYhkQiEhWTVm0BiemvkUQ/oModiKGdZ3GM9e8Swj+qd4r1IRyQt5vbupsTH6hnEq92Nu69NPo1tvDhmS3vjuUFcX3VO6X7/0ppFIRLcgHToUysvTm0ZzczQd9+5x8HrauGlsOXkLDYkGKkorsO5QtIh0KK+3JFavhrPPTu/+vffcA8OGwciRcNNNqY+fTMLll8PJJ0chs2RJ6tOor4/GP/VU+NKXoruqperVV+GPf4Q334Tzzus+X6YzMyrLKhUQIt1cXt+Zzqzai4pqGDcOTjghtXFfeaX1jl1m0fcNUvmOwZ490S0/W66VVF4efVchFR9/HD1a6hg6NAqNVLz1FuwNl+np0wcefxwuuyy1aYhIYcnmnenyendTi3R2N5m1v61jqv/QFhdnXsOR0yguhl0Hd7Fx10YamxupqqzipP5fosiKO57AEfN1h969U69DuremZBPLNyxn54GdfPPL3+T4iuNzXZIUkLwPiSuuiK5pluof6SVL4Hvfi/bl3303/MM/pD7vm2+GO+6AgQNh6VKoTjGXGxrg0kujrZoJE+DW+a8z838v4EDTAQC2l/Ri/Enn8dzfPhc7jXXr4MILo+MaV1wB3/hG6p9Duq/6/fVMfnAy2/dvB6LAeGrmU0wbNy3HlUmhyOvdTX37VvvevTVpj992d1Mm08h0t3rLNKY+PpXf1f6u3Xu9S3qz+gerGT1wdKfXId3PtUuu5aG3HyKRTBxu61fej/of11NWrEtwSMeyubsprw9cZ8os8z+s2fjD3DKNlv8G2yopKmHngZ1dUod0P89veL5dQAC4O+s/XZ+jiqTQ9OiQyDdXnXYVvUvaH1QoLylnwpAJOapI8t3448cf1XYgcYBDzYc66C2SfQqJLnT92dczY/wMyovLqSytZHDlYJZ+b+lR31AWafGLKb+gT1kfSqx1HXGccx86l/lvz89hZVIoevQxiXy1bd82Pj3wKeOOG0dxUfyZTSIAGz7bwLQnprG2fi1O6+9rRWkF9T+up6JU12+S9nRMopsb3Gcwp1SdooCQL2T0wNE0Nje2CwiAYivmw50f5qgqKRRdHhJmdrGZrTOzWjO7+Vh9m5th//6uqkwkf5059EyKrP2va1OyiZEDRuamICkYXRoSZlYM3ANcApwCfNfMTonrf+AAnHJKdNl8kUJ22wW30a+8H71LelNEERWlFdz69VvpW94316VJD9fVR0y/BtS6+wYAM3sSmA6siRvh009h8WKYNauLKhTJQ2MGjWH99et5bNVj1O+vZ8b4GZw17KxclyUFoEsPXJvZTOBid//7MPx94Gx3/6c2feYCc8PgV4HVXVZgfjse2JHrIvKElkUrLYtWWhatxrl7VjYzu3pLoqOvhLVLKXefB8wDMLOabB2h7+60LFppWbTSsmilZdHKzLJ2WmhXH7iuA9refWY4sKWLaxARkS+oq0PiTWCsmY0yszLgSmBxF9cgIiJfUJfubnL3JjP7J+A5oBh4yN3fP8Yo87qmsm5By6KVlkUrLYtWWhatsrYs8vob1yIiklv6xrWIiMRSSIiISKy8DYlULt/RE5jZCDN70czWmtn7ZnZDaB9kZsvMbH14Hhjazcx+FZbPKjM7M7efILvMrNjM3jazJWF4lJmtCMvhqXDiA2ZWHoZrw/sjc1l3ZzCzAWa20Mw+COvH5EJcL8zsn8Pvxmoze8LMehXSemFmD5nZdjNb3aYt5fXAzGaH/uvNbPbnzTcvQyLVy3f0EE3ATe7+FWAScF34zDcDy919LLA8DEO0bMaGx1zg3q4vuVPdAKxtM/xL4K6wHD4D5oT2OcBn7j4GuCv062n+A/i9u48HTidaLgW1XpjZMOCHQLW7f5XoxJcrKaz14mHg4iPaUloPzGwQ8FPgbKIrYPy0JVhiuXvePYDJwHNthm8Bbsl1XV28DBYB3wTWAUND21BgXXh9P/DdNv0P9+vuD6LvzywHLgCWEH0JcwdQcuT6QXSm3OTwuiT0s1x/hiwui37AR0d+pkJbL4BhwGZgUPg5LwEuKrT1AhgJrE53PQC+C9zfpr1dv44eebklQesK0aIutBWEsGl8BrACGOzuWwHC8wmhW09eRncD/wIkw/BxwC53bwrDbT/r4eUQ3t8d+vcUo4F6YH7Y/faAmVVSYOuFu38C3A5sArYS/ZxXUrjrRYtU14OU1498DYnPvXxHT2VmfYDfAD9y9z3H6tpBW7dfRmb2LWC7u69s29xBV/8C7/UEJcCZwL3ufgawn9ZdCh3pkcsj7BKZDowCTgQqiXapHKlQ1ovPE/f5U14u+RoSBXn5DjMrJQqIx939mdC8zcyGhveHAttDe09dRucCl5nZRuBJol1OdwMDzA7fw7PtZz28HML7/YGdXVlwJ6sD6tx9RRheSBQahbZefAP4yN3r3T0BPAOcQ+GuFy1SXQ9SXj/yNSQK7vIdZmbAg8Bad7+zzVuLgZYzEGYTHatoaZ8VzmKYBOxu2ezsztz9Fncf7u4jiX7uL7j7VcCLwMzQ7cjl0LJ8Zob+PeY/Rnf/M7DZzMaFpilEl9YvqPWCaDfTJDOrCL8rLcuhINeLNlJdD54DLjSzgWHr7MLQFi/XB2KOcYBmKvAn4EPg/+W6ni74vH9JtNm3CngnPKYS7UddDqwPz4NCfyM6A+xD4D2isz5y/jmyvEzOB5aE16OBN4Ba4GmgPLT3CsO14f3Rua67E5bDBKAmrBu/BQYW4noB/Az4gOj2AY8C5YW0XgBPEB2PSRBtEcxJZz0ArgnLpRa4+vPmq8tyiIhIrHzd3SQiInlAISEiIrEUEiIiEkshISIisRQSIiISSyEhIiKxFBIiIhLr/wOHmRsbAN0ydwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode reward for your Pytorch agent:  1.0064179895962713\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Average episode reward for your Pytorch agent: \",\n",
    "      pytorch_trainer.evaluate(1, render=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
