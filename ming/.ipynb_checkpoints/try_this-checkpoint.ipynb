{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from random import randint, choice\n",
    "from IPython.display import clear_output\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAVEnvironment():\n",
    "    \"\"\"\n",
    "    Game environment for UAV test\n",
    "    \n",
    "    ---Map---\n",
    "    \n",
    "    y-axis(length)\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "     _______________________ x-axis(width)\n",
    "     \n",
    "    Hight is a fixed value\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        # Game config\n",
    "        self.action_space = (0, 1, 2, 3) # up, right, down, left, total 4 actions\n",
    "        self.total_steps = config[\"total_steps\"] # when the game end\n",
    "        self.current_step = 0\n",
    "        if config[\"is_random_env\"] == False:\n",
    "            self.random_seed = config[\"random_seed\"]\n",
    "            random.seed(self.random_seed)\n",
    "        \n",
    "        # Map config\n",
    "        self.map = dict(width=config[\"map\"][\"width\"], length=config[\"map\"][\"length\"], height=config[\"map\"][\"height\"])\n",
    "        self.UAV_speed = config[\"UAV_speed\"]\n",
    "        self.UAV_initial_pos = config[\"UAV_initial_pos\"] \n",
    "        self.inital_state=config[\"initial_state\"]\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        self.number_of_user = config[\"number_of_user\"]\n",
    "        self.users_pos = list()\n",
    "        self.UAV_path = [] # record the path of UAV\n",
    "        for i in range(0, self.number_of_user):\n",
    "            self.users_pos.append((randint(0, self.map[\"width\"]), randint(0, self.map[\"length\"])))\n",
    "        \n",
    "        # Wireless config\n",
    "        self.g0 = config[\"wireless_parameter\"][\"g0\"]\n",
    "        self.B = config[\"wireless_parameter\"][\"B\"]\n",
    "        self.Pk = config[\"wireless_parameter\"][\"Pk\"]\n",
    "        self.noise = config[\"wireless_parameter\"][\"noise\"]\n",
    "        \n",
    "    def get_reward(self,prev_pos, UAV_pos):\n",
    "        reward = 0\n",
    "        for user_index in range(0, self.number_of_user):\n",
    "            gkm = self.g0 / (self.map[\"height\"] ** 2 + (UAV_pos[0] - self.users_pos[user_index][0]) ** 2 + (UAV_pos[1] - self.users_pos[user_index][1]) ** 2)\n",
    "            user_utility = (self.B/self.number_of_user)* math.log(1 + self.Pk * gkm / self.noise, 2)\n",
    "            reward = reward + user_utility\n",
    "        return (reward) / (10 ** 6) # Use Mkbps as signal basic unit\n",
    "    \n",
    " \n",
    "    def transition_dynamics(self, action, speed, state):\n",
    "        # given the action (direction), calculate the next state (UAV current position)\n",
    "        assert action in self.action_space\n",
    "        next_UAV_pos = list(state)\n",
    "        if action == 0:\n",
    "            # move up\n",
    "            next_UAV_pos[1] = min(next_UAV_pos[1] + speed, self.map[\"length\"])\n",
    "        if action == 1:\n",
    "            # move right\n",
    "            next_UAV_pos[0] = min(next_UAV_pos[0] + speed, self.map[\"width\"])\n",
    "        if action == 2:\n",
    "            # move down\n",
    "            next_UAV_pos[1] = max(next_UAV_pos[1] - speed, 0)\n",
    "        if action == 3:\n",
    "            # move left\n",
    "            next_UAV_pos[0] = max(next_UAV_pos[0] - speed, 0)\n",
    "        return np.array(next_UAV_pos)\n",
    "                    \n",
    "    def step(self, action, speed=-1):\n",
    "        # assume we use the max speed as the default speed, when come near to the opt-position, we can slow down the speed\n",
    "        if speed < 0 or speed >= self.UAV_speed:\n",
    "            speed = self.UAV_speed\n",
    "        \n",
    "        prev_pos=self.UAV_current_pos\n",
    "        #update pos\n",
    "        self.UAV_path.append(prev_pos)\n",
    "        self.UAV_current_pos = self.transition_dynamics(action, speed, self.UAV_current_pos)\n",
    "        self.current_step = self.current_step + 1\n",
    "        done = False\n",
    "        if self.current_step == self.total_steps:\n",
    "            done =  True\n",
    "        state=self.UAV_current_pos/1000\n",
    "        return state, self.get_reward(prev_pos, self.UAV_current_pos), done\n",
    "    \n",
    "    def action_sample(self):\n",
    "        return choice(self.action_space)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        return self.UAV_current_pos\n",
    "        \n",
    "    def print_attribute(self):\n",
    "        attrs = vars(self)\n",
    "        print(', '.join(\"%s: %s\" % item for item in attrs.items()))\n",
    "        \n",
    "    def print_locations(self):\n",
    "        print(\"UAV position is: {}\".format(self.UAV_current_pos))\n",
    "        print(\"Users position are: {}\".format(self.users_pos))\n",
    "        \n",
    "    def print_map(self):\n",
    "        x_list = [pos[0] for pos in self.users_pos]\n",
    "        y_list = [pos[1] for pos in self.users_pos]\n",
    "        x_list.append(self.UAV_current_pos[0])\n",
    "        y_list.append(self.UAV_current_pos[1])\n",
    "        for i in range(0, len(self.UAV_path)):\n",
    "            x_list.append(self.UAV_path[i][0])\n",
    "            y_list.append(self.UAV_path[i][1])\n",
    "        \n",
    "        colors = np.array([\"red\", \"green\", \"blue\"])\n",
    "        sizes = []\n",
    "        colors_map = []\n",
    "        for i in range(0, self.number_of_user):\n",
    "            sizes.append(25)\n",
    "            colors_map.append(1)\n",
    "        sizes.append(50)\n",
    "        colors_map.append(0)\n",
    "        for i in range(0, len(self.UAV_path)):\n",
    "            sizes.append(10)\n",
    "            colors_map.append(2)\n",
    "        for i in range(0, len(self.UAV_path) - 1):\n",
    "            x_values = [self.UAV_path[i][0], self.UAV_path[i+1][0]]\n",
    "            y_values = [self.UAV_path[i][1], self.UAV_path[i+1][1]]\n",
    "            plt.plot(x_values, y_values, 'b')\n",
    "        plt.scatter(x_list, y_list, c=colors[colors_map], s=sizes) \n",
    "        plt.axis([0, self.map[\"width\"], 0, self.map[\"length\"]])\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "environment_config = dict(\n",
    "    total_steps = 50,\n",
    "    random_seed = 10,\n",
    "    is_random_env = False,\n",
    "    map=dict(\n",
    "        width=1000,\n",
    "        length=1000,\n",
    "        height=300\n",
    "    ),\n",
    "    number_of_user = 10,\n",
    "    UAV_speed = 50,\n",
    "    UAV_initial_pos = np.array([0, 0]),\n",
    "    initial_state=0,\n",
    "    wireless_parameter = dict(\n",
    "        g0 = 10 ** (-5),\n",
    "        B = 10 ** (6),\n",
    "        Pk = 0.1,\n",
    "        noise = 10 ** (-9)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def evaluate(policy, num_episodes=1, render=False):\n",
    "    global environment_config\n",
    "    env = UAVEnvironment(environment_config)\n",
    "    rewards = []\n",
    "    if render: num_episodes = 1\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        act = policy(obs)\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            obs, reward, done = env.step(act)\n",
    "            act = policy(obs)\n",
    "            ep_reward += reward\n",
    "            if render:\n",
    "                clear_output(wait=True)\n",
    "                env.print_attribute()\n",
    "                print(\"Current Step: {}\".format(env.current_step))\n",
    "                print(\"Action: {}\".format(act))\n",
    "                print(\"UAV current position x: {}, y: {}\".format(env.UAV_current_pos[0], env.UAV_current_pos[1]))\n",
    "                print(\"Current step reward: {}, episodes rewards: {}\".format(reward, ep_reward))\n",
    "                env.print_map()\n",
    "                wait(sleep=0.2)\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "    return np.mean(rewards)\n",
    "\n",
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    assert inspect.isclass(trainer_cls)\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    trainer = trainer_cls(config)\n",
    "    config = trainer.config\n",
    "    start = now = time.time()\n",
    "    stats = []\n",
    "    for i in range(config['max_iteration'] + 1):\n",
    "        stat = trainer.train()\n",
    "        stats.append(stat or {})\n",
    "        if i % config['evaluate_interval'] == 0 or \\\n",
    "                i == config[\"max_iteration\"]:\n",
    "            reward = trainer.evaluate(config.get(\"evaluate_num_episodes\", 50))\n",
    "            print(\"({:.1f}s,+{:.1f}s)\\tIteration {}, current mean episode \"\n",
    "                  \"reward is {}. {}\".format(\n",
    "                time.time() - start, time.time() - now, i, reward,\n",
    "                {k: round(np.mean(v), 4) for k, v in\n",
    "                 stat.items()} if stat else \"\"))\n",
    "            now = time.time()\n",
    "        if reward_threshold is not None and reward > reward_threshold:\n",
    "            print(\"In {} iteration, current mean episode reward {:.3f} is \"\n",
    "                  \"greater than reward threshold {}. Congratulation! Now we \"\n",
    "                  \"exit the training process.\".format(\n",
    "                i, reward, reward_threshold))\n",
    "            break\n",
    "    return trainer, stats\n",
    "\n",
    "\n",
    "def to_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.float32)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    if x.dim() == 3 or x.dim() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    assert x.dim() == 2 or x.dim() == 4, x.shape\n",
    "    return x\n",
    "\n",
    "def to_long_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.long)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    if x.dim() == 3 or x.dim() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    assert x.dim() == 2 or x.dim() == 4, x.shape\n",
    "    return x\n",
    "\n",
    "class ExperienceReplayMemory:\n",
    "    \"\"\"Store and sample the transitions\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class PytorchModel(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(PytorchModel, self).__init__()\n",
    "        self.action_value = torch.nn.Sequential(\n",
    "            torch.nn.Linear(obs_dim,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,act_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.action_value(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_config = dict(\n",
    "    max_iteration=1000,\n",
    "    max_episode_length=1000,\n",
    "    evaluate_interval=100,\n",
    "    gamma=0.99,\n",
    "    eps=0.3,\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "    hidden_dim=100,\n",
    "    clip_norm=1.0,\n",
    "    clip_gradient=True,\n",
    "    memory_size=50000,\n",
    "    learn_start=5000,\n",
    "    batch_size=32,\n",
    "    target_update_freq=500,  # in steps\n",
    "    learn_freq=1,  # in steps\n",
    "    n=1\n",
    ")\n",
    "\n",
    "class DQNTrainer():\n",
    "    def __init__(self, config):\n",
    "        self.config = merge_config(config, pytorch_config)\n",
    "        self.env = UAVEnvironment(config)\n",
    "        self.act_dim=4\n",
    "        self.obs_dim=2 \n",
    "        self.eps = self.config['eps']\n",
    "        self.hidden_dim = self.config[\"hidden_dim\"]\n",
    "        self.max_episode_length = self.config[\"max_episode_length\"]\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.gamma = self.config[\"gamma\"]\n",
    "        self.n = self.config[\"n\"]\n",
    "        self.initialize_parameters()\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.learn_start = self.config[\"learn_start\"]\n",
    "        self.batch_size = self.config[\"batch_size\"]\n",
    "        self.target_update_freq = self.config[\"target_update_freq\"]\n",
    "        self.clip_norm = self.config[\"clip_norm\"]\n",
    "        self.step_since_update = 0\n",
    "        self.total_step = 0\n",
    "        self.memory = ExperienceReplayMemory(self.config[\"memory_size\"])\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        self.network = PytorchModel(self.obs_dim,self.act_dim)\n",
    "        self.network.eval()\n",
    "        self.network.share_memory()\n",
    "        self.target_network = PytorchModel(self.obs_dim,self.act_dim)\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.target_network.eval()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.network.parameters(), lr=self.learning_rate\n",
    "        )\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        values=self.network(processed_state)\n",
    "        return values.data.numpy()\n",
    "\n",
    "    def train(self):\n",
    "        s = self.env.reset()\n",
    "        processed_s = self.process_state(s)\n",
    "        act = self.compute_action(processed_s)\n",
    "        stat = {\"loss\": []}\n",
    "        for t in range(self.max_episode_length):\n",
    "            next_state, reward, done = self.env.step(act)\n",
    "            next_processed_s = self.process_state(next_state)\n",
    "            self.memory.push((processed_s, act, reward, next_processed_s, done))\n",
    "            processed_s = next_processed_s\n",
    "            act = self.compute_action(next_processed_s)\n",
    "            self.step_since_update += 1\n",
    "            self.total_step += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            if t % self.config[\"learn_freq\"] != 0:\n",
    "                continue\n",
    "            if len(self.memory) < self.learn_start:\n",
    "                continue\n",
    "            elif len(self.memory) == self.learn_start:\n",
    "                print(\"Current memory contains {} transitions, \"\n",
    "                      \"start learning!\".format(self.learn_start))\n",
    "\n",
    "            batch = self.memory.sample(self.batch_size)\n",
    "            state_batch = to_tensor( np.stack([transition[0] for transition in batch]))\n",
    "            action_batch = to_long_tensor(np.stack([transition[1] for transition in batch]))\n",
    "            reward_batch = to_tensor(np.stack([transition[2] for transition in batch]))\n",
    "            next_state_batch = torch.stack([transition[3] for transition in batch])\n",
    "            done_batch = to_tensor(np.stack([transition[4] for transition in batch]))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_state_values=self.target_network(next_state_batch)\n",
    "                Q_t_plus_one = next_state_values.max(1)[0].detach()             \n",
    "                assert isinstance(Q_t_plus_one, torch.Tensor)\n",
    "                assert Q_t_plus_one.dim() == 1\n",
    "                Q_target = reward_batch+(1-done_batch)*Q_t_plus_one\n",
    "            self.network.train()\n",
    "            \n",
    "            state_action_values=self.network(state_batch)\n",
    "            Q_t = torch.t(state_action_values).gather(0,action_batch)\n",
    "            assert Q_t.shape == Q_target.shape\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss(input=Q_t, target=Q_target)\n",
    "            loss_value = loss.item()\n",
    "            stat['loss'].append(loss_value)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.network.parameters(), self.clip_norm)   \n",
    "            self.optimizer.step()\n",
    "            self.network.eval()\n",
    "\n",
    "        if len(self.memory) >= self.learn_start and self.step_since_update > self.target_update_freq:\n",
    "            self.step_since_update = 0\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "            self.target_network.eval()\n",
    "        return {\"loss\": np.mean(stat[\"loss\"]), \"episode_len\": t}\n",
    "    \n",
    "    def compute_action(self, processed_state, eps=None):\n",
    "        values = self.compute_values(processed_state)\n",
    "        assert values.ndim == 1, values.shape\n",
    "        if eps is None:\n",
    "            eps = self.eps\n",
    "        if np.random.uniform(0,1)< eps:\n",
    "            action=self.env.action_sample()\n",
    "        else:\n",
    "            action=np.argmax(values)\n",
    "        return action\n",
    "\n",
    "    def compute_activation(self, processed_state):\n",
    "        activation = np.dot(self.hidden_parameters.T,processed_state)\n",
    "        return activation\n",
    "\n",
    "    def compute_gradient(self, processed_states, actions, rewards, tau, T):\n",
    "        n = self.n\n",
    "        G = 0\n",
    "        for i in range(tau+1,min(T,tau+n)+1):\n",
    "            G+=np.power(self.gamma,i-tau-1)*rewards[i]\n",
    "        if tau + n < T:\n",
    "            Q_tau_plus_n = self.compute_values(processed_states[tau+n])\n",
    "            act=actions[tau+n]  \n",
    "            G = G + (self.gamma ** n) * Q_tau_plus_n[act]\n",
    "        cur_state = processed_states[tau]\n",
    "        loss_grad = np.zeros((self.act_dim, 1))  # [act_dim, 1]\n",
    "        q=self.compute_values(cur_state)\n",
    "        act=actions[tau]\n",
    "        loss_grad[act]=-(G-q[act])\n",
    "\n",
    "        activation=self.compute_activation(cur_state)\n",
    "        activation=np.array([activation])\n",
    "        output_gradient = np.dot(activation.T,loss_grad.T)\n",
    "  \n",
    "        assert np.all(np.isfinite(output_gradient)), \\\n",
    "            \"Invalid value occurs in output_gradient! {}\".format(\n",
    "                output_gradient)\n",
    "        assert np.all(np.isfinite(hidden_gradient)), \\\n",
    "            \"Invalid value occurs in hidden_gradient! {}\".format(\n",
    "                hidden_gradient)\n",
    "        return [hidden_gradient, output_gradient]\n",
    "\n",
    "    def apply_gradient(self, gradients):\n",
    "        \"\"\"Apply the gradientss to the two layers' parameters.\"\"\"\n",
    "        assert len(gradients) == 2\n",
    "        hidden_gradient, output_gradient = gradients\n",
    "        assert output_gradient.shape == (self.hidden_dim, self.act_dim)\n",
    "        assert hidden_gradient.shape == (self.obs_dim, self.hidden_dim)\n",
    "        if self.config[\"clip_gradient\"]:\n",
    "            clip_norm = self.config[\"clip_norm\"]\n",
    "            output_gradient=output_gradient*clip_norm/max(clip_norm, np.linalg.norm(output_gradient))\n",
    "            hidden_gradient=hidden_gradient*clip_norm/max(clip_norm, np.linalg.norm(hidden_gradient))\n",
    "\n",
    "        self.output_parameters-= self.learning_rate*output_gradient\n",
    "        self.hidden_parameters-= self.learning_rate*hidden_gradient\n",
    "\n",
    "    def evaluate(self, num_episodes=50, *args, **kwargs):\n",
    "        policy = lambda raw_state: self.compute_action(\n",
    "            self.process_state(raw_state), eps=0.0)\n",
    "        result = evaluate(policy, num_episodes, *args, **kwargs)\n",
    "        return result\n",
    "    \n",
    "    def process_state(self, state):\n",
    "        return torch.from_numpy(state).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5s,+0.5s)\tIteration 0, current mean episode reward is 0.053481979872960064. {'loss': nan, 'episode_len': 49.0}\n",
      "(15.7s,+15.2s)\tIteration 100, current mean episode reward is 0.04614102362643464. {'loss': 0.0002, 'episode_len': 49.0}\n",
      "(30.5s,+14.8s)\tIteration 200, current mean episode reward is 0.04463498605876133. {'loss': 0.0003, 'episode_len': 49.0}\n",
      "(48.2s,+17.8s)\tIteration 300, current mean episode reward is 0.04731868605223995. {'loss': 0.0003, 'episode_len': 49.0}\n",
      "(63.6s,+15.4s)\tIteration 400, current mean episode reward is 0.05196289038846519. {'loss': 0.0004, 'episode_len': 49.0}\n",
      "(78.3s,+14.7s)\tIteration 500, current mean episode reward is 0.0536591010716146. {'loss': 0.0004, 'episode_len': 49.0}\n",
      "(95.2s,+16.8s)\tIteration 600, current mean episode reward is 0.05307933291827272. {'loss': 0.0003, 'episode_len': 49.0}\n",
      "(112.1s,+16.9s)\tIteration 700, current mean episode reward is 0.0506829852457185. {'loss': 0.0005, 'episode_len': 49.0}\n",
      "(127.3s,+15.2s)\tIteration 800, current mean episode reward is 0.050024182391946885. {'loss': 0.0007, 'episode_len': 49.0}\n",
      "(147.6s,+20.3s)\tIteration 900, current mean episode reward is 0.04818729681431187. {'loss': 0.0006, 'episode_len': 49.0}\n",
      "(163.5s,+15.9s)\tIteration 1000, current mean episode reward is 0.0506829852457185. {'loss': 0.0006, 'episode_len': 49.0}\n"
     ]
    }
   ],
   "source": [
    "config=merge_config(environment_config, dict(\n",
    "    max_iteration=1000,\n",
    "    evaluate_interval=100, \n",
    "    learning_rate=0.0001,\n",
    "    clip_norm=10.0,\n",
    "    memory_size=50000,\n",
    "    learn_start=1000,\n",
    "    eps=0.03,\n",
    "    target_update_freq=1000,\n",
    "    batch_size=32,\n",
    "))\n",
    "pytorch_trainer, pytorch_stat = run(DQNTrainer, config,reward_threshold=5) #,reward_threshold=2\n",
    "\n",
    "reward = pytorch_trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space: (0, 1, 2, 3), total_steps: 50, current_step: 49, random_seed: 10, map: {'width': 1000, 'length': 1000, 'height': 1000}, UAV_speed: 50, UAV_initial_pos: [0 0], inital_state: 0, UAV_current_pos: [0 0], number_of_user: 10, users_pos: [(585, 33), (439, 494), (591, 15), (211, 473), (832, 503), (843, 284), (669, 830), (164, 35), (533, 501), (335, 77)], UAV_path: [array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0]), array([0, 0])], g0: 1e-05, B: 1000000, Pk: 0.1, noise: 1e-09\n",
      "Current Step: 49\n",
      "Action: 2\n",
      "UAV current position x: 0, y: 0\n",
      "Current step reward: 0.0010136597049143705, episodes rewards: 0.04966932554080414\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(\"Average episode reward for your Pytorch agent: \",\n",
    "      pytorch_trainer.evaluate(1, render=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
