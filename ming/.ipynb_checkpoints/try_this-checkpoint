{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from random import randint, choice\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAVEnvironment():\n",
    "    \"\"\"\n",
    "    Game environment for UAV test\n",
    "    \n",
    "    ---Map---\n",
    "    \n",
    "    y-axis(length)\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "     _______________________ x-axis(width)\n",
    "     \n",
    "    Hight is a fixed value\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        # Game config\n",
    "        self.action_space = (0, 1, 2, 3) # up, right, down, left, total 4 actions\n",
    "        self.total_steps = config[\"total_steps\"] # when the game end\n",
    "        self.current_step = 0\n",
    "        if config[\"is_random_env\"] == False:\n",
    "            self.random_seed = config[\"random_seed\"]\n",
    "            random.seed(self.random_seed)\n",
    "        \n",
    "        # Map config\n",
    "        self.map = dict(width=config[\"map\"][\"width\"], length=config[\"map\"][\"length\"], height=config[\"map\"][\"height\"])\n",
    "        self.UAV_speed = config[\"UAV_speed\"]\n",
    "        self.UAV_initial_pos = config[\"UAV_initial_pos\"] \n",
    "        self.inital_state=config[\"initial_state\"]\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        self.number_of_user = config[\"number_of_user\"]\n",
    "        self.users_pos = list()\n",
    "        for i in range(0, self.number_of_user):\n",
    "            self.users_pos.append((randint(0, self.map[\"width\"]), randint(0, self.map[\"length\"])))\n",
    "        \n",
    "        # Wireless config\n",
    "        self.g0 = config[\"wireless_parameter\"][\"g0\"]\n",
    "        self.B = config[\"wireless_parameter\"][\"B\"]\n",
    "        self.Pk = config[\"wireless_parameter\"][\"Pk\"]\n",
    "        self.noise = config[\"wireless_parameter\"][\"noise\"]\n",
    "        \n",
    "    def get_reward(self,prev_pos, UAV_pos):\n",
    "        # One step Reward is define as the summation of all user's utility\n",
    "        \n",
    "        \n",
    "        #prev_reward =0\n",
    "        #for user_index in range(0, self.number_of_user):\n",
    "        #    gkm = self.g0 / (self.map[\"height\"] ** 2 + (prev_pos[0] - self.users_pos[user_index][0]) ** 2 + (prev_pos[1] - self.users_pos[user_index][1]) ** 2)\n",
    "        #    user_utility = self.B * math.log(1 + self.Pk * gkm / self.noise, 2)\n",
    "        #    prev_reward = prev_reward + user_utility\n",
    "        reward = 0\n",
    "        for user_index in range(0, self.number_of_user):\n",
    "            gkm = self.g0 / (self.map[\"height\"] ** 2 + (UAV_pos[0] - self.users_pos[user_index][0]) ** 2 + (UAV_pos[1] - self.users_pos[user_index][1]) ** 2)\n",
    "            user_utility = (self.B/self.number_of_user)* math.log(1 + self.Pk * gkm / self.noise, 2)\n",
    "            reward = reward + user_utility\n",
    "        return (reward) / (10 ** 6) # Use Mkbps as signal basic unit\n",
    "    \n",
    " \n",
    "    def transition_dynamics(self, action, speed, state):\n",
    "        # given the action (direction), calculate the next state (UAV current position)\n",
    "        assert action in self.action_space\n",
    "        next_UAV_pos = list(state)\n",
    "        if action == 0:\n",
    "            # move up\n",
    "            next_UAV_pos[1] = min(next_UAV_pos[1] + speed, self.map[\"length\"])\n",
    "        if action == 1:\n",
    "            # move right\n",
    "            next_UAV_pos[0] = min(next_UAV_pos[0] + speed, self.map[\"width\"])\n",
    "        if action == 2:\n",
    "            # move down\n",
    "            next_UAV_pos[1] = max(next_UAV_pos[1] - speed, 0)\n",
    "        if action == 3:\n",
    "            # move left\n",
    "            next_UAV_pos[0] = max(next_UAV_pos[0] - speed, 0)\n",
    "        return np.array(next_UAV_pos)\n",
    "                    \n",
    "    def step(self, action, speed=-1):\n",
    "        # assume we use the max speed as the default speed, when come near to the opt-position, we can slow down the speed\n",
    "        if speed < 0 or speed >= self.UAV_speed:\n",
    "            speed = self.UAV_speed\n",
    "        \n",
    "        prev_pos=self.UAV_current_pos\n",
    "        #update pos\n",
    "        self.UAV_current_pos = self.transition_dynamics(action, speed, self.UAV_current_pos)\n",
    "        self.current_step = self.current_step + 1\n",
    "        done = False\n",
    "        if self.current_step == self.total_steps:\n",
    "            done =  True\n",
    "        state=self.UAV_current_pos/1000\n",
    "        return state, self.get_reward(prev_pos, self.UAV_current_pos), done\n",
    "    \n",
    "    def action_sample(self):\n",
    "        return choice(self.action_space)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        return self.UAV_current_pos\n",
    "        \n",
    "    def print_attribute(self):\n",
    "        attrs = vars(self)\n",
    "        print(', '.join(\"%s: %s\" % item for item in attrs.items()))\n",
    "        \n",
    "    def print_locations(self):\n",
    "        print(\"UAV position is: {}\".format(self.UAV_current_pos))\n",
    "        print(\"Users position are: {}\".format(self.users_pos))\n",
    "        \n",
    "    def print_map(self):\n",
    "        x_list = [pos[0] for pos in self.users_pos]\n",
    "        y_list = [pos[1] for pos in self.users_pos]\n",
    "        x_list.append(self.UAV_current_pos[0])\n",
    "        y_list.append(self.UAV_current_pos[1])\n",
    "        \n",
    "        colors = np.array([\"red\", \"green\"])\n",
    "        sizes = []\n",
    "        colors_map = []\n",
    "        for i in range(0, self.number_of_user):\n",
    "            sizes.append(25)\n",
    "            colors_map.append(1)\n",
    "        sizes.append(50)\n",
    "        colors_map.append(0)\n",
    "        plt.scatter(x_list, y_list, c=colors[colors_map], s=sizes) \n",
    "        plt.axis([0, self.map[\"width\"], 0, self.map[\"length\"]])\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_config = dict(\n",
    "    total_steps = 50,\n",
    "    random_seed = 10,\n",
    "    is_random_env = False,\n",
    "    map=dict(\n",
    "        width=1000,\n",
    "        length=1000,\n",
    "        height=100\n",
    "    ),\n",
    "    number_of_user = 10,\n",
    "    UAV_speed = 20,\n",
    "    UAV_initial_pos = np.array([0, 0]),\n",
    "    initial_state=0,\n",
    "    wireless_parameter = dict(\n",
    "        g0 = 10 ** (-5),\n",
    "        B = 10 ** (6),\n",
    "        Pk = 0.1,\n",
    "        noise = 10 ** (-9)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "def evaluate(policy, num_episodes=1, render=False):\n",
    "    \"\"\"This function evaluate the given policy and return the mean episode \n",
    "    reward.\n",
    "    :param policy: a function whose input is the observation\n",
    "    :param num_episodes: number of episodes you wish to run\n",
    "    :param seed: the random seed\n",
    "    :param env_name: the name of the environment\n",
    "    :param render: a boolean flag indicating whether to render policy\n",
    "    :return: the averaged episode reward of the given policy.\n",
    "    \"\"\"\n",
    "    global environment_config\n",
    "    env = UAVEnvironment(environment_config)\n",
    "    rewards = []\n",
    "    if render: num_episodes = 1\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        act = policy(obs)\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            obs, reward, done = env.step(act)\n",
    "            act = policy(obs)\n",
    "            ep_reward += reward\n",
    "            if render:\n",
    "                clear_output(wait=True)\n",
    "                env.print_attribute()\n",
    "                print(\"Current Step: {}\".format(env.current_step))\n",
    "                print(\"Action: {}\".format(act))\n",
    "                print(\"UAV current position x: {}, y: {}\".format(env.UAV_current_pos[0], env.UAV_current_pos[1]))\n",
    "                print(\"Current step reward: {}, episodes rewards: {}\".format(reward, ep_reward))\n",
    "                env.print_map()\n",
    "                wait(sleep=0.2)\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "    return np.mean(rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    \"\"\"Run the trainer and report progress, agnostic to the class of trainer\n",
    "    :param trainer_cls: A trainer class \n",
    "    :param config: A dict\n",
    "    :param reward_threshold: the reward threshold to break the training\n",
    "    :return: The trained trainer and a dataframe containing learning progress\n",
    "    \"\"\"\n",
    "    assert inspect.isclass(trainer_cls)\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    trainer = trainer_cls(config)\n",
    "    config = trainer.config\n",
    "    start = now = time.time()\n",
    "    stats = []\n",
    "    for i in range(config['max_iteration'] + 1):\n",
    "        stat = trainer.train()\n",
    "        stats.append(stat or {})\n",
    "        if i % config['evaluate_interval'] == 0 or \\\n",
    "                i == config[\"max_iteration\"]:\n",
    "            reward = trainer.evaluate(config.get(\"evaluate_num_episodes\", 50))\n",
    "            print(\"({:.1f}s,+{:.1f}s)\\tIteration {}, current mean episode \"\n",
    "                  \"reward is {}. {}\".format(\n",
    "                time.time() - start, time.time() - now, i, reward,\n",
    "                {k: round(np.mean(v), 4) for k, v in\n",
    "                 stat.items()} if stat else \"\"))\n",
    "            now = time.time()\n",
    "        if reward_threshold is not None and reward > reward_threshold:\n",
    "            print(\"In {} iteration, current mean episode reward {:.3f} is \"\n",
    "                  \"greater than reward threshold {}. Congratulation! Now we \"\n",
    "                  \"exit the training process.\".format(\n",
    "                i, reward, reward_threshold))\n",
    "            break\n",
    "    return trainer, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve TODOs and remove \"pass\"\n",
    "\n",
    "default_config = dict(\n",
    "    max_iteration=1000,\n",
    "    max_episode_length=1000,\n",
    "    evaluate_interval=100,\n",
    "    gamma=0.99,\n",
    "    eps=0.3,\n",
    ")\n",
    "\n",
    "\n",
    "class AbstractTrainer:\n",
    "    \"\"\"This is the abstract class for value-based RL trainer. We will inherent\n",
    "    the specify algorithm's trainer from this abstract class, so that we can\n",
    "    reuse the codes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = merge_config(config, default_config)\n",
    "\n",
    "        # Create the environment\n",
    "        self.env = UAVEnvironment(config)\n",
    "\n",
    "        # We set self.obs_dim to the number of possible observation\n",
    "        # if observation space is discrete, otherwise the number\n",
    "        # of observation's dimensions. The same to self.act_dim.        \n",
    "        self.act_dim=4\n",
    "        self.obs_dim=2 \n",
    "        self.eps = self.config['eps']\n",
    "\n",
    "        # You need to setup the parameter for your function approximator.\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        self.parameters = None\n",
    "        raise NotImplementedError(\n",
    "            \"You need to override the \"\n",
    "            \"Trainer._initialize_parameters() function.\")\n",
    "\n",
    "    def process_state(self, state):\n",
    "        \"\"\"Preprocess the state (observation) if necessary\"\"\"\n",
    "        processed_state = state\n",
    "        return processed_state\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        \"\"\"Approximate the state value of given state.\n",
    "        This is a private function.\n",
    "        Note that you should NOT preprocess the state here.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"You need to override the \"\n",
    "                                  \"Trainer.compute_values() function.\")\n",
    "\n",
    "    def compute_action(self, processed_state, eps=None):\n",
    "        \"\"\"Compute the action given the state. Note that the input\n",
    "        is the processed state.\"\"\"\n",
    "\n",
    "        values = self.compute_values(processed_state)\n",
    "        assert values.ndim == 1, values.shape\n",
    "        if eps is None:\n",
    "            eps = self.eps\n",
    "\n",
    "        # Implement the epsilon-greedy policy here. We have `eps`\n",
    "        #  probability to choose a uniformly random action in action_space,\n",
    "        #  otherwise choose action that maximizes the values.\n",
    "        # Hint: Use the function of self.env.action_space to sample random\n",
    "        # action.\n",
    "        if np.random.uniform(0,1)< eps:\n",
    "            action=self.env.action_sample()\n",
    "        else:\n",
    "            action=np.argmax(values)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def evaluate(self, num_episodes=50, *args, **kwargs):\n",
    "        \"\"\"Use the function you write to evaluate current policy.\n",
    "        Return the mean episode reward of 50 episodes.\"\"\"\n",
    "        policy = lambda raw_state: self.compute_action(\n",
    "            self.process_state(raw_state), eps=0.0)\n",
    "        result = evaluate(policy, num_episodes, *args, **kwargs)\n",
    "        return result\n",
    "\n",
    "    def compute_gradient(self, *args, **kwargs):\n",
    "        \"\"\"Compute the gradient.\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"You need to override the Trainer.compute_gradient() function.\")\n",
    "\n",
    "    def apply_gradient(self, *args, **kwargs):\n",
    "        \"\"\"Compute the gradient\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"You need to override the Trainer.apply_gradient() function.\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        raise NotImplementedError(\"You need to override the \"\n",
    "                                  \"Trainer.train() function.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode reward for a random policy in 500 episodes:  0.601180942355749\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "class TestTrainer(AbstractTrainer):\n",
    "    \"\"\"This class is used for testing. We don't really train anything.\"\"\"\n",
    "    def compute_values(self, state):\n",
    "        return np.random.random_sample(size=self.act_dim)\n",
    "    def initialize_parameters(self):\n",
    "        self.parameters = np.random.random_sample(size=(self.obs_dim, self.act_dim))\n",
    "    \n",
    "t = TestTrainer(environment_config)\n",
    "obs = np.random.random_sample(size=2)\n",
    "\n",
    "processed = t.process_state(obs)\n",
    "assert np.all(processed == obs)\n",
    "# Test compute_action\n",
    "values = t.compute_values(processed)\n",
    "correct_act = np.argmax(values)\n",
    "#assert t.compute_action(processed, eps=0) == correct_act\n",
    "\n",
    "print(\"Average episode reward for a random policy in 500 episodes: \",\n",
    "      t.evaluate(num_episodes=500))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Linear function approximation\n",
    "\n",
    "In this section, we implement a simple linear function whose input is the state (or the processed state) and output is the state-action values.\n",
    "\n",
    "First, we implement a `LinearTrainer` class which implements (1). Linear function approximation and (2). n-step semi-gradient method to update the linear function.\n",
    "\n",
    "Then we further implement a `LinearTrainerWithFeatureConstruction` class which processes the input state and provide polynomial features which increase the utility of linear function approximation.\n",
    "\n",
    "We refer the Chapter 9.4 (linear method), 9.5 (feature construction), and 10.2 (n-step semi-gradient method) of the RL textbook to you.\n",
    "\n",
    "In this section, we leverage the n-step semi-gradient Sarsa as the training algorithm. What is the \"correct value\" of an action $a_t$ at state $s_t$ in one-step case? We consider it is $r_t + \\gamma Q(s_{t+1}, a_{t+1})$ and thus lead to the TD error $TD = r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)$. In n-step case, the target value of Q is natually extended:\n",
    "\n",
    "$$Q(s_t, a_t) = \\sum_{i=t}^{t+n-1}\\gamma^{i-t}r_i + \\gamma^n Q(s_{t+n}, a_{t+n})$$\n",
    "\n",
    "We follow the pipeline depicted in Chapter 10.2 (page 247) of the textbook to implement this logic. Note that notation of the time step of reward is different in this assignment and in the textbook. In textbook, the reward $R_{t+1}$ is the reward when apply action $a_{t}$ to the environment at state $s_t$. In the equation above the $r_t$ has exactly the same meaning. In the code below, we store the states, actions and rewards to a list during training. You need to make sure the indices of these list, like the `tau` in  `actions[tau]` has the correct meaning.\n",
    "\n",
    "After computing the target Q value, we need to derive the gradient to update the parameters. Consider a loss function, the Mean Square Error between the target Q value and the output Q value: \n",
    "\n",
    "$$\\text{loss} = \\cfrac{1}{2}[\\sum_{i=t}^{t+n-1}\\gamma^{i-t}r_i + \\gamma^n Q(s_{t+n}, a_{t+n}) - Q(s_t, a_t)]^2$$\n",
    "\n",
    "Compute the gradient of Loss with respect to the Q function:\n",
    "\n",
    "$$\\cfrac{d \\text{loss}}{d Q} = -(\\sum_{i=t}^{t+n-1}\\gamma^{i-t}r_i + \\gamma^n Q(s_{t+n}, a_{t+n}) - Q(s_t, a_t))$$\n",
    "\n",
    "According to the chain rule, the gradient of the loss w.r.t. the parameter ($W$) is:\n",
    "\n",
    "$$\\cfrac{d \\text{loss}}{d W} = -(\\sum_{i=t}^{t+n-1}\\gamma^{i-t}r_i + \\gamma^n Q(s_{t+n}, a_{t+n}) - Q(s_t, a_t))\\cfrac{d Q}{d W}$$\n",
    "\n",
    "To minimize the loss, we only need to descent the gradient:\n",
    "\n",
    "$$W = W - lr \\cfrac{d \\text{loss}}{d W}$$\n",
    "\n",
    "wherein $lr$ is the learning rate. Therefore, in conclusion the update rule of parameters is:\n",
    "\n",
    "$$W = W + lr (\\sum_{i=t}^{t+n-1}\\gamma^{i-t}r_i + \\gamma^n Q(s_{t+n}, a_{t+n}) - Q(s_t, a_t))\\cfrac{d Q}{d W}$$\n",
    "\n",
    "In the following codes, we denote $G = \\sum_{i=t}^{t+n-1}\\gamma^{i-t}r_i + \\gamma^n Q(s_{t+n}, a_{t+n})$ and will compute $dQ / dW$ according to the form of the approximator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1: Basics\n",
    "\n",
    "(30 / 100 points)\n",
    "\n",
    "We want to approximate the state-action values. That is, the expected return when applying action $a_t$ in state $s_t$. Linear function approximates state-action value function by the inner product between a parameter matrix $W$ and the input state vector $s$:\n",
    "\n",
    "$$v(s, W) = W^T s$$\n",
    "\n",
    "Note that $W\\in \\mathbb R^{(O, A)}$ and $s \\in \\mathbb R^{(O, 1)}$, wherein O is the observation (state) dimensions, namely the `self.obs_dim` and A is the action dimension, namely the `self.act_dim`. Each action corresponding to one state-action values $Q(s, a)$.\n",
    "\n",
    "Note that you should finish this section **purely by Numpy without calling any other packages**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "# Build the algorithm-specify config.\n",
    "linear_approximator_config = merge_config(dict(\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "    n=3,\n",
    "), default_config)\n",
    "\n",
    "\n",
    "class LinearTrainer(AbstractTrainer):\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, linear_approximator_config)\n",
    "\n",
    "        # Initialize the abstract class.\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.max_episode_length = self.config[\"max_episode_length\"]\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.gamma = self.config[\"gamma\"]\n",
    "        self.n = self.config[\"n\"]\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        #  Initialize self.parameters, which is a two dimensional matrix,\n",
    "        #  and subjects to a normal distribution with scale\n",
    "        #  config[\"parameter_std\"].\n",
    "        \n",
    "        std = self.config[\"parameter_std\"]\n",
    "        self.parameters = np.random.normal(0,std,(self.obs_dim, self.act_dim))\n",
    "        \n",
    "        print(\"Initialize parameters with shape: {}.\".format(\n",
    "            self.parameters.shape))\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        assert processed_state.ndim == 1, processed_state.shape\n",
    "        \n",
    "        # Compute the value for each potential action. Note that you\n",
    "        #  should NOT preprocess the state here.\"\"\"\n",
    "\n",
    "        values = np.dot(self.parameters.T,processed_state)\n",
    "        #print(processed_state)\n",
    "        #print(values)\n",
    "        return values\n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Please implement the n-step Sarsa algorithm presented in Chapter 10.2\n",
    "        of the textbook. You algorithm should reduce the convention one-step\n",
    "        Sarsa when n = 1. That is:\n",
    "            TD = r_t + gamma * Q(s_t+1, a_t+1) - Q(s_t, a_t)\n",
    "            Q(s_t, a_t) = Q(s_t, a_t) + learning_rate * TD\n",
    "        \"\"\"\n",
    "        s = self.env.reset()\n",
    "        processed_s = self.process_state(s)\n",
    "        processed_states = [processed_s]\n",
    "        rewards = [0.0]\n",
    "        action=self.compute_action(processed_s)\n",
    "        actions = [action]\n",
    "        T = float(\"inf\")\n",
    "\n",
    "        for t in range(self.max_episode_length):\n",
    "            if t < T:\n",
    "                #  When the termination is not reach, apply action,\n",
    "                #  process state, record state / reward / action to the\n",
    "                #  lists defined above, and deal with termination.\n",
    "                \n",
    "                next_state,reward,done=self.env.step(action)\n",
    "                processed_s=self.process_state(next_state)\n",
    "                processed_states.append(processed_s)\n",
    "                rewards.append(reward)           \n",
    "            \n",
    "                if done:\n",
    "                    T=t+1\n",
    "                else:\n",
    "                    action=self.compute_action(processed_s)\n",
    "                    actions.append(action)\n",
    "\n",
    "            tau = t - self.n + 1\n",
    "            if tau >= 0:\n",
    "                gradient = self.compute_gradient(\n",
    "                    processed_states, actions, rewards, tau, T\n",
    "                )\n",
    "                self.apply_gradient(gradient)\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "\n",
    "    def compute_gradient(self, processed_states, actions, rewards, tau, T):\n",
    "        \"\"\"Compute the gradient\"\"\"\n",
    "        n = self.n\n",
    "\n",
    "        # Compute the approximation goal, the truth state action value\n",
    "        #  G. It is a n-step discounted sum of rewards. Refer to Chapter 10.2\n",
    "        #  of the textbook.\n",
    "        G = 0\n",
    "        for i in range(tau+1,min(T,tau+n)+1):\n",
    "            G+=np.power(self.gamma,i-tau-1)*rewards[i]\n",
    "    \n",
    "        if tau + n < T:   \n",
    "            q_values=self.compute_values(processed_states[tau+n])\n",
    "            G+=np.power(self.gamma,n)*q_values[actions[tau+n]]\n",
    "            \n",
    "        \n",
    "        # Denote the state-action value function Q, then the loss of\n",
    "        # prediction error w.r.t. the weights can be separated into two\n",
    "        # parts (the chain rule):\n",
    "        #     dLoss / dweight = (dLoss / dQ) * (dQ / dweight)\n",
    "        # We call the first one loss_grad, and the latter one\n",
    "        # value_grad. We consider the Mean Square Error between the target\n",
    "        # value (G) and the predicted value (Q(s_t, a_t)) to be the loss.\n",
    "\n",
    "        \n",
    "        q=self.compute_values(processed_states[tau])\n",
    "        act=actions[tau]\n",
    "        loss_grad = np.zeros((self.act_dim, 1))\n",
    "        # fill the propoer value of loss_grad, denoting the gradient\n",
    "        # of the MSE w.r.t. the output of the linear function.\n",
    "        # Hint: only one element of loss_grad is not zero.    \n",
    "        loss_grad[act]=-(G-q[act])\n",
    "\n",
    "        # compute the value of value_grad, denoting the gradient of\n",
    "        # the output of the linear function w.r.t. the parameters.\n",
    "        value_grad = np.zeros((self.obs_dim, 1))\n",
    "        for i in range(self.obs_dim):\n",
    "             value_grad[i]=processed_states[tau][i]\n",
    "        \n",
    "        assert loss_grad.shape == (self.act_dim, 1)\n",
    "        assert value_grad.shape == (self.obs_dim, 1)\n",
    "\n",
    "        #  merge two gradients to get the gradient of loss w.r.t. the\n",
    "        # parameters.\n",
    "        gradient = np.dot(value_grad,loss_grad.T)\n",
    "        return gradient\n",
    "\n",
    "    def apply_gradient(self, gradient):\n",
    "        \"\"\"Apply the gradient to the parameter.\"\"\"\n",
    "        assert gradient.shape == self.parameters.shape, (\n",
    "            gradient.shape, self.parameters.shape)\n",
    "        # apply the gradient to self.parameters\n",
    "        #print(self.parameters)\n",
    "        self.parameters-= self.learning_rate*gradient\n",
    "        #print(gradient)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2: Linear Model with Feature Construction\n",
    "\n",
    "(15 / 100 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "linear_fc_config = merge_config(dict(\n",
    "    polynomial_order=1,\n",
    "), linear_approximator_config)\n",
    "\n",
    "\n",
    "def polynomial_feature(sequence, order=1):\n",
    "    \"\"\"Construct the order-n polynomial-basis feature of the state.\n",
    "    Refer to Chapter 9.5.1 of the textbook. We expect to get a \n",
    "    vector of length (n+1)^k as the output.\n",
    "    Example:\n",
    "    When the state is [2, 3, 4], the first order polynomial feature\n",
    "    of the state is [\n",
    "        1,\n",
    "        2,\n",
    "        3,\n",
    "        4,\n",
    "        2 * 3 = 6,\n",
    "        2 * 4 = 8,\n",
    "        3 * 4 = 12,\n",
    "        2 * 3 * 4 = 24\n",
    "    ]\n",
    "    It's OK for function polynomial() to return values in different order.\n",
    "    \"\"\"\n",
    "    # finish this function.\n",
    "    s=sequence[0]\n",
    "    features=[s**n for n in range(order+1)]  \n",
    "    for i in range(len(sequence)-1):\n",
    "        s=sequence[i+1]\n",
    "        s_l=[s**n for n in range(order+1)]\n",
    "        tmp=np.array([features])\n",
    "        s_l=np.array([s_l])\n",
    "        features=np.dot(tmp.T,s_l)         \n",
    "        features=np.hstack(features).tolist()\n",
    "        \n",
    "    features= np.array(features).squeeze()\n",
    "    \n",
    "    return features\n",
    "    \n",
    "    \n",
    "\n",
    "assert sorted(polynomial_feature([2, 3, 4])) == [1, 2, 3, 4, 6, 8, 12, 24]\n",
    "assert len(polynomial_feature([2, 3, 4], 2)) == 27\n",
    "assert len(polynomial_feature([2, 3, 4], 3)) == 64\n",
    "\n",
    "class LinearTrainerWithFeatureConstruction(LinearTrainer):\n",
    "    \"\"\"In this class, we will expand the dimension of the state.\n",
    "    This procedure is done at self.process_state function.\n",
    "    The modification of self.obs_dim and the shape of parameters\n",
    "    is also needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, linear_fc_config)\n",
    "        # Initialize the abstract class.\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.polynomial_order = self.config[\"polynomial_order\"]\n",
    "\n",
    "        # Expand the size of observation\n",
    "        self.obs_dim = (self.polynomial_order + 1) ** self.obs_dim\n",
    "\n",
    "        # Since we change self.obs_dim, reset the parameters.\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def process_state(self, state):\n",
    "        \"\"\"Please finish the polynomial function.\"\"\"\n",
    "        processed = polynomial_feature(state, self.polynomial_order)\n",
    "        processed = np.asarray(processed)\n",
    "        assert len(processed) == self.obs_dim, processed.shape\n",
    "        return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Multi-layer Perceptron as the approximiator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are required to implement a single agent MLP using purely Numpy package. The differences between MLP and linear function are (1). MLP has a hidden layer which increase its representation capacity (2). MLP can leverage activation function after the output of each layer which introduce not linearity.\n",
    "\n",
    "Consider a MLP with one hidden layer containing 100 neurons and activation function `f()`. We call the layer that accepts the state as input and output the activation **hidden layer**, and the layer that accepts the activation as input and produces the values **output layer**. The activation of the hidden layer is:\n",
    "\n",
    "$$a(s_t) = f( W_h^T s_t)$$\n",
    "\n",
    "obvious the activation is a 100-length vector. The output values is:\n",
    "\n",
    "$$Q(s_t) = f(W_o ^ T a(s_t))$$\n",
    "\n",
    "wherein $W_h, W_o$ are the parameters of hidden layer and output layer, respectively. In this section we do not add activation function and hence $f(x) = x$.\n",
    "\n",
    "Moreover, we also introduce the gradient clipping mechanism. In on-policy learning, the norm of gradient is prone to vary drastically, since the output of Q function is unbounded and it can be as large as possible, which leads to exploding gradient issue. Gradient clipping is used to bound the norm of gradient while keeps the direction of gradient vector unchanged. Concretely, the formulation of gradient clipping is:\n",
    "\n",
    "$$g_{clipped} = g_{original} \\cfrac{c}{\\max(c, \\text{norm}(g))}$$\n",
    "\n",
    "wherein $c$ is a hyperparameter which is `config[\"clip_norm\"]` in our implementation. Gradient clipping bounds the gradient norm to $c$ if the norm of original gradient is greater than $c$. You need to implement this mechanism in function `apply_gradient` in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "# Build the algorithm-specify config.\n",
    "mlp_trainer_config = merge_config(dict(\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "    hidden_dim=100,\n",
    "    n=3,\n",
    "    clip_norm=1.0,\n",
    "    clip_gradient=True\n",
    "), default_config)\n",
    "\n",
    "\n",
    "class MLPTrainer(LinearTrainer):\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, mlp_trainer_config)\n",
    "        self.hidden_dim = config[\"hidden_dim\"]\n",
    "        super().__init__(config)\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        #  Initialize self.hidden_parameters and self.output_parameters,\n",
    "        #  which are two dimensional matrices, and subject to normal\n",
    "        #  distributions with scale config[\"parameter_std\"]\n",
    "        std = self.config[\"parameter_std\"]\n",
    "        self.hidden_parameters = np.random.normal(0,std,(self.obs_dim, self.hidden_dim))\n",
    "        self.output_parameters = np.random.normal(0,std,(self.hidden_dim, self.act_dim))\n",
    "\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        \"\"\"Compute the value for each potential action. Note that you\n",
    "        should NOT preprocess the state here.\"\"\"\n",
    "        assert processed_state.ndim == 1, processed_state.shape\n",
    "        activation = self.compute_activation(processed_state)\n",
    "        values = np.dot(self.output_parameters.T,activation)\n",
    "        return values\n",
    "\n",
    "    def compute_activation(self, processed_state):\n",
    "        \"\"\"Given a processed state, first we need to compute the activtaion\n",
    "        (the output of hidden layer). Then we compute the values (the output of\n",
    "        the output layer).\n",
    "        \"\"\"\n",
    "        activation = np.dot(self.hidden_parameters.T,processed_state)\n",
    "        return activation\n",
    "\n",
    "    def compute_gradient(self, processed_states, actions, rewards, tau, T):\n",
    "        n = self.n\n",
    "        \n",
    "        # compute the target value.\n",
    "        # Hint: copy your codes in LinearTrainer.\n",
    "        G = 0\n",
    "        for i in range(tau+1,min(T,tau+n)+1):\n",
    "            G+=np.power(self.gamma,i-tau-1)*rewards[i]\n",
    "    \n",
    "    \n",
    "        if tau + n < T:\n",
    "            # Hint: Since we use Sarsa algorithm here,\n",
    "            #  the Q value of time tau+n is the Q value of action\n",
    "            #  in time tau+n. So you should take the tau+n element of\n",
    "            #  processed_states as input to compute the Q values\n",
    "            #  and then take the \"actions[tau+n]\" as the index to get\n",
    "            #  the Q value in tau+n.\n",
    "            Q_tau_plus_n = self.compute_values(processed_states[tau+n])\n",
    "            act=actions[tau+n]  \n",
    "            G = G + (self.gamma ** n) * Q_tau_plus_n[act]\n",
    " \n",
    "        # Denote the state-action value function Q, then the loss of\n",
    "        # prediction error w.r.t. the output layer weights can be \n",
    "        # separated into two parts (the chain rule):\n",
    "        #     dError / dweight = (dError / dQ) * (dQ / dweight)\n",
    "        # We call the first one loss_grad, and the latter one\n",
    "        # value_grad. We consider the Mean Square Error between the target\n",
    "        # value (G) and the predict value (Q(s_t, a_t)) to be the loss.\n",
    "        cur_state = processed_states[tau]\n",
    "        \n",
    "\n",
    "        loss_grad = np.zeros((self.act_dim, 1))  # [act_dim, 1]\n",
    "        # compute loss_grad\n",
    "        q=self.compute_values(cur_state)\n",
    "        act=actions[tau]\n",
    "        loss_grad[act]=-(G-q[act])\n",
    "\n",
    "        # compute the gradient of output layer parameters\n",
    "        # loss_grad* activation\n",
    "        activation=self.compute_activation(cur_state)\n",
    "        activation=np.array([activation])\n",
    "        output_gradient = np.dot(activation.T,loss_grad.T)\n",
    "        \n",
    "        # compute the gradient of hidden layer parameters\n",
    "        # Hint: using chain rule and derive the formulation\n",
    "        #loss_grad *\n",
    "        \n",
    "        cur_state=np.array([cur_state])\n",
    "        hidden_gradient = cur_state.T@loss_grad.T@self.output_parameters.T\n",
    "        #self.output_parameters@loss_grad@cur_state.T\n",
    "  \n",
    "    \n",
    "        assert np.all(np.isfinite(output_gradient)), \\\n",
    "            \"Invalid value occurs in output_gradient! {}\".format(\n",
    "                output_gradient)\n",
    "        assert np.all(np.isfinite(hidden_gradient)), \\\n",
    "            \"Invalid value occurs in hidden_gradient! {}\".format(\n",
    "                hidden_gradient)\n",
    "        return [hidden_gradient, output_gradient]\n",
    "\n",
    "    def apply_gradient(self, gradients):\n",
    "        \"\"\"Apply the gradientss to the two layers' parameters.\"\"\"\n",
    "        assert len(gradients) == 2\n",
    "        hidden_gradient, output_gradient = gradients\n",
    "\n",
    "        assert output_gradient.shape == (self.hidden_dim, self.act_dim)\n",
    "        assert hidden_gradient.shape == (self.obs_dim, self.hidden_dim)\n",
    "        \n",
    "        # Implement the clip gradient mechansim\n",
    "        # Hint: when the old gradient has norm less that clip_norm,\n",
    "        #  then nothing happens. Otherwise shrink the gradient to\n",
    "        #  make its norm equal to clip_norm.\n",
    "        if self.config[\"clip_gradient\"]:\n",
    "            clip_norm = self.config[\"clip_norm\"]\n",
    "            output_gradient=output_gradient*clip_norm/max(clip_norm, np.linalg.norm(output_gradient))\n",
    "            hidden_gradient=hidden_gradient*clip_norm/max(clip_norm, np.linalg.norm(hidden_gradient))\n",
    "\n",
    "        #  update the parameters\n",
    "        # Hint: Remember to check the sign when applying the gradient\n",
    "        #  into the parameters. Should you add or minus the gradients?      \n",
    "        self.output_parameters-= self.learning_rate*output_gradient\n",
    "        self.hidden_parameters-= self.learning_rate*hidden_gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Implement Deep Q Learning in Pytorch\n",
    "\n",
    "(50 / 100 points)\n",
    "\n",
    "In this section, you will get familiar with the basic logic of pytorch, which lay the ground for further learning. We will implement a MLP similar to the one in Section 3 using Pytorch, a powerful Deep Learning framework. Before start, you need to make sure using `pip install torch` to install it.\n",
    "\n",
    "If you are not familiar with Pytorch, we suggest you to go through pytorch official quickstart tutorials:\n",
    "1. [quickstart](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "2. [tutorial on RL](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    "\n",
    "Different from the algorithm in Section 3, we will implement Deep Q Network (DQN) in this section. The main differences are concluded as following:\n",
    "\n",
    "**DQN requires an experience replay memory to store the transitions.** A replay memory is implemented in the following `ExperienceReplayMemory` class. It can contain a certain amount of transitions: `(s_t, a_t, r_t, s_t+1, done_t)`. When the memory is full, the earliest transition is discarded to store the latest one.\n",
    "\n",
    "The introduction of replay memory increase the sample efficiency (since each transition might be used multiple times) when solving complex task, though you may find it learn slowly in this assignment since the CartPole-v0 is a relatively easy environment.\n",
    "\n",
    "\n",
    "**DQN is an off-policy algorithm and has difference when computing TD error, compared to Sarsa.** In Sarsa, the TD error is computed as: \n",
    "\n",
    "$$(r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$$ \n",
    "\n",
    "wherein the next action $a_{t+1}$ is the one the policy selects. However, in traditional Q learning, it assume the next action is the one that maximizes the action values and use this assumption to compute the TD: \n",
    "\n",
    "$$(r_t + \\gamma \\max_{a_{t+1}} Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$$\n",
    "\n",
    "**DQN has make delayed update target network, which is another difference even compared to the traditional Q learning.** DQN maintains another neural network called target network that has identical structure of the Q network. After a certain amount of steps has been taken, the target network copies the parameters of the Q network to itself. Normally, the update of target network is much less frequent than the update of the Q network. The Q network is updated in each step.\n",
    "\n",
    "The reason to leverage the target network is to stabilize the estimation of TD error. In DQN, the TD error is evaluated as:\n",
    "\n",
    "$$(r_t + \\gamma \\max_{a_{t+1}} Q^{target}(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$$\n",
    "\n",
    "The Q values of next state is estimated by the target network, not the Q network that is updating. This mechanism can reduce the variance of gradient because the estimation of Q values of next states is not influenced by the update of the Q network.\n",
    "\n",
    "In the engineering aspect, the differences between `DQNTrainer` and the previous `MLPTrainer` are:\n",
    "\n",
    "1. DQN uses pytorch model to serve as the approximator. So we need to rewrite the `initialize_parameter` function to build the pytorch model. Also the `train` function is changed since the gradient optimization is conducted by pytorch, therefore we need to write the pytorch pipeline in `train`.\n",
    "2. DQN has replay memory. So we need to initialize it, feed data into it and take the transitions out.\n",
    "3. Thank to the replay memory and pytorch, DQN can be updated in a batch. So you need to carefully compute the Q target via matrix computation.\n",
    "4. We use Adam optimizer to conduct the gradient optimization. You need to get familiar with how to compute the loss and conduct backward propagation.\n",
    "\n",
    "\\* Note that in this assignment, we use purely CPU to train the agent. You do not need to right your pytorch code that compatible with GPU and also do not need to run your code in a GPU machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ExperienceReplayMemory:\n",
    "    \"\"\"Store and sample the transitions\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        # deque is a useful class which acts like a list but only contain\n",
    "        # finite elements.When appending new element make deque exceeds the \n",
    "        # `maxlen`, the oldest element (the index 0 element) will be removed.\n",
    "        \n",
    "        #  uncomment next line. \n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "class PytorchModel(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(PytorchModel, self).__init__()\n",
    "        \n",
    "        # [TODO] Build a sequential model with two hidden layers.\n",
    "        # The first hidden layer has 100 hidden units, followed by\n",
    "        # a ReLU activation function.\n",
    "        # The second output layer take the activation vector as input\n",
    "        # and pass through another 100 hidden units, also followed\n",
    "        # by a ReLU activation function.\n",
    "        # Then the output 100-length vector passes the output layer\n",
    "        # and output the values of all actions.\n",
    "        # \n",
    "        #     input (length: obs_dim) \n",
    "        # ->  100 units hidden layer\n",
    "        # ->  ReLU \n",
    "        # ->  100 units hidden layer\n",
    "        # ->  ReLU\n",
    "        # ->  act_dim units hidden layer\n",
    "        # -> output (length: act_dim)\n",
    "        \n",
    "        self.action_value = torch.nn.Sequential(\n",
    "            torch.nn.Linear(obs_dim,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,act_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.action_value(obs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "pytorch_config = merge_config(dict(\n",
    "    memory_size=50000,\n",
    "    learn_start=5000,\n",
    "    batch_size=32,\n",
    "    target_update_freq=500,  # in steps\n",
    "    learn_freq=1,  # in steps\n",
    "    n=1\n",
    "), mlp_trainer_config)\n",
    "\n",
    "\n",
    "def to_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.float32)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    if x.dim() == 3 or x.dim() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    assert x.dim() == 2 or x.dim() == 4, x.shape\n",
    "    return x\n",
    "\n",
    "def to_long_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.long)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    if x.dim() == 3 or x.dim() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    assert x.dim() == 2 or x.dim() == 4, x.shape\n",
    "    return x\n",
    "\n",
    "\n",
    "class DQNTrainer(MLPTrainer):\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, pytorch_config)\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.memory = ExperienceReplayMemory(config[\"memory_size\"])\n",
    "        self.learn_start = config[\"learn_start\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.target_update_freq = config[\"target_update_freq\"]\n",
    "        self.clip_norm = config[\"clip_norm\"]\n",
    "        self.step_since_update = 0\n",
    "        self.total_step = 0\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"Initialize the pytorch model as the Q network and the target network\"\"\"\n",
    "        # Initialize two network using PytorchModel class\n",
    "        self.network = PytorchModel(self.obs_dim,self.act_dim)\n",
    "\n",
    "        self.network.eval()\n",
    "        self.network.share_memory()\n",
    "\n",
    "        # Initialize target network, which is identical to self.network,\n",
    "        # and should have the same weights with self.network. So you should\n",
    "        # put the weights of self.network into self.target_network.\n",
    "        self.target_network = PytorchModel(self.obs_dim,self.act_dim)\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "\n",
    "        self.target_network.eval()\n",
    "\n",
    "        # Build Adam optimizer and MSE Loss.\n",
    "        # Uncomment next few lines\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.network.parameters(), lr=self.learning_rate\n",
    "        )\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        \"\"\"Compute the value for each potential action. Note that you\n",
    "        should NOT preprocess the state here.\"\"\"\n",
    "        # Convert the output of neural network to numpy array\n",
    "        values=self.network(processed_state)\n",
    "        return values.data.numpy()\n",
    "\n",
    "    def train(self):\n",
    "        s = self.env.reset()\n",
    "        processed_s = self.process_state(s)\n",
    "        act = self.compute_action(processed_s)\n",
    "        stat = {\"loss\": []}\n",
    "\n",
    "        for t in range(self.max_episode_length):\n",
    "            next_state, reward, done = self.env.step(act)\n",
    "            next_processed_s = self.process_state(next_state)\n",
    "\n",
    "            # Push the transition into memory.\n",
    "            self.memory.push(\n",
    "                (processed_s, act, reward, next_processed_s, done)\n",
    "            )\n",
    "\n",
    "            processed_s = next_processed_s\n",
    "            act = self.compute_action(next_processed_s)\n",
    "            self.step_since_update += 1\n",
    "            self.total_step += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            if t % self.config[\"learn_freq\"] != 0:\n",
    "                # It's not necessary to update in each step.\n",
    "                continue\n",
    "\n",
    "            if len(self.memory) < self.learn_start:\n",
    "                continue\n",
    "            elif len(self.memory) == self.learn_start:\n",
    "                print(\"Current memory contains {} transitions, \"\n",
    "                      \"start learning!\".format(self.learn_start))\n",
    "\n",
    "            batch = self.memory.sample(self.batch_size)\n",
    "\n",
    "            # Transform a batch of state / action / .. into a tensor.\n",
    "            state_batch = to_tensor(\n",
    "                np.stack([transition[0] for transition in batch])\n",
    "            )\n",
    "            #action_batch = to_tensor(\n",
    "            #    np.stack([transition[1] for transition in batch])\n",
    "            #)\n",
    "            #action_batch = np.stack([transition[1] for transition in batch])\n",
    "            #action_batch=torch.from_numpy(action_batch).type(torch.long)\n",
    "            action_batch = to_long_tensor(\n",
    "                np.stack([transition[1] for transition in batch])\n",
    "            )\n",
    "            reward_batch = to_tensor(\n",
    "                np.stack([transition[2] for transition in batch])\n",
    "            )\n",
    "            next_state_batch = torch.stack(\n",
    "                [transition[3] for transition in batch]\n",
    "            )\n",
    "            done_batch = to_tensor(\n",
    "                np.stack([transition[4] for transition in batch])\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Compute the values of Q in next state in batch.\n",
    "                # Hint: \n",
    "                #  1. Q_t_plus_one is the maximum value of Q values of possible\n",
    "                #     actions in next state. So the input to the network is \n",
    "                #     next_state_batch.\n",
    "                #  2. Q_t_plus_one is computed using the target network.\n",
    "                \n",
    "                next_state_values=self.target_network(next_state_batch)\n",
    "                Q_t_plus_one = next_state_values.max(1)[0].detach()\n",
    "                \n",
    "                \n",
    "                assert isinstance(Q_t_plus_one, torch.Tensor)\n",
    "                assert Q_t_plus_one.dim() == 1\n",
    "                \n",
    "                # Compute the target value of Q in batch.\n",
    "                # Hint: The Q target is simply r_t + gamma * Q_t+1 \n",
    "                #  IF the episode is not done at time t.\n",
    "                #  That is, the (gamma*Q_t+1) term should be masked out\n",
    "                #  if done_batch[t] is True.\n",
    "                #  A smart way to do so is: using (1-done_batch) as multiplier\n",
    "                Q_target = reward_batch+(1-done_batch)*Q_t_plus_one\n",
    "                \n",
    "                \n",
    "                #assert Q_target.shape == (self.batch_size,)\n",
    "            \n",
    "            # Collect the Q values in batch.\n",
    "            # Hint: Remember to call self.network.train()\n",
    "            #  before you get the Q value from self.network(state_batch),\n",
    "            #  otherwise the graident will not be recorded by pytorch.\n",
    "            self.network.train()\n",
    "            \n",
    "            state_action_values=self.network(state_batch)\n",
    "            \n",
    "            Q_t = torch.t(state_action_values).gather(0,action_batch)\n",
    "    \n",
    "            assert Q_t.shape == Q_target.shape\n",
    "\n",
    "            # Update the network\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss(input=Q_t, target=Q_target)\n",
    "            loss_value = loss.item()\n",
    "            stat['loss'].append(loss_value)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping. Uncomment next line\n",
    "            nn.utils.clip_grad_norm_(self.network.parameters(), self.clip_norm)   \n",
    "            self.optimizer.step()\n",
    "            self.network.eval()\n",
    "\n",
    "        if len(self.memory) >= self.learn_start and \\\n",
    "                self.step_since_update > self.target_update_freq:\n",
    "            #print(\"{} steps has passed since last update. Now update the\"\n",
    "            #      \" parameter of the behavior policy. Current step: {}\".format(\n",
    "            #    self.step_since_update, self.total_step\n",
    "            #))\n",
    "            self.step_since_update = 0\n",
    "            # Copy the weights of self.network to self.target_network.\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "            \n",
    "            self.target_network.eval()\n",
    "            \n",
    "        return {\"loss\": np.mean(stat[\"loss\"]), \"episode_len\": t}\n",
    "\n",
    "    def process_state(self, state):\n",
    "        return torch.from_numpy(state).type(torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5s,+0.5s)\tIteration 0, current mean episode reward is 0.8109354832422612. {'loss': nan, 'episode_len': 49.0}\n",
      "(8.3s,+7.8s)\tIteration 100, current mean episode reward is 0.8496104274114584. {'loss': 0.0004, 'episode_len': 49.0}\n",
      "(17.6s,+9.2s)\tIteration 200, current mean episode reward is 1.2097326321486788. {'loss': 0.0012, 'episode_len': 49.0}\n",
      "In 200 iteration, current mean episode reward 1.210 is greater than reward threshold 1. Congratulation! Now we exit the training process.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "config=merge_config(environment_config, dict(\n",
    "    max_iteration=10000,\n",
    "    evaluate_interval=100, \n",
    "    learning_rate=0.0001,\n",
    "    clip_norm=10.0,\n",
    "    memory_size=50000,\n",
    "    learn_start=1000,\n",
    "    eps=0.03,\n",
    "    target_update_freq=1000,\n",
    "    batch_size=32,\n",
    "))\n",
    "pytorch_trainer, pytorch_stat = run(DQNTrainer, config,reward_threshold=1) #,reward_threshold=2\n",
    "\n",
    "reward = pytorch_trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space: (0, 1, 2, 3), total_steps: 50, current_step: 50, random_seed: 10, map: {'width': 1000, 'length': 1000, 'height': 100}, UAV_speed: 20, UAV_initial_pos: [0 0], inital_state: 0, UAV_current_pos: [600  40], number_of_user: 10, users_pos: [(585, 33), (439, 494), (591, 15), (211, 473), (832, 503), (843, 284), (669, 830), (164, 35), (533, 501), (335, 77)], g0: 1e-05, B: 1000000, Pk: 0.1, noise: 1e-09\n",
      "Current Step: 50\n",
      "Action: 1\n",
      "UAV current position x: 600, y: 40\n",
      "Current step reward: 0.032262442321572306, episodes rewards: 1.2097326321486785\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode reward for your Pytorch agent:  1.2097326321486785\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Average episode reward for your Pytorch agent: \",\n",
    "      pytorch_trainer.evaluate(1, render=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
