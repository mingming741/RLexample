{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "from utils import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from random import randint, choice\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAVEnvironment():\n",
    "    \"\"\"\n",
    "    Game environment for UAV test\n",
    "    \n",
    "    ---Map---\n",
    "    \n",
    "    y-axis(length)\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "     _______________________ x-axis(width)\n",
    "     \n",
    "    Hight is a fixed value\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        # Game config\n",
    "        self.action_space = (0, 1, 2, 3) # up, right, down, left, total 4 actions\n",
    "        self.total_steps = config[\"total_steps\"] # when the game end\n",
    "        self.current_step = 0\n",
    "        if config[\"is_random_env\"] == False:\n",
    "            self.random_seed = config[\"random_seed\"]\n",
    "            random.seed(self.random_seed)\n",
    "        \n",
    "        # Map config\n",
    "        self.map = dict(width=config[\"map\"][\"width\"], length=config[\"map\"][\"length\"], height=config[\"map\"][\"height\"])\n",
    "        self.UAV_speed = config[\"UAV_speed\"]\n",
    "        self.UAV_initial_pos = config[\"UAV_initial_pos\"] # a tuple\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        self.number_of_user = config[\"number_of_user\"]\n",
    "        self.users_pos = list()\n",
    "        self.UAV_path = [] # record the path of UAV\n",
    "        for i in range(0, self.number_of_user):\n",
    "            self.users_pos.append((randint(0, self.map[\"width\"]), randint(0, self.map[\"length\"])))\n",
    "        \n",
    "        # Wireless config\n",
    "        self.g0 = config[\"wireless_parameter\"][\"g0\"]\n",
    "        self.B = config[\"wireless_parameter\"][\"B\"]\n",
    "        self.Pk = config[\"wireless_parameter\"][\"Pk\"]\n",
    "        self.noise = config[\"wireless_parameter\"][\"noise\"]\n",
    "        \n",
    "    def get_reward(self, UAV_pos):\n",
    "        # One step Reward is define as the summation of all user's utility\n",
    "        reward = 0\n",
    "        for user_index in range(0, self.number_of_user):\n",
    "            gkm = self.g0 / (self.map[\"height\"] ** 2 + (UAV_pos[0] - self.users_pos[user_index][0]) ** 2 + (UAV_pos[1] - self.users_pos[user_index][1]) ** 2)\n",
    "            #user_utility = self.B * math.log(1 + self.Pk * gkm / self.noise, 2)\n",
    "            user_utility = (self.B/self.number_of_user)* math.log(1 + self.Pk * gkm / self.noise, 2)\n",
    "            reward = reward + user_utility\n",
    "        return reward / (10 ** 6) # Use Mkbps as signal basic unit\n",
    "    \n",
    "    def transition_dynamics(self, action, speed, state):\n",
    "        # given the action (direction), calculate the next state (UAV current position)\n",
    "        assert action in self.action_space\n",
    "        next_UAV_pos = list(state)\n",
    "        if action == 0:\n",
    "            # move up\n",
    "            next_UAV_pos[1] = min(next_UAV_pos[1] + speed, self.map[\"length\"])\n",
    "        if action == 1:\n",
    "            # move right\n",
    "            next_UAV_pos[0] = min(next_UAV_pos[0] + speed, self.map[\"width\"])\n",
    "        if action == 2:\n",
    "            # move down\n",
    "            next_UAV_pos[1] = max(next_UAV_pos[1] - speed, 0)\n",
    "        if action == 3:\n",
    "            # move left\n",
    "            next_UAV_pos[0] = max(next_UAV_pos[0] - speed, 0)\n",
    "        return tuple(next_UAV_pos)\n",
    "    \n",
    "    def get_transition(self):\n",
    "        # This function only works for model based, we are trying to disable this function to try more algorithm\n",
    "        # Return a table of transition, we assume UAV use fixed flying speed\n",
    "        \"\"\"\n",
    "        Structure:\n",
    "        transition[\n",
    "            x_0[\n",
    "                y_0[\n",
    "                    {next_state, reward}, # for action 1\n",
    "                    {next_state, reward}, # for action 2\n",
    "                    ...\n",
    "                    {next_state, reward}, # for action 20\n",
    "                ],\n",
    "                y_1*v[],\n",
    "                ...\n",
    "                y_h-1*v[]\n",
    "            ],\n",
    "            x_1*v[],\n",
    "            x_2*v[],\n",
    "            ...\n",
    "            x_w-1*v[]\n",
    "        ]\n",
    "        \"\"\"\n",
    "        transition = list()\n",
    "        for state_x in range(0, int(self.map[\"width\"] / self.UAV_speed) + 1):\n",
    "            transition.append(list())\n",
    "            for state_y in range(0, int(self.map[\"length\"] / self.UAV_speed) + 1):\n",
    "                transition[state_x].append(list())\n",
    "                for action in self.action_space:\n",
    "                    next_state = self.transition_dynamics(action, self.UAV_speed, (state_x * self.UAV_speed, state_y * self.UAV_speed))\n",
    "                    reward = self.get_reward(next_state)\n",
    "                    transition[state_x][state_y].append(dict(next_state=next_state,reward=reward))\n",
    "        return transition\n",
    "                    \n",
    "    def step(self, action, speed=-1):\n",
    "        # assume we use the max speed as the default speed, when come near to the opt-position, we can slow down the speed\n",
    "        if speed < 0 or speed >= self.UAV_speed:\n",
    "            speed = self.UAV_speed\n",
    "            \n",
    "        self.UAV_path.append(self.UAV_current_pos)\n",
    "        self.UAV_current_pos = self.transition_dynamics(action, speed, self.UAV_current_pos)\n",
    "        self.current_step = self.current_step + 1\n",
    "        done = False\n",
    "        if self.current_step == self.total_steps:\n",
    "            done =  True\n",
    "        return self.UAV_current_pos, self.get_reward(self.UAV_current_pos), done\n",
    "    \n",
    "    def action_sample(self):\n",
    "        return choice(self.action_space)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        return self.UAV_current_pos\n",
    "        \n",
    "    def print_attribute(self):\n",
    "        attrs = vars(self)\n",
    "        print(', '.join(\"%s: %s\" % item for item in attrs.items()))\n",
    "        \n",
    "    def print_locations(self):\n",
    "        print(\"UAV position is: {}\".format(self.UAV_current_pos))\n",
    "        print(\"Users position are: {}\".format(self.users_pos))\n",
    "        \n",
    "    def print_map(self):\n",
    "        x_list = [pos[0] for pos in self.users_pos]\n",
    "        y_list = [pos[1] for pos in self.users_pos]\n",
    "        x_list.append(self.UAV_current_pos[0])\n",
    "        y_list.append(self.UAV_current_pos[1])\n",
    "        for i in range(0, len(self.UAV_path)):\n",
    "            x_list.append(self.UAV_path[i][0])\n",
    "            y_list.append(self.UAV_path[i][1])\n",
    "        \n",
    "        colors = np.array([\"red\", \"green\", \"blue\"])\n",
    "        sizes = []\n",
    "        colors_map = []\n",
    "        for i in range(0, self.number_of_user):\n",
    "            sizes.append(25)\n",
    "            colors_map.append(1)\n",
    "        sizes.append(50)\n",
    "        colors_map.append(0)\n",
    "        for i in range(0, len(self.UAV_path)):\n",
    "            sizes.append(10)\n",
    "            colors_map.append(2)\n",
    "        for i in range(0, len(self.UAV_path) - 1):\n",
    "            x_values = [self.UAV_path[i][0], self.UAV_path[i+1][0]]\n",
    "            y_values = [self.UAV_path[i][1], self.UAV_path[i+1][1]]\n",
    "            plt.plot(x_values, y_values, 'b')\n",
    "        plt.scatter(x_list, y_list, c=colors[colors_map], s=sizes) \n",
    "        plt.axis([0, self.map[\"width\"], 0, self.map[\"length\"]])\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(policy, config, num_episodes=1, render=False):\n",
    "    env = UAVEnvironment(config)\n",
    "    \n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        # all policy will return a direction and a speed\n",
    "        act_direction, act_speed = policy(obs)\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            obs, reward, done = env.step(act_direction, act_speed)\n",
    "            act_direction, act_speed = policy(obs)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            if render == True:\n",
    "                clear_output(wait=True)\n",
    "                env.print_attribute()\n",
    "                print(\"Current Step: {}\".format(env.current_step))\n",
    "                print(\"Policy choice direction: {}, speed: {}\".format(act_direction, act_speed))\n",
    "                print(\"UAV current position x: {}, y: {}\".format(env.UAV_current_pos[0], env.UAV_current_pos[1]))\n",
    "                print(\"Current step reward: {}, episodes rewards: {}\".format(reward, ep_reward))\n",
    "                env.print_map()\n",
    "                wait(sleep=0.2)\n",
    "        rewards.append(ep_reward)\n",
    "    return np.mean(rewards)\n",
    "\n",
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Class for all RL algorithm\n",
    "class UAVTrainer: \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env = UAVEnvironment(self.config)\n",
    "\n",
    "# Value Iteration, Tabular, transition dynamic is known, assume only use fixed speed to reduce action space\n",
    "class UAVTrainerValueIteration(UAVTrainer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.transitions = self.env.get_transition()\n",
    "        self.q_table = []\n",
    "        self.obs_dim = (int(self.env.map[\"width\"] / self.env.UAV_speed), int(self.env.map[\"length\"] / self.env.UAV_speed))\n",
    "        self.act_dim = len(self.env.action_space)\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        \n",
    "        for x in range(0, self.obs_dim[0]+1):\n",
    "            self.q_table.append(list())\n",
    "            for y in range(0, self.obs_dim[1]+1):\n",
    "                self.q_table[x].append(0)\n",
    "            \n",
    "    def get_transition(self, state, act):\n",
    "        transition = self.transitions[state[0]][state[1]][act]\n",
    "        return transition[\"next_state\"], transition[\"reward\"]\n",
    "    \n",
    "    def print_transitions(self):\n",
    "        print(\"Transition width {}, length {}, number of act {}\".format(len(self.transitions), len(self.transitions[0]), len(self.transitions[0][0])))\n",
    "        print(self.transitions)\n",
    "        \n",
    "    def print_table(self):\n",
    "        for j in range(len(self.q_table[0])-1, -1, -1):\n",
    "            for i in range(0, len(self.q_table)):\n",
    "                print(self.q_table[i][j], end =\" \")\n",
    "            print(\"\")\n",
    "            \n",
    "            \n",
    "    def copy_current_table(self):\n",
    "        old_table = []\n",
    "        for x in range(0, self.obs_dim[0]+1):\n",
    "            old_table.append(list())\n",
    "            for y in range(0, self.obs_dim[1]+1):\n",
    "                old_table[x].append(self.q_table[x][y])\n",
    "        return old_table\n",
    "\n",
    "    def update_value_function(self):\n",
    "        old_table = self.copy_current_table()\n",
    "        for state_x in range(self.obs_dim[0] + 1):\n",
    "            for state_y in range(self.obs_dim[1] + 1):\n",
    "                state_value = 0\n",
    "                state_action_values = [0 for i in range(0, self.act_dim)]\n",
    "\n",
    "                for act in range(self.act_dim):\n",
    "                    next_state, reward = self.get_transition((state_x, state_y), act)\n",
    "                    table_x = int(next_state[0] / self.env.UAV_speed)\n",
    "                    table_y = int(next_state[1] / self.env.UAV_speed)\n",
    "                    #print(table_x, table_y)\n",
    "                    state_action_values[act] = state_action_values[act] + reward + self.gamma * old_table[table_x][table_y]   \n",
    "                state_value = np.max(state_action_values)\n",
    "                self.q_table[state_x][state_y] = state_value\n",
    "                #print(\"Update x: {}, y: {} to value {}\".format(state_x, state_y, state_value))\n",
    "            \n",
    "    def train(self):\n",
    "        old_state_value_table = self.copy_current_table()\n",
    "        current_step = 0\n",
    "        while current_step < self.config['max_iteration']:  \n",
    "            current_step = current_step + 1\n",
    "            self.update_value_function()\n",
    "            if current_step % self.config[\"evaluate_interval\"] == 0:\n",
    "                print(\"Iteration {}, Mean Reward is: {}\".format(current_step, evaluate(self.policy, config = self.config, num_episodes=1, render=False)))\n",
    "                #print(\"Iteration {}, Mean Reward is: {}\".format(current_step, 0))\n",
    "                # check exist\n",
    "                stop = True\n",
    "                flag = 0\n",
    "                for x in range(self.obs_dim[0] + 1):\n",
    "                    for y in range(self.obs_dim[1] + 1):\n",
    "                        if abs(self.q_table[x][y] - old_state_value_table[x][y]) > self.config[\"return_threshold\"]:\n",
    "                            stop = False\n",
    "                            flag = 1\n",
    "                    if flag == 1:\n",
    "                        break\n",
    "                if stop == True:\n",
    "                    print(\"Train converge at i = {}\".format(current_step))\n",
    "                    current_step = self.config['max_iteration']\n",
    "                else:\n",
    "                    old_state_value_table = self.copy_current_table()\n",
    "\n",
    "    def policy(self, obs):\n",
    "        table_x = int(obs[0] / self.env.UAV_speed)\n",
    "        table_y = int(obs[1] / self.env.UAV_speed)\n",
    "        next_state_value_list = []\n",
    "        for act in range(0, self.act_dim):\n",
    "            next_state, reward = self.get_transition((table_x, table_y), act)\n",
    "            next_state_x = int(next_state[0] / self.env.UAV_speed)\n",
    "            next_state_y = int(next_state[1] / self.env.UAV_speed)\n",
    "            next_state_value_list.append(self.q_table[next_state_x][next_state_y])\n",
    "        act = np.argmax(next_state_value_list)\n",
    "        return act, self.env.UAV_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space: (0, 1, 2, 3), total_steps: 50, current_step: 49, random_seed: 40, map: {'width': 1000, 'length': 1000, 'height': 300}, UAV_speed: 50, UAV_initial_pos: (0, 0), UAV_current_pos: (450, 700), number_of_user: 10, users_pos: [(469, 593), (898, 536), (32, 251), (289, 680), (984, 652), (680, 211), (131, 780), (356, 283), (900, 761), (451, 838)], UAV_path: [(0, 0), (0, 50), (0, 100), (0, 150), (0, 200), (0, 250), (50, 250), (100, 250), (100, 300), (150, 300), (200, 300), (200, 350), (250, 350), (250, 400), (250, 450), (250, 500), (300, 500), (300, 550), (300, 600), (350, 600), (350, 650), (400, 650), (400, 700), (450, 700), (450, 650), (450, 700), (450, 650), (450, 700), (450, 650), (450, 700), (450, 650), (450, 700), (450, 650), (450, 700), (450, 650), (450, 700), (450, 650), (450, 700), (450, 650), (450, 700), (450, 650), (450, 700), (450, 650), (450, 700), (450, 650), (450, 700), (450, 650), (450, 700), (450, 650)], g0: 1e-05, B: 1000000, Pk: 0.1, noise: 1e-09\n",
      "Current Step: 49\n",
      "Policy choice direction: 2, speed: 50\n",
      "UAV current position x: 450, y: 700\n",
      "Current step reward: 0.00722398926972661, episodes rewards: 0.3190513552686094\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward is: 0.32636665952588884\n"
     ]
    }
   ],
   "source": [
    "environment_config = dict(\n",
    "    total_steps = 50,\n",
    "    random_seed = 10,\n",
    "    is_random_env = False,\n",
    "    map=dict(\n",
    "        width=1000,\n",
    "        length=1000,\n",
    "        height=300\n",
    "    ),\n",
    "    number_of_user = 10,\n",
    "    UAV_speed = 50,\n",
    "    UAV_initial_pos = (0, 0),\n",
    "    wireless_parameter = dict(\n",
    "        g0 = 10 ** (-5),\n",
    "        B = 10 ** (6),\n",
    "        Pk = 0.1,\n",
    "        noise = 10 ** (-9)\n",
    "    )\n",
    ")\n",
    "\n",
    "value_iteration_config = merge_config(dict(\n",
    "    max_iteration=10000,\n",
    "    evaluate_interval=100,  # don't need to update policy each iteration\n",
    "    gamma=0.9,\n",
    "    return_threshold=1,\n",
    "), environment_config)\n",
    "trainer = UAVTrainerValueIteration(value_iteration_config)\n",
    "trainer.env.print_locations()\n",
    "trainer.train()\n",
    "\n",
    "print(\"Mean Reward is: {}\".format(evaluate(trainer.policy, config = value_iteration_config, num_episodes=1, render=True))) # Enable render=True can see the agent behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rest is we try to create more complex environment, with more action space and state, not finished yet, please ignore the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "class UAVEnvironmentComplex(UAVEnvironment):\n",
    "    \"\"\"\n",
    "    Complex UAV environment, action can be compose as (speed, direction), UAV position can be continous float number\n",
    "    \n",
    "    State = (self.UAV_current_pos, self.users_pos [list])\n",
    "    Action = (speed, direction)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(UAVEnvironmentComplex, self).__init__(config)\n",
    "        delattr(self, \"action_space\") \n",
    "\n",
    "\n",
    "    def transition_dynamics(self, action, state):\n",
    "        # action = (speed, direction), speed with 0 and self.UAV_speed, direction with (0, 2 * pi)\n",
    "        speed = action[0]\n",
    "        direction = action[1]\n",
    "        next_x = self.UAV_current_pos[0] + speed * math.cos(direction)\n",
    "        next_y = self.UAV_current_pos[1] + speed * math.sin(direction)\n",
    "        next_x = max(0, next_x)\n",
    "        next_x = min(self.map[\"width\"], next_x)\n",
    "        next_y = max(0, next_y)\n",
    "        next_y = min(self.map[\"length\"], next_y)\n",
    "        return (next_x, next_y)\n",
    "  \n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        return (self.UAV_current_pos, self.users_pos)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # action = (speed, direction)\n",
    "        # This function return the state = (self.UAV_current_pos, self.users_pos [list])\n",
    "        speed = action[0]\n",
    "        speed = max(0, speed)\n",
    "        speed = min(self.UAV_speed, speed)\n",
    "        \n",
    "        standarded_action = (speed, action[1])\n",
    "        self.UAV_current_pos = self.transition_dynamics(standarded_action, self.UAV_current_pos)\n",
    "        self.current_step = self.current_step + 1\n",
    "        done = False\n",
    "        if self.current_step == self.total_steps:\n",
    "            done =  True\n",
    "        state = (self.UAV_current_pos, self.users_pos)\n",
    "        reward = self.get_reward(self.UAV_current_pos)\n",
    "        return state, reward, done\n",
    "    \n",
    "    def action_sample(self):\n",
    "        speed = random.uniform(0, 1) * self.UAV_speed\n",
    "        random_direction = math.pi * 2 * random.uniform(0, 1)\n",
    "        action = (speed, random_direction)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGame = UAVEnvironmentComplex(environment_config)\n",
    "myGame.print_locations()\n",
    "myGame.print_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_complex(policy, config, num_episodes=1, render=False):\n",
    "    env = UAVEnvironmentComplex(config)\n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        # all policy will return a direction and a speed\n",
    "        action = policy(obs)\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            obs, reward, done = env.step(action)\n",
    "            action = policy(obs)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            if render == True:\n",
    "                clear_output(wait=True)\n",
    "                env.print_locations()\n",
    "                print(\"Current Step: {}\".format(env.current_step))\n",
    "                print(\"Policy choice direction: {}, speed: {}\".format((action[1] * 180 / math.pi), action[0]))\n",
    "                print(\"Current step reward: {}, episodes rewards: {}\".format(reward, ep_reward))\n",
    "                env.print_map()\n",
    "                wait(sleep=0.2)\n",
    "        rewards.append(ep_reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Class for all RL algorithm\n",
    "class UAVComplexTrainer: \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env = UAVEnvironmentComplex(self.config)\n",
    "            \n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def compute_values(self, state):\n",
    "        pass\n",
    "    \n",
    "    def state_to_feature_vector(self, UAV_pos, User_pos_list):\n",
    "        # State (UAV_pos, Users_pos), assmue there is max_number_of_user which UAV can be server, to define the feature vector length\n",
    "        # Feature vector: a vector contain tuple of positions\n",
    "        capacity = self.config[\"max_number_of_user\"] + 1 # UAV + user\n",
    "        features = [0] * capacity * 2\n",
    "        features[0] = UAV_pos[0]\n",
    "        features[1] = UAV_pos[1]\n",
    "        for i in range(0, len(User_pos_list)):\n",
    "            features[2*i+2] = User_pos_list[i][0]\n",
    "            features[2*i+3] = User_pos_list[i][1]\n",
    "        return features\n",
    "\n",
    "    def policy(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from random policy\n",
    "class UAVComplexTrainerRandomPolicy(UAVComplexTrainer):\n",
    "    def __init__(self, config):\n",
    "        super(UAVComplexTrainerRandomPolicy, self).__init__(config)\n",
    "        \n",
    "    def policy(self, obs):\n",
    "        action = self.env.action_sample()\n",
    "        return action\n",
    "\n",
    "random_policy_config = environment_config\n",
    "trainer = UAVComplexTrainerRandomPolicy(random_policy_config)\n",
    "print(\"Mean Reward is: {}\".format(evaluate_complex(trainer.policy, config = random_policy_config, num_episodes=1, render=True))) # Enable render=True can see the agent behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def to_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        x = np.array(x) \n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.float32)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    return x\n",
    "\n",
    "\n",
    "class NetworkModel(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(NetworkModel, self).__init__()\n",
    "        self.network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(obs_dim,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,act_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.network(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Class for all RL algorithm\n",
    "class UAVComplexTrainerNN(UAVComplexTrainer): \n",
    "    def __init__(self, config):\n",
    "        super(UAVComplexTrainerNN, self).__init__(config)\n",
    "        self.max_episode_length = self.config[\"max_episode_length\"]\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.gamma = self.config[\"gamma\"]\n",
    "        self.model = NetworkModel((self.config[\"max_number_of_user\"] + 1) * 2, 2)\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def policy(self, state):\n",
    "        if np.random.uniform(0,1) <= self.config[\"eps\"]:\n",
    "            action = self.env.action_sample()\n",
    "        else:\n",
    "            feature = self.state_to_feature_vector(state[0], state[1])\n",
    "            model_input = to_tensor(feature).squeeze()\n",
    "            action = self.model(model_input)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_function_config = merge_config(dict(\n",
    "    total_steps = 50,\n",
    "    number_of_user = 10,\n",
    "    max_number_of_user = 20,\n",
    "    map=dict(\n",
    "        width=1000,\n",
    "        length=1000,\n",
    "        height=100\n",
    "    ),\n",
    "    max_episode_length=10000,\n",
    "    eps=0.01,\n",
    "    gamma=0.9,\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "), environment_config)\n",
    "\n",
    "NNTrainer = UAVComplexTrainerNN(linear_function_config)\n",
    "print(\"Mean Reward is: {}\".format(evaluate_complex(NNTrainer.policy, config = linear_function_config, num_episodes=1, render=True))) # Enable render=True can see the agent behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "s = np.random.normal(mu, sigma, (10,3))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
