{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from random import randint, choice\n",
    "import matplotlib.pyplot as plt\n",
    "from util import *\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAVEnvironment:\n",
    "    \"\"\"\n",
    "    Game environment for UAV test\n",
    "    \n",
    "    ---Map---\n",
    "    \n",
    "    y-axis(length)\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "     _______________________ x-axis(width)\n",
    "     \n",
    "    Hight is a fixed value\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        # Game config\n",
    "        self.action_space = (0, 1, 2, 3) # up, right, down, left, total 4 actions\n",
    "        self.total_steps = config[\"total_steps\"] # when the game end\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Map config\n",
    "        self.map = dict(width=config[\"map\"][\"width\"], length=config[\"map\"][\"length\"], height=config[\"map\"][\"height\"])\n",
    "        self.UAV_speed = config[\"UAV_speed\"]\n",
    "        self.UAV_initial_pos = config[\"UAV_initial_pos\"] # a tuple\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        self.number_of_user = config[\"number_of_user\"]\n",
    "        self.users_pos = list()\n",
    "        for i in range(0, self.number_of_user):\n",
    "            self.users_pos.append((randint(0, self.map[\"width\"]), randint(0, self.map[\"length\"])))\n",
    "        \n",
    "        # Wireless config\n",
    "        self.g0 = config[\"wireless_parameter\"][\"g0\"]\n",
    "        self.B = config[\"wireless_parameter\"][\"B\"]\n",
    "        self.Pk = config[\"wireless_parameter\"][\"Pk\"]\n",
    "        self.noise = config[\"wireless_parameter\"][\"noise\"]\n",
    "        \n",
    "    def get_reward(self, UAV_pos):\n",
    "        # One step Reward is define as the summation of all user's utility\n",
    "        reward = 0\n",
    "        for user_index in range(0, self.number_of_user):\n",
    "            gkm = self.g0 / (self.map[\"height\"] ** 2 + (UAV_pos[0] - self.users_pos[user_index][0]) ** 2 + (UAV_pos[1] - self.users_pos[user_index][1]) ** 2)\n",
    "            user_utility = self.B * math.log(1 + self.Pk * gkm / self.noise, 2)\n",
    "            reward = reward + user_utility\n",
    "        return reward / (10 ** 6) # Use Mkbps as signal basic unit\n",
    "    \n",
    "    def transition_dynamics(self, action, speed, state):\n",
    "        # given the action (direction), calculate the next state (UAV current position)\n",
    "        assert action in self.action_space\n",
    "        next_UAV_pos = list(state)\n",
    "        if action == 0:\n",
    "            # move up\n",
    "            next_UAV_pos[1] = min(next_UAV_pos[1] + speed, self.map[\"length\"])\n",
    "        if action == 1:\n",
    "            # move right\n",
    "            next_UAV_pos[0] = min(next_UAV_pos[0] + speed, self.map[\"width\"])\n",
    "        if action == 2:\n",
    "            # move down\n",
    "            next_UAV_pos[1] = max(next_UAV_pos[1] - speed, 0)\n",
    "        if action == 3:\n",
    "            # move left\n",
    "            next_UAV_pos[0] = max(next_UAV_pos[0] - speed, 0)\n",
    "        return tuple(next_UAV_pos)\n",
    "    \n",
    "    def get_transition(self):\n",
    "        # This function only works for model based, we are trying to disable this function to try more algorithm\n",
    "        # Return a table of transition, we assume UAV use fixed flying speed\n",
    "        \"\"\"\n",
    "        Structure:\n",
    "        transition[\n",
    "            x_0[\n",
    "                y_0[\n",
    "                    {next_state, reward}, # for action 1\n",
    "                    {next_state, reward}, # for action 2\n",
    "                    ...\n",
    "                    {next_state, reward}, # for action 20\n",
    "                ],\n",
    "                y_1*v[],\n",
    "                ...\n",
    "                y_h-1*v[]\n",
    "            ],\n",
    "            x_1*v[],\n",
    "            x_2*v[],\n",
    "            ...\n",
    "            x_w-1*v[]\n",
    "        ]\n",
    "        \n",
    "        \"\"\"\n",
    "        transition = list()\n",
    "        for state_x in range(0, int(self.map[\"width\"] / self.UAV_speed) + 1):\n",
    "            transition.append(list())\n",
    "            for state_y in range(0, int(self.map[\"length\"] / self.UAV_speed) + 1):\n",
    "                transition[state_x].append(list())\n",
    "                for action in self.action_space:\n",
    "                    next_state = self.transition_dynamics(action, self.UAV_speed, (state_x * self.UAV_speed, state_y * self.UAV_speed))\n",
    "                    reward = self.get_reward(next_state)\n",
    "                    transition[state_x][state_y].append(dict(next_state=next_state,reward=reward))\n",
    "        return transition\n",
    "                    \n",
    "    def step(self, action, speed=-1):\n",
    "        # assume we use the max speed as the default speed, when come near to the opt-position, we can slow down the speed\n",
    "        if speed < 0 or speed >= self.UAV_speed:\n",
    "            speed = self.UAV_speed\n",
    "            \n",
    "        self.UAV_current_pos = self.transition_dynamics(action, speed, self.UAV_current_pos)\n",
    "        self.current_step = self.current_step + 1\n",
    "        done = False\n",
    "        if self.current_step == self.total_steps:\n",
    "            done =  True\n",
    "        return self.UAV_current_pos, self.get_reward(self.UAV_current_pos), done\n",
    "    \n",
    "    def action_sample(self):\n",
    "        return choice(self.action_space)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        self.users_pos = list()\n",
    "        for i in range(0, self.number_of_user):\n",
    "            self.users_pos.append((randint(0, self.map[\"width\"]), randint(0, self.map[\"length\"])))\n",
    "        return self.UAV_current_pos\n",
    "        \n",
    "    def print_attribute(self):\n",
    "        attrs = vars(self)\n",
    "        print(', '.join(\"%s: %s\" % item for item in attrs.items()))\n",
    "        \n",
    "    def print_map(self):\n",
    "        x_list = [pos[0] for pos in self.users_pos]\n",
    "        y_list = [pos[1] for pos in self.users_pos]\n",
    "        x_list.append(self.UAV_current_pos[0])\n",
    "        y_list.append(self.UAV_current_pos[1])\n",
    "        \n",
    "        colors = np.array([\"red\", \"green\"])\n",
    "        sizes = []\n",
    "        colors_map = []\n",
    "        for i in range(0, self.number_of_user):\n",
    "            sizes.append(25)\n",
    "            colors_map.append(1)\n",
    "        sizes.append(50)\n",
    "        colors_map.append(0)\n",
    "        plt.scatter(x_list, y_list, c=colors[colors_map], s=sizes) \n",
    "        plt.axis([0, self.map[\"width\"], 0, self.map[\"length\"]])\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_config = dict(\n",
    "    total_steps = 50,\n",
    "    map=dict(\n",
    "        width=120,\n",
    "        length=60,\n",
    "        height=100\n",
    "    ),\n",
    "    number_of_user = 1,\n",
    "    UAV_speed = 20,\n",
    "    UAV_initial_pos = (0, 0),\n",
    "    wireless_parameter = dict(\n",
    "        g0 = 10 ** (-5),\n",
    "        B = 10 ** (6),\n",
    "        Pk = 0.1,\n",
    "        noise = 10 ** (-9)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space: (0, 1, 2, 3), total_steps: 50, current_step: 0, map: {'width': 120, 'length': 60, 'height': 100}, UAV_speed: 20, UAV_initial_pos: (0, 0), UAV_current_pos: (0, 0), number_of_user: 1, users_pos: [(43, 39)], g0: 1e-05, B: 1000000, Pk: 0.1, noise: 1e-09\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "myGame = UAVEnvironment(environment_config)\n",
    "myGame.print_attribute()\n",
    "myGame.print_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(policy, num_episodes=1, render=False):\n",
    "    env = UAVEnvironment(environment_config)\n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        # all policy will return a direction and a speed\n",
    "        act_direction, act_speed = policy(obs)\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            obs, reward, done = env.step(act_direction, act_speed)\n",
    "            act_direction, act_speed = policy(obs)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            if render:\n",
    "                clear_output(wait=True)\n",
    "                print(\"Current Step: {}\".format(env.current_step))\n",
    "                print(\"Policy choice direction: {}, speed: {}\".format(act_direction, act_speed))\n",
    "                print(\"UAV current position x: {}, y: {}\".format(env.UAV_current_pos[0], env.UAV_current_pos[1]))\n",
    "                print(\"Current step reward: {}, episodes rewards: {}\".format(reward, ep_reward))\n",
    "                env.print_map()\n",
    "                wait(sleep=0.2)\n",
    "        rewards.append(ep_reward)\n",
    "    return np.mean(rewards)\n",
    "\n",
    "\n",
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Class for all RL algorithm\n",
    "class UAVTrainer: \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env = UAVEnvironment(self.config)next_state_value_list\n",
    "    \n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def policy(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward is: 5.612808969660213\n"
     ]
    }
   ],
   "source": [
    "# Start from random policy\n",
    "class UAVTrainerRandomPolicy(UAVTrainer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "    def policy(self, obs):\n",
    "        max_speed = self.env.UAV_speed\n",
    "        return self.env.action_sample(), max_speed\n",
    "\n",
    "random_policy_config = environment_config\n",
    "trainer = UAVTrainerRandomPolicy(random_policy_config)\n",
    "print(\"Mean Reward is: {}\".format(evaluate(trainer.policy, num_episodes=1, render=False))) # Enable render=True can see the agent behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100, Mean Reward is: ~ \n",
      "Iteration 200, Mean Reward is: ~ \n",
      "Train converge at i = 200\n",
      "[1.2663484234695048, 1.3065133343174375, 1.3356370645229736, 1.3475230886391918]\n",
      "[1.3065133343174375, 1.3367139898365685, 1.358241738993891, 1.3673692369363137]\n",
      "[1.3367139898365685, 1.358241738993891, 1.3688111467250823, 1.3738558628669826]\n",
      "[1.3511401905664628, 1.3688111467250823, 1.3738558628669826, 1.3738558628669826]\n",
      "[1.342566134651386, 1.3622813023527762, 1.3702851999038177, 1.3738558628669826]\n",
      "[1.31649778239327, 1.3437069760526958, 1.3622813023527762, 1.3702851999038177]\n",
      "[1.2788740432074819, 1.31649778239327, 1.3437069760526958, 1.3548801843336893]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Value Iteration, Tabular, transition dynamic is known, assume only use fixed speed to reduce action space\n",
    "class UAVTrainerValueIteration(UAVTrainer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.transitions = self.env.get_transition()\n",
    "        self.q_table = []\n",
    "        self.obs_dim = (int(self.env.map[\"width\"] / self.env.UAV_speed), int(self.env.map[\"length\"] / self.env.UAV_speed))\n",
    "        self.act_dim = len(self.env.action_space)\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        \n",
    "        for x in range(0, self.obs_dim[0]+1):\n",
    "            self.q_table.append(list())\n",
    "            for y in range(0, self.obs_dim[1]+1):\n",
    "                self.q_table[x].append(0)\n",
    "            \n",
    "    def get_transition(self, state, act):\n",
    "        transition = self.transitions[state[0]][state[1]][act]\n",
    "        return transition[\"next_state\"], transition[\"reward\"]\n",
    "    \n",
    "    def print_transitions(self):\n",
    "        print(\"Transition width {}, length {}, number of act {}\".format(len(self.transitions), len(self.transitions[0]), len(self.transitions[0][0])))\n",
    "        print(self.transitions)\n",
    "        \n",
    "    def print_table(self):\n",
    "        view_table = [None] * 5\n",
    "        for x in len(self.q_table):\n",
    "            for y in len(self.q_table[0]):\n",
    "                pass\n",
    "            \n",
    "            \n",
    "    def copy_current_table(self):\n",
    "        old_table = []\n",
    "        for x in range(0, self.obs_dim[0]+1):\n",
    "            old_table.append(list())\n",
    "            for y in range(0, self.obs_dim[1]+1):\n",
    "                old_table[x].append(self.q_table[x][y])\n",
    "        return old_table\n",
    "\n",
    "    def update_value_function(self):\n",
    "        old_table = self.copy_current_table()\n",
    "        for state_x in range(self.obs_dim[0] + 1):\n",
    "            for state_y in range(self.obs_dim[1] + 1):\n",
    "                state_value = 0\n",
    "                state_action_values = [0 for i in range(0, self.act_dim)]\n",
    "\n",
    "                for act in range(self.act_dim):\n",
    "                    next_state, reward = self.get_transition((state_x, state_y), act)\n",
    "                    table_x = int(next_state[0] / self.env.UAV_speed)\n",
    "                    table_y = int(next_state[1] / self.env.UAV_speed)\n",
    "                    #print(table_x, table_y)\n",
    "                    state_action_values[act] = state_action_values[act] + reward + self.gamma * old_table[table_x][table_y]   \n",
    "                state_value = np.max(state_action_values)\n",
    "                self.q_table[state_x][state_y] = state_value\n",
    "                #print(\"Update x: {}, y: {} to value {}\".format(state_x, state_y, state_value))\n",
    "            \n",
    "    def train(self):\n",
    "        old_state_value_table = self.copy_current_table()\n",
    "        current_step = 0\n",
    "        while current_step < self.config['max_iteration']:  \n",
    "            current_step = current_step + 1\n",
    "            self.update_value_function()\n",
    "            if current_step % self.config[\"evaluate_interval\"] == 0:\n",
    "                print(\"Iteration {}, Mean Reward is: ~ \".format(current_step))\n",
    "                # check exist\n",
    "                stop = True\n",
    "                flag = 0\n",
    "                for x in range(self.obs_dim[0] + 1):\n",
    "                    for y in range(self.obs_dim[1] + 1):\n",
    "                        if abs(self.q_table[x][y] - old_state_value_table[x][y]) > self.config[\"return_threshold\"]:\n",
    "                            stop = False\n",
    "                            flag = 1\n",
    "                    if flag == 1:\n",
    "                        break\n",
    "                if stop == True:\n",
    "                    print(\"Train converge at i = {}\".format(current_step))\n",
    "                    current_step = self.config['max_iteration']\n",
    "                else:\n",
    "                    old_state_value_table = self.copy_current_table()\n",
    "\n",
    "    def policy(self, obs):\n",
    "        table_x = int(obs[0] / self.env.UAV_speed)\n",
    "        table_y = int(obs[1] / self.env.UAV_speed)\n",
    "        next_state_value_list = []\n",
    "        for act in range(0, self.act_dim):\n",
    "            next_state, reward = self.get_transition((table_x, table_y), act)\n",
    "            next_state_x = int(next_state[0] / self.env.UAV_speed)\n",
    "            next_state_y = int(next_state[1] / self.env.UAV_speed)\n",
    "            next_state_value_list.append(self.q_table[next_state_x][next_state_y])\n",
    "        print(next_state_value_list, act)\n",
    "        act = np.argmax(next_state_value_list)\n",
    "        return act, self.env.UAV_speed\n",
    "\n",
    "value_iteration_config = merge_config(dict(\n",
    "    max_iteration=10000,\n",
    "    evaluate_interval=100,  # don't need to update policy each iteration\n",
    "    gamma=0.9,\n",
    "    return_threshold=1\n",
    "), environment_config)\n",
    "trainer = UAVTrainerValueIteration(value_iteration_config)\n",
    "#trainer.print_transitions()\n",
    "trainer.train()\n",
    "trainer.print_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Step: 49\n",
      "Policy choice direction: 0, speed: 20\n",
      "UAV current position x: 40, y: 60\n",
      "Current step reward: 0.1356277515030935, episodes rewards: 6.587685675725059\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3738558628669826, 1.3738558628669826, 1.3688111467250823, 1.3673692369363137] 3\n",
      "Mean Reward is: 6.7233134272281525\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Reward is: {}\".format(evaluate(trainer.policy, num_episodes=1, render=True))) # Enable render=True can see the agent behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
