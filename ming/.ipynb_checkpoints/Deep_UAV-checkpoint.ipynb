{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import random\n",
    "from random import randint, choice\n",
    "import matplotlib.pyplot as plt\n",
    "from util import *\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAVEnvironment():\n",
    "    \"\"\"\n",
    "    Game environment for UAV test\n",
    "    \n",
    "    ---Map---\n",
    "    \n",
    "    y-axis(length)\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "     _______________________ x-axis(width)\n",
    "     \n",
    "    Hight is a fixed value\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        # Game config\n",
    "        self.action_space = (0, 1, 2, 3) # up, right, down, left, total 4 actions\n",
    "        self.total_steps = config[\"total_steps\"] # when the game end\n",
    "        self.current_step = 0\n",
    "        if config[\"is_random_env\"] == False:\n",
    "            self.random_seed = config[\"random_seed\"]\n",
    "            random.seed(self.random_seed)\n",
    "        \n",
    "        # Map config\n",
    "        self.map = dict(width=config[\"map\"][\"width\"], length=config[\"map\"][\"length\"], height=config[\"map\"][\"height\"])\n",
    "        self.UAV_speed = config[\"UAV_speed\"]\n",
    "        self.UAV_initial_pos = config[\"UAV_initial_pos\"] # a tuple\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        self.number_of_user = config[\"number_of_user\"]\n",
    "        self.users_pos = list()\n",
    "        for i in range(0, self.number_of_user):\n",
    "            self.users_pos.append((randint(0, self.map[\"width\"]), randint(0, self.map[\"length\"])))\n",
    "        \n",
    "        # Wireless config\n",
    "        self.g0 = config[\"wireless_parameter\"][\"g0\"]\n",
    "        self.B = config[\"wireless_parameter\"][\"B\"]\n",
    "        self.Pk = config[\"wireless_parameter\"][\"Pk\"]\n",
    "        self.noise = config[\"wireless_parameter\"][\"noise\"]\n",
    "        \n",
    "    def get_reward(self, UAV_pos):\n",
    "        # One step Reward is define as the summation of all user's utility\n",
    "        reward = 0\n",
    "        for user_index in range(0, self.number_of_user):\n",
    "            gkm = self.g0 / (self.map[\"height\"] ** 2 + (UAV_pos[0] - self.users_pos[user_index][0]) ** 2 + (UAV_pos[1] - self.users_pos[user_index][1]) ** 2)\n",
    "            user_utility = self.B * math.log(1 + self.Pk * gkm / self.noise, 2)\n",
    "            reward = reward + user_utility\n",
    "        return reward / (10 ** 6) # Use Mkbps as signal basic unit\n",
    "    \n",
    "    def transition_dynamics(self, action, speed, state):\n",
    "        # given the action (direction), calculate the next state (UAV current position)\n",
    "        assert action in self.action_space\n",
    "        next_UAV_pos = list(state)\n",
    "        if action == 0:\n",
    "            # move up\n",
    "            next_UAV_pos[1] = min(next_UAV_pos[1] + speed, self.map[\"length\"])\n",
    "        if action == 1:\n",
    "            # move right\n",
    "            next_UAV_pos[0] = min(next_UAV_pos[0] + speed, self.map[\"width\"])\n",
    "        if action == 2:\n",
    "            # move down\n",
    "            next_UAV_pos[1] = max(next_UAV_pos[1] - speed, 0)\n",
    "        if action == 3:\n",
    "            # move left\n",
    "            next_UAV_pos[0] = max(next_UAV_pos[0] - speed, 0)\n",
    "        return tuple(next_UAV_pos)\n",
    "    \n",
    "    def get_transition(self):\n",
    "        # This function only works for model based, we are trying to disable this function to try more algorithm\n",
    "        # Return a table of transition, we assume UAV use fixed flying speed\n",
    "        \"\"\"\n",
    "        Structure:\n",
    "        transition[\n",
    "            x_0[\n",
    "                y_0[\n",
    "                    {next_state, reward}, # for action 1\n",
    "                    {next_state, reward}, # for action 2\n",
    "                    ...\n",
    "                    {next_state, reward}, # for action 20\n",
    "                ],\n",
    "                y_1*v[],\n",
    "                ...\n",
    "                y_h-1*v[]\n",
    "            ],\n",
    "            x_1*v[],\n",
    "            x_2*v[],\n",
    "            ...\n",
    "            x_w-1*v[]\n",
    "        ]\n",
    "        \"\"\"\n",
    "        transition = list()\n",
    "        for state_x in range(0, int(self.map[\"width\"] / self.UAV_speed) + 1):\n",
    "            transition.append(list())\n",
    "            for state_y in range(0, int(self.map[\"length\"] / self.UAV_speed) + 1):\n",
    "                transition[state_x].append(list())\n",
    "                for action in self.action_space:\n",
    "                    next_state = self.transition_dynamics(action, self.UAV_speed, (state_x * self.UAV_speed, state_y * self.UAV_speed))\n",
    "                    reward = self.get_reward(next_state)\n",
    "                    transition[state_x][state_y].append(dict(next_state=next_state,reward=reward))\n",
    "        return transition\n",
    "                    \n",
    "    def step(self, action, speed=-1):\n",
    "        # assume we use the max speed as the default speed, when come near to the opt-position, we can slow down the speed\n",
    "        if speed < 0 or speed >= self.UAV_speed:\n",
    "            speed = self.UAV_speed\n",
    "            \n",
    "        self.UAV_current_pos = self.transition_dynamics(action, speed, self.UAV_current_pos)\n",
    "        self.current_step = self.current_step + 1\n",
    "        done = False\n",
    "        if self.current_step == self.total_steps:\n",
    "            done =  True\n",
    "        return self.UAV_current_pos, self.get_reward(self.UAV_current_pos), done\n",
    "    \n",
    "    def action_sample(self):\n",
    "        return choice(self.action_space)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        return self.UAV_current_pos\n",
    "        \n",
    "    def print_attribute(self):\n",
    "        attrs = vars(self)\n",
    "        print(', '.join(\"%s: %s\" % item for item in attrs.items()))\n",
    "        \n",
    "    def print_locations(self):\n",
    "        print(\"UAV position is: {}\".format(self.UAV_current_pos))\n",
    "        print(\"Users position are: {}\".format(self.users_pos))\n",
    "        \n",
    "    def print_map(self):\n",
    "        x_list = [pos[0] for pos in self.users_pos]\n",
    "        y_list = [pos[1] for pos in self.users_pos]\n",
    "        x_list.append(self.UAV_current_pos[0])\n",
    "        y_list.append(self.UAV_current_pos[1])\n",
    "        \n",
    "        colors = np.array([\"red\", \"green\"])\n",
    "        sizes = []\n",
    "        colors_map = []\n",
    "        for i in range(0, self.number_of_user):\n",
    "            sizes.append(25)\n",
    "            colors_map.append(1)\n",
    "        sizes.append(50)\n",
    "        colors_map.append(0)\n",
    "        plt.scatter(x_list, y_list, c=colors[colors_map], s=sizes) \n",
    "        plt.axis([0, self.map[\"width\"], 0, self.map[\"length\"]])\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_config = dict(\n",
    "    total_steps = 50,\n",
    "    random_seed = 0,\n",
    "    is_random_env = True,\n",
    "    map=dict(\n",
    "        width=1000,\n",
    "        length=1000,\n",
    "        height=100\n",
    "    ),\n",
    "    number_of_user = 10,\n",
    "    UAV_speed = 20,\n",
    "    UAV_initial_pos = (0, 0),\n",
    "    wireless_parameter = dict(\n",
    "        g0 = 10 ** (-5),\n",
    "        B = 10 ** (6),\n",
    "        Pk = 0.1,\n",
    "        noise = 10 ** (-9)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAV position is: (0, 0)\n",
      "Users position are: [(769, 500), (922, 202), (937, 711), (650, 932), (863, 142), (829, 55), (355, 720), (919, 371), (524, 185), (434, 149)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "myGame = UAVEnvironment(environment_config)\n",
    "myGame.print_locations()\n",
    "myGame.print_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(policy, config, num_episodes=1, render=False):\n",
    "    env = UAVEnvironment(config)\n",
    "    \n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        # all policy will return a direction and a speed\n",
    "        act_direction, act_speed = policy(obs)\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            obs, reward, done = env.step(act_direction, act_speed)\n",
    "            act_direction, act_speed = policy(obs)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            if render == True:\n",
    "                clear_output(wait=True)\n",
    "                env.print_attribute()\n",
    "                print(\"Current Step: {}\".format(env.current_step))\n",
    "                print(\"Policy choice direction: {}, speed: {}\".format(act_direction, act_speed))\n",
    "                print(\"UAV current position x: {}, y: {}\".format(env.UAV_current_pos[0], env.UAV_current_pos[1]))\n",
    "                print(\"Current step reward: {}, episodes rewards: {}\".format(reward, ep_reward))\n",
    "                env.print_map()\n",
    "                wait(sleep=0.2)\n",
    "        rewards.append(ep_reward)\n",
    "    return np.mean(rewards)\n",
    "\n",
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAV position is: (0, 0)\n",
      "Users position are: [(910, 248), (936, 339), (771, 394), (85, 553), (221, 526), (8, 13), (773, 291), (61, 643), (741, 908), (580, 365)]\n",
      "Mean Reward is: 2.2223227801873096\n"
     ]
    }
   ],
   "source": [
    "# Basic Class for all RL algorithm\n",
    "class UAVTrainer: \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env = UAVEnvironment(self.config)\n",
    "\n",
    "# Start from random policy\n",
    "class UAVTrainerRandomPolicy(UAVTrainer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "    def policy(self, obs):\n",
    "        max_speed = self.env.UAV_speed\n",
    "        return self.env.action_sample(), max_speed\n",
    "\n",
    "random_policy_config = environment_config\n",
    "trainer = UAVTrainerRandomPolicy(random_policy_config)\n",
    "trainer.env.print_locations()\n",
    "print(\"Mean Reward is: {}\".format(evaluate(trainer.policy, config = random_policy_config, num_episodes=1, render=False))) # Enable render=True can see the agent behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAV position is: (0, 0)\n",
      "Users position are: [(585, 33), (439, 494), (591, 15), (211, 473), (832, 503), (843, 284), (669, 830), (164, 35), (533, 501), (335, 77)]\n",
      "Iteration 100, Mean Reward is: 12.21220913177311\n",
      "Iteration 200, Mean Reward is: 12.21220913177311\n",
      "Train converge at i = 200\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Value Iteration, Tabular, transition dynamic is known, assume only use fixed speed to reduce action space\n",
    "class UAVTrainerValueIteration(UAVTrainer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.transitions = self.env.get_transition()\n",
    "        self.q_table = []\n",
    "        self.obs_dim = (int(self.env.map[\"width\"] / self.env.UAV_speed), int(self.env.map[\"length\"] / self.env.UAV_speed))\n",
    "        self.act_dim = len(self.env.action_space)\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        \n",
    "        for x in range(0, self.obs_dim[0]+1):\n",
    "            self.q_table.append(list())\n",
    "            for y in range(0, self.obs_dim[1]+1):\n",
    "                self.q_table[x].append(0)\n",
    "            \n",
    "    def get_transition(self, state, act):\n",
    "        transition = self.transitions[state[0]][state[1]][act]\n",
    "        return transition[\"next_state\"], transition[\"reward\"]\n",
    "    \n",
    "    def print_transitions(self):\n",
    "        print(\"Transition width {}, length {}, number of act {}\".format(len(self.transitions), len(self.transitions[0]), len(self.transitions[0][0])))\n",
    "        print(self.transitions)\n",
    "        \n",
    "    def print_table(self):\n",
    "        for j in range(len(self.q_table[0])-1, -1, -1):\n",
    "            for i in range(0, len(self.q_table)):\n",
    "                print(self.q_table[i][j], end =\" \")\n",
    "            print(\"\")\n",
    "            \n",
    "            \n",
    "    def copy_current_table(self):\n",
    "        old_table = []\n",
    "        for x in range(0, self.obs_dim[0]+1):\n",
    "            old_table.append(list())\n",
    "            for y in range(0, self.obs_dim[1]+1):\n",
    "                old_table[x].append(self.q_table[x][y])\n",
    "        return old_table\n",
    "\n",
    "    def update_value_function(self):\n",
    "        old_table = self.copy_current_table()\n",
    "        for state_x in range(self.obs_dim[0] + 1):\n",
    "            for state_y in range(self.obs_dim[1] + 1):\n",
    "                state_value = 0\n",
    "                state_action_values = [0 for i in range(0, self.act_dim)]\n",
    "\n",
    "                for act in range(self.act_dim):\n",
    "                    next_state, reward = self.get_transition((state_x, state_y), act)\n",
    "                    table_x = int(next_state[0] / self.env.UAV_speed)\n",
    "                    table_y = int(next_state[1] / self.env.UAV_speed)\n",
    "                    #print(table_x, table_y)\n",
    "                    state_action_values[act] = state_action_values[act] + reward + self.gamma * old_table[table_x][table_y]   \n",
    "                state_value = np.max(state_action_values)\n",
    "                self.q_table[state_x][state_y] = state_value\n",
    "                #print(\"Update x: {}, y: {} to value {}\".format(state_x, state_y, state_value))\n",
    "            \n",
    "    def train(self):\n",
    "        old_state_value_table = self.copy_current_table()\n",
    "        current_step = 0\n",
    "        while current_step < self.config['max_iteration']:  \n",
    "            current_step = current_step + 1\n",
    "            self.update_value_function()\n",
    "            if current_step % self.config[\"evaluate_interval\"] == 0:\n",
    "                print(\"Iteration {}, Mean Reward is: {}\".format(current_step, evaluate(self.policy, config = self.config, num_episodes=1, render=False)))\n",
    "                #print(\"Iteration {}, Mean Reward is: {}\".format(current_step, 0))\n",
    "                # check exist\n",
    "                stop = True\n",
    "                flag = 0\n",
    "                for x in range(self.obs_dim[0] + 1):\n",
    "                    for y in range(self.obs_dim[1] + 1):\n",
    "                        if abs(self.q_table[x][y] - old_state_value_table[x][y]) > self.config[\"return_threshold\"]:\n",
    "                            stop = False\n",
    "                            flag = 1\n",
    "                    if flag == 1:\n",
    "                        break\n",
    "                if stop == True:\n",
    "                    print(\"Train converge at i = {}\".format(current_step))\n",
    "                    current_step = self.config['max_iteration']\n",
    "                else:\n",
    "                    old_state_value_table = self.copy_current_table()\n",
    "\n",
    "    def policy(self, obs):\n",
    "        table_x = int(obs[0] / self.env.UAV_speed)\n",
    "        table_y = int(obs[1] / self.env.UAV_speed)\n",
    "        next_state_value_list = []\n",
    "        for act in range(0, self.act_dim):\n",
    "            next_state, reward = self.get_transition((table_x, table_y), act)\n",
    "            next_state_x = int(next_state[0] / self.env.UAV_speed)\n",
    "            next_state_y = int(next_state[1] / self.env.UAV_speed)\n",
    "            next_state_value_list.append(self.q_table[next_state_x][next_state_y])\n",
    "        act = np.argmax(next_state_value_list)\n",
    "        return act, self.env.UAV_speed\n",
    "\n",
    "value_iteration_config = merge_config(dict(\n",
    "    max_iteration=10000,\n",
    "    evaluate_interval=100,  # don't need to update policy each iteration\n",
    "    gamma=0.9,\n",
    "    return_threshold=1,\n",
    "    random_seed = 10,\n",
    "    is_random_env = False\n",
    "), environment_config)\n",
    "trainer = UAVTrainerValueIteration(value_iteration_config)\n",
    "trainer.env.print_locations()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward is: 12.21220913177311\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Reward is: {}\".format(evaluate(trainer.policy, config = value_iteration_config, num_episodes=1, render=False))) # Enable render=True can see the agent behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "class UAVEnvironmentComplex(UAVEnvironment):\n",
    "    \"\"\"\n",
    "    Complex UAV environment, action can be compose as (speed, direction), UAV position can be continous float number\n",
    "    \n",
    "    State = (self.UAV_current_pos, self.users_pos [list])\n",
    "    Action = (speed, direction)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(UAVEnvironmentComplex, self).__init__(config)\n",
    "        delattr(self, \"action_space\") \n",
    "\n",
    "\n",
    "    def transition_dynamics(self, action, state):\n",
    "        # action = (speed, direction), speed with 0 and self.UAV_speed, direction with (0, 2 * pi)\n",
    "        speed = action[0]\n",
    "        direction = action[1]\n",
    "        next_x = self.UAV_current_pos[0] + speed * math.cos(direction)\n",
    "        next_y = self.UAV_current_pos[1] + speed * math.sin(direction)\n",
    "        next_x = max(0, next_x)\n",
    "        next_x = min(self.map[\"width\"], next_x)\n",
    "        next_y = max(0, next_y)\n",
    "        next_y = min(self.map[\"length\"], next_y)\n",
    "        return (next_x, next_y)\n",
    "  \n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        return (self.UAV_current_pos, self.users_pos)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # action = (speed, direction)\n",
    "        # This function return the state = (self.UAV_current_pos, self.users_pos [list])\n",
    "        speed = action[0]\n",
    "        speed = max(0, speed)\n",
    "        speed = min(self.UAV_speed, speed)\n",
    "        \n",
    "        standarded_action = (speed, action[1])\n",
    "        self.UAV_current_pos = self.transition_dynamics(standarded_action, self.UAV_current_pos)\n",
    "        self.current_step = self.current_step + 1\n",
    "        done = False\n",
    "        if self.current_step == self.total_steps:\n",
    "            done =  True\n",
    "        state = (self.UAV_current_pos, self.users_pos)\n",
    "        reward = self.get_reward(self.UAV_current_pos)\n",
    "        return state, reward, done\n",
    "    \n",
    "    def action_sample(self):\n",
    "        speed = random.uniform(0, 1) * self.UAV_speed\n",
    "        random_direction = math.pi * 2 * random.uniform(0, 1)\n",
    "        action = (speed, random_direction)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAV position is: (0, 0)\n",
      "Users position are: [(469, 86), (907, 661), (809, 302), (742, 270), (471, 830), (442, 842), (806, 67), (993, 93), (526, 367), (501, 452)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "myGame = UAVEnvironmentComplex(environment_config)\n",
    "myGame.print_locations()\n",
    "myGame.print_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_complex(policy, config, num_episodes=1, render=False):\n",
    "    env = UAVEnvironmentComplex(config)\n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        # all policy will return a direction and a speed\n",
    "        action = policy(obs)\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            obs, reward, done = env.step(action)\n",
    "            action = policy(obs)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            if render == True:\n",
    "                clear_output(wait=True)\n",
    "                env.print_locations()\n",
    "                print(\"Current Step: {}\".format(env.current_step))\n",
    "                print(\"Policy choice direction: {}, speed: {}\".format((action[1] * 180 / math.pi), action[0]))\n",
    "                print(\"Current step reward: {}, episodes rewards: {}\".format(reward, ep_reward))\n",
    "                env.print_map()\n",
    "                wait(sleep=0.2)\n",
    "        rewards.append(ep_reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Class for all RL algorithm\n",
    "class UAVComplexTrainer: \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env = UAVEnvironmentComplex(self.config)\n",
    "            \n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def compute_values(self, state):\n",
    "        pass\n",
    "    \n",
    "    def state_to_feature_vector(self, UAV_pos, User_pos_list):\n",
    "        # State (UAV_pos, Users_pos), assmue there is max_number_of_user which UAV can be server, to define the feature vector length\n",
    "        # Feature vector: a vector contain tuple of positions\n",
    "        capacity = self.config[\"max_number_of_user\"] + 1 # UAV + user\n",
    "        features = [0] * capacity * 2\n",
    "        features[0] = UAV_pos[0]\n",
    "        features[1] = UAV_pos[1]\n",
    "        for i in range(0, len(User_pos_list)):\n",
    "            features[2*i+2] = User_pos_list[i][0]\n",
    "            features[2*i+3] = User_pos_list[i][1]\n",
    "        return features\n",
    "\n",
    "    def policy(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward is: 1.4088875274428188\n"
     ]
    }
   ],
   "source": [
    "# Start from random policy\n",
    "class UAVComplexTrainerRandomPolicy(UAVComplexTrainer):\n",
    "    def __init__(self, config):\n",
    "        super(UAVComplexTrainerRandomPolicy, self).__init__(config)\n",
    "        \n",
    "    def policy(self, obs):\n",
    "        action = self.env.action_sample()\n",
    "        return action\n",
    "\n",
    "random_policy_config = environment_config\n",
    "trainer = UAVComplexTrainerRandomPolicy(random_policy_config)\n",
    "print(\"Mean Reward is: {}\".format(evaluate_complex(trainer.policy, config = random_policy_config, num_episodes=1, render=False))) # Enable render=True can see the agent behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def to_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        x = np.array(x) \n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.float32)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    return x\n",
    "\n",
    "\n",
    "class NetworkModel(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(NetworkModel, self).__init__()\n",
    "        self.network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(obs_dim,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,act_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.network(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Class for all RL algorithm\n",
    "class UAVComplexTrainerNN(UAVComplexTrainer): \n",
    "    def __init__(self, config):\n",
    "        super(UAVComplexTrainerNN, self).__init__(config)\n",
    "        self.max_episode_length = self.config[\"max_episode_length\"]\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.gamma = self.config[\"gamma\"]\n",
    "        self.model = NetworkModel((self.config[\"max_number_of_user\"] + 1) * 2, 2)\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def policy(self, state):\n",
    "        if np.random.uniform(0,1) <= self.config[\"eps\"]:\n",
    "            action = self.env.action_sample()\n",
    "        else:\n",
    "            feature = self.state_to_feature_vector(state[0], state[1])\n",
    "            model_input = to_tensor(feature).squeeze() \n",
    "            action = self.model(model_input)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAV position is: (0, 0)\n",
      "Users position are: [(350, 582), (471, 902), (127, 440), (891, 829), (398, 61), (616, 12), (799, 378), (407, 169), (11, 134), (862, 583)]\n",
      "Current Step: 49\n",
      "Policy choice direction: 2164.46142578125, speed: -87.59222412109375\n",
      "Current step reward: 0.08469010600081821, episodes rewards: 4.149815194040089\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0.,   0., 350., 582., 471., 902., 127., 440., 891., 829., 398.,  61.,\n",
      "        616.,  12., 799., 378., 407., 169.,  11., 134., 862., 583.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.])\n",
      "Mean Reward is: 4.234505300040907\n"
     ]
    }
   ],
   "source": [
    "linear_function_config = merge_config(dict(\n",
    "    total_steps = 50,\n",
    "    number_of_user = 10,\n",
    "    max_number_of_user = 20,\n",
    "    map=dict(\n",
    "        width=1000,\n",
    "        length=1000,\n",
    "        height=100\n",
    "    ),\n",
    "    max_episode_length=10000,\n",
    "    eps=0.01,\n",
    "    gamma=0.9,\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "), environment_config)\n",
    "\n",
    "NNTrainer = UAVComplexTrainerNN(linear_function_config)\n",
    "print(\"Mean Reward is: {}\".format(evaluate_complex(NNTrainer.policy, config = linear_function_config, num_episodes=1, render=True))) # Enable render=True can see the agent behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "s = np.random.normal(mu, sigma, (10,3))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
