{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import random\n",
    "from random import randint, choice\n",
    "import matplotlib.pyplot as plt\n",
    "from util import *\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAVEnvironment():\n",
    "    \"\"\"\n",
    "    Game environment for UAV test\n",
    "    \n",
    "    ---Map---\n",
    "    \n",
    "    y-axis(length)\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "     _______________________ x-axis(width)\n",
    "     \n",
    "    Hight is a fixed value\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        # Game config\n",
    "        self.action_space = (0, 1, 2, 3) # up, right, down, left, total 4 actions\n",
    "        self.total_steps = config[\"total_steps\"] # when the game end\n",
    "        self.current_step = 0\n",
    "        if config[\"is_random_env\"] == False:\n",
    "            self.random_seed = config[\"random_seed\"]\n",
    "            random.seed(self.random_seed)\n",
    "        \n",
    "        # Map config\n",
    "        self.map = dict(width=config[\"map\"][\"width\"], length=config[\"map\"][\"length\"], height=config[\"map\"][\"height\"])\n",
    "        self.UAV_speed = config[\"UAV_speed\"]\n",
    "        self.UAV_initial_pos = config[\"UAV_initial_pos\"] # a tuple\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        self.number_of_user = config[\"number_of_user\"]\n",
    "        self.users_pos = list()\n",
    "        for i in range(0, self.number_of_user):\n",
    "            self.users_pos.append((randint(0, self.map[\"width\"]), randint(0, self.map[\"length\"])))\n",
    "        \n",
    "        # Wireless config\n",
    "        self.g0 = config[\"wireless_parameter\"][\"g0\"]\n",
    "        self.B = config[\"wireless_parameter\"][\"B\"]\n",
    "        self.Pk = config[\"wireless_parameter\"][\"Pk\"]\n",
    "        self.noise = config[\"wireless_parameter\"][\"noise\"]\n",
    "        \n",
    "    def get_reward(self, UAV_pos):\n",
    "        # One step Reward is define as the summation of all user's utility\n",
    "        reward = 0\n",
    "        for user_index in range(0, self.number_of_user):\n",
    "            gkm = self.g0 / (self.map[\"height\"] ** 2 + (UAV_pos[0] - self.users_pos[user_index][0]) ** 2 + (UAV_pos[1] - self.users_pos[user_index][1]) ** 2)\n",
    "            user_utility = self.B * math.log(1 + self.Pk * gkm / self.noise, 2)\n",
    "            reward = reward + user_utility\n",
    "        return reward / (10 ** 6) # Use Mkbps as signal basic unit\n",
    "    \n",
    "    def transition_dynamics(self, action, speed, state):\n",
    "        # given the action (direction), calculate the next state (UAV current position)\n",
    "        assert action in self.action_space\n",
    "        next_UAV_pos = list(state)\n",
    "        if action == 0:\n",
    "            # move up\n",
    "            next_UAV_pos[1] = min(next_UAV_pos[1] + speed, self.map[\"length\"])\n",
    "        if action == 1:\n",
    "            # move right\n",
    "            next_UAV_pos[0] = min(next_UAV_pos[0] + speed, self.map[\"width\"])\n",
    "        if action == 2:\n",
    "            # move down\n",
    "            next_UAV_pos[1] = max(next_UAV_pos[1] - speed, 0)\n",
    "        if action == 3:\n",
    "            # move left\n",
    "            next_UAV_pos[0] = max(next_UAV_pos[0] - speed, 0)\n",
    "        return tuple(next_UAV_pos)\n",
    "    \n",
    "    def get_transition(self):\n",
    "        # This function only works for model based, we are trying to disable this function to try more algorithm\n",
    "        # Return a table of transition, we assume UAV use fixed flying speed\n",
    "        \"\"\"\n",
    "        Structure:\n",
    "        transition[\n",
    "            x_0[\n",
    "                y_0[\n",
    "                    {next_state, reward}, # for action 1\n",
    "                    {next_state, reward}, # for action 2\n",
    "                    ...\n",
    "                    {next_state, reward}, # for action 20\n",
    "                ],\n",
    "                y_1*v[],\n",
    "                ...\n",
    "                y_h-1*v[]\n",
    "            ],\n",
    "            x_1*v[],\n",
    "            x_2*v[],\n",
    "            ...\n",
    "            x_w-1*v[]\n",
    "        ]\n",
    "        \"\"\"\n",
    "        transition = list()\n",
    "        for state_x in range(0, int(self.map[\"width\"] / self.UAV_speed) + 1):\n",
    "            transition.append(list())\n",
    "            for state_y in range(0, int(self.map[\"length\"] / self.UAV_speed) + 1):\n",
    "                transition[state_x].append(list())\n",
    "                for action in self.action_space:\n",
    "                    next_state = self.transition_dynamics(action, self.UAV_speed, (state_x * self.UAV_speed, state_y * self.UAV_speed))\n",
    "                    reward = self.get_reward(next_state)\n",
    "                    transition[state_x][state_y].append(dict(next_state=next_state,reward=reward))\n",
    "        return transition\n",
    "                    \n",
    "    def step(self, action, speed=-1):\n",
    "        # assume we use the max speed as the default speed, when come near to the opt-position, we can slow down the speed\n",
    "        if speed < 0 or speed >= self.UAV_speed:\n",
    "            speed = self.UAV_speed\n",
    "            \n",
    "        self.UAV_current_pos = self.transition_dynamics(action, speed, self.UAV_current_pos)\n",
    "        self.current_step = self.current_step + 1\n",
    "        done = False\n",
    "        if self.current_step == self.total_steps:\n",
    "            done =  True\n",
    "        return self.UAV_current_pos, self.get_reward(self.UAV_current_pos), done\n",
    "    \n",
    "    def action_sample(self):\n",
    "        return choice(self.action_space)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        return self.UAV_current_pos\n",
    "        \n",
    "    def print_attribute(self):\n",
    "        attrs = vars(self)\n",
    "        print(', '.join(\"%s: %s\" % item for item in attrs.items()))\n",
    "        \n",
    "    def print_locations(self):\n",
    "        print(\"UAV position is: {}\".format(self.UAV_current_pos))\n",
    "        print(\"Users position are: {}\".format(self.users_pos))\n",
    "        \n",
    "    def print_map(self):\n",
    "        x_list = [pos[0] for pos in self.users_pos]\n",
    "        y_list = [pos[1] for pos in self.users_pos]\n",
    "        x_list.append(self.UAV_current_pos[0])\n",
    "        y_list.append(self.UAV_current_pos[1])\n",
    "        \n",
    "        colors = np.array([\"red\", \"green\"])\n",
    "        sizes = []\n",
    "        colors_map = []\n",
    "        for i in range(0, self.number_of_user):\n",
    "            sizes.append(25)\n",
    "            colors_map.append(1)\n",
    "        sizes.append(50)\n",
    "        colors_map.append(0)\n",
    "        plt.scatter(x_list, y_list, c=colors[colors_map], s=sizes) \n",
    "        plt.axis([0, self.map[\"width\"], 0, self.map[\"length\"]])\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_config = dict(\n",
    "    total_steps = 50,\n",
    "    random_seed = 0,\n",
    "    is_random_env = True,\n",
    "    map=dict(\n",
    "        width=1000,\n",
    "        length=1000,\n",
    "        height=100\n",
    "    ),\n",
    "    number_of_user = 10,\n",
    "    UAV_speed = 50,\n",
    "    UAV_initial_pos = (0, 0),\n",
    "    wireless_parameter = dict(\n",
    "        g0 = 10 ** (-5),\n",
    "        B = 10 ** (6),\n",
    "        Pk = 0.1,\n",
    "        noise = 10 ** (-9)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(policy, config, num_episodes=1, render=False):\n",
    "    env = UAVEnvironment(config)\n",
    "    \n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        # all policy will return a direction and a speed\n",
    "        act_direction, act_speed = policy(obs)\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            obs, reward, done = env.step(act_direction, act_speed)\n",
    "            act_direction, act_speed = policy(obs)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            if render == True:\n",
    "                clear_output(wait=True)\n",
    "                env.print_attribute()\n",
    "                print(\"Current Step: {}\".format(env.current_step))\n",
    "                print(\"Policy choice direction: {}, speed: {}\".format(act_direction, act_speed))\n",
    "                print(\"UAV current position x: {}, y: {}\".format(env.UAV_current_pos[0], env.UAV_current_pos[1]))\n",
    "                print(\"Current step reward: {}, episodes rewards: {}\".format(reward, ep_reward))\n",
    "                env.print_map()\n",
    "                wait(sleep=0.2)\n",
    "        rewards.append(ep_reward)\n",
    "    return np.mean(rewards)\n",
    "\n",
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space: (0, 1, 2, 3), total_steps: 50, current_step: 49, map: {'width': 1000, 'length': 1000, 'height': 100}, UAV_speed: 50, UAV_initial_pos: (0, 0), UAV_current_pos: (300, 400), number_of_user: 10, users_pos: [(94, 642), (530, 824), (861, 503), (414, 83), (211, 586), (248, 35), (206, 96), (85, 194), (259, 710), (782, 290)], g0: 1e-05, B: 1000000, Pk: 0.1, noise: 1e-09\n",
      "Current Step: 49\n",
      "Policy choice direction: 2, speed: 50\n",
      "UAV current position x: 300, y: 400\n",
      "Current step reward: 0.11831115972664977, episodes rewards: 10.563685288441969\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward is: 10.681822555792433\n"
     ]
    }
   ],
   "source": [
    "# Basic Class for all RL algorithm\n",
    "class UAVTrainer: \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env = UAVEnvironment(self.config)\n",
    "\n",
    "# Start from random policy\n",
    "class UAVTrainerRandomPolicy(UAVTrainer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "    def policy(self, obs):\n",
    "        max_speed = self.env.UAV_speed\n",
    "        return self.env.action_sample(), max_speed\n",
    "\n",
    "random_policy_config = environment_config\n",
    "trainer = UAVTrainerRandomPolicy(random_policy_config)\n",
    "trainer.env.print_locations()\n",
    "print(\"Mean Reward is: {}\".format(evaluate(trainer.policy, config = random_policy_config, num_episodes=1, render=True))) # Enable render=True can see the agent behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAV position is: (0, 0)\n",
      "Users position are: [(927, 740), (702, 805), (784, 901), (926, 154), (266, 690), (650, 869), (926, 103), (893, 335), (586, 927), (173, 27)]\n",
      "Iteration 100, Mean Reward is: 6.114380738755387\n",
      "Iteration 200, Mean Reward is: 6.114380738755387\n",
      "Train converge at i = 200\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Value Iteration, Tabular, transition dynamic is known, assume only use fixed speed to reduce action space\n",
    "class UAVTrainerValueIteration(UAVTrainer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.transitions = self.env.get_transition()\n",
    "        self.q_table = []\n",
    "        self.obs_dim = (int(self.env.map[\"width\"] / self.env.UAV_speed), int(self.env.map[\"length\"] / self.env.UAV_speed))\n",
    "        self.act_dim = len(self.env.action_space)\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        \n",
    "        for x in range(0, self.obs_dim[0]+1):\n",
    "            self.q_table.append(list())\n",
    "            for y in range(0, self.obs_dim[1]+1):\n",
    "                self.q_table[x].append(0)\n",
    "            \n",
    "    def get_transition(self, state, act):\n",
    "        transition = self.transitions[state[0]][state[1]][act]\n",
    "        return transition[\"next_state\"], transition[\"reward\"]\n",
    "    \n",
    "    def print_transitions(self):\n",
    "        print(\"Transition width {}, length {}, number of act {}\".format(len(self.transitions), len(self.transitions[0]), len(self.transitions[0][0])))\n",
    "        print(self.transitions)\n",
    "        \n",
    "    def print_table(self):\n",
    "        for j in range(len(self.q_table[0])-1, -1, -1):\n",
    "            for i in range(0, len(self.q_table)):\n",
    "                print(self.q_table[i][j], end =\" \")\n",
    "            print(\"\")\n",
    "            \n",
    "            \n",
    "    def copy_current_table(self):\n",
    "        old_table = []\n",
    "        for x in range(0, self.obs_dim[0]+1):\n",
    "            old_table.append(list())\n",
    "            for y in range(0, self.obs_dim[1]+1):\n",
    "                old_table[x].append(self.q_table[x][y])\n",
    "        return old_table\n",
    "\n",
    "    def update_value_function(self):\n",
    "        old_table = self.copy_current_table()\n",
    "        for state_x in range(self.obs_dim[0] + 1):\n",
    "            for state_y in range(self.obs_dim[1] + 1):\n",
    "                state_value = 0\n",
    "                state_action_values = [0 for i in range(0, self.act_dim)]\n",
    "\n",
    "                for act in range(self.act_dim):\n",
    "                    next_state, reward = self.get_transition((state_x, state_y), act)\n",
    "                    table_x = int(next_state[0] / self.env.UAV_speed)\n",
    "                    table_y = int(next_state[1] / self.env.UAV_speed)\n",
    "                    #print(table_x, table_y)\n",
    "                    state_action_values[act] = state_action_values[act] + reward + self.gamma * old_table[table_x][table_y]   \n",
    "                state_value = np.max(state_action_values)\n",
    "                self.q_table[state_x][state_y] = state_value\n",
    "                #print(\"Update x: {}, y: {} to value {}\".format(state_x, state_y, state_value))\n",
    "            \n",
    "    def train(self):\n",
    "        old_state_value_table = self.copy_current_table()\n",
    "        current_step = 0\n",
    "        while current_step < self.config['max_iteration']:  \n",
    "            current_step = current_step + 1\n",
    "            self.update_value_function()\n",
    "            if current_step % self.config[\"evaluate_interval\"] == 0:\n",
    "                print(\"Iteration {}, Mean Reward is: {}\".format(current_step, evaluate(self.policy, config = self.config, num_episodes=1, render=False)))\n",
    "                #print(\"Iteration {}, Mean Reward is: {}\".format(current_step, 0))\n",
    "                # check exist\n",
    "                stop = True\n",
    "                flag = 0\n",
    "                for x in range(self.obs_dim[0] + 1):\n",
    "                    for y in range(self.obs_dim[1] + 1):\n",
    "                        if abs(self.q_table[x][y] - old_state_value_table[x][y]) > self.config[\"return_threshold\"]:\n",
    "                            stop = False\n",
    "                            flag = 1\n",
    "                    if flag == 1:\n",
    "                        break\n",
    "                if stop == True:\n",
    "                    print(\"Train converge at i = {}\".format(current_step))\n",
    "                    current_step = self.config['max_iteration']\n",
    "                else:\n",
    "                    old_state_value_table = self.copy_current_table()\n",
    "                #old_state_value_table = self.copy_current_table()\n",
    "\n",
    "\n",
    "    def policy(self, obs):\n",
    "        table_x = int(obs[0] / self.env.UAV_speed)\n",
    "        table_y = int(obs[1] / self.env.UAV_speed)\n",
    "        next_state_value_list = []\n",
    "        for act in range(0, self.act_dim):\n",
    "            next_state, reward = self.get_transition((table_x, table_y), act)\n",
    "            next_state_x = int(next_state[0] / self.env.UAV_speed)\n",
    "            next_state_y = int(next_state[1] / self.env.UAV_speed)\n",
    "            next_state_value_list.append(self.q_table[next_state_x][next_state_y])\n",
    "        act = np.argmax(next_state_value_list)\n",
    "        return act, self.env.UAV_speed\n",
    "\n",
    "value_iteration_config = merge_config(dict(\n",
    "    max_iteration=10000,\n",
    "    evaluate_interval=100,  # don't need to update policy each iteration\n",
    "    gamma=0.9,\n",
    "    return_threshold=1,\n",
    "    random_seed = 20,\n",
    "    is_random_env = False\n",
    "), environment_config)\n",
    "trainer = UAVTrainerValueIteration(value_iteration_config)\n",
    "trainer.env.print_locations()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space: (0, 1, 2, 3), total_steps: 50, current_step: 49, random_seed: 20, map: {'width': 1000, 'length': 1000, 'height': 100}, UAV_speed: 50, UAV_initial_pos: (0, 0), UAV_current_pos: (200, 50), number_of_user: 10, users_pos: [(927, 740), (702, 805), (784, 901), (926, 154), (266, 690), (650, 869), (926, 103), (893, 335), (586, 927), (173, 27)], g0: 1e-05, B: 1000000, Pk: 0.1, noise: 1e-09\n",
      "Current Step: 49\n",
      "Policy choice direction: 0, speed: 50\n",
      "UAV current position x: 200, y: 50\n",
      "Current step reward: 0.1416760536087171, episodes rewards: 6.006877261178452\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward is: 6.114380738755387\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Reward is: {}\".format(evaluate(trainer.policy, config = value_iteration_config, num_episodes=1, render=True))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import torch.nn as nn\n",
    "\n",
    "class ExperienceReplayMemory:\n",
    "    \"\"\"Store and sample the transitions\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        # deque is a useful class which acts like a list but only contain\n",
    "        # finite elements.When appending new element make deque exceeds the \n",
    "        # `maxlen`, the oldest element (the index 0 element) will be removed.\n",
    "        \n",
    "        #  uncomment next line. \n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchModel(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(PytorchModel, self).__init__()\n",
    "        self.action_value = torch.nn.Sequential(\n",
    "            torch.nn.Linear(obs_dim,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,act_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.action_value(obs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_config = merge_config(dict(\n",
    "    memory_size=50000,\n",
    "    learn_start=5000,\n",
    "    batch_size=32,\n",
    "    target_update_freq=500,  # in steps\n",
    "    learn_freq=1,  # in steps\n",
    "    n=1\n",
    "), value_iteration_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.float32)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    if x.dim() == 3 or x.dim() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    assert x.dim() == 2 or x.dim() == 4, x.shape\n",
    "    return x\n",
    "\n",
    "def to_long_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.long)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    if x.dim() == 3 or x.dim() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    assert x.dim() == 2 or x.dim() == 4, x.shape\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNTrainer(UAVTrainerValueIteration):\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, pytorch_config)\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.memory = ExperienceReplayMemory(config[\"memory_size\"])\n",
    "        self.learn_start = config[\"learn_start\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.target_update_freq = config[\"target_update_freq\"]\n",
    "        self.clip_norm = config[\"clip_norm\"]\n",
    "        self.step_since_update = 0\n",
    "        self.total_step = 0\n",
    "        self.eps=config[\"eps\"]\n",
    "        self.max_episode_length=config[\"max_episode_length\"]\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"Initialize the pytorch model as the Q network and the target network\"\"\"\n",
    "        # Initialize two network using PytorchModel class\n",
    "        self.obs_dim= 2;\n",
    "        self.network = PytorchModel(self.obs_dim,self.act_dim)\n",
    "\n",
    "        self.network.eval()\n",
    "        self.network.share_memory()\n",
    "\n",
    "        # Initialize target network, which is identical to self.network,\n",
    "        # and should have the same weights with self.network. So you should\n",
    "        # put the weights of self.network into self.target_network.\n",
    "        self.target_network = PytorchModel(self.obs_dim,self.act_dim)\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "\n",
    "        self.target_network.eval()\n",
    "\n",
    "        # Build Adam optimizer and MSE Loss.\n",
    "        # Uncomment next few lines\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.network.parameters(), lr=self.learning_rate\n",
    "        )\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        \"\"\"Compute the value for each potential action. Note that you\n",
    "        should NOT preprocess the state here.\"\"\"\n",
    "        # Convert the output of neural network to numpy array\n",
    "        values=self.network(processed_state)\n",
    "        return values.data.numpy()\n",
    "    \n",
    "    def compute_action(self, processed_state, eps=None):\n",
    "        \"\"\"Compute the action given the state. Note that the input\n",
    "        is the processed state.\"\"\"\n",
    "\n",
    "        values = self.compute_values(processed_state)\n",
    "        assert values.ndim == 1, values.shape\n",
    "\n",
    "        if eps is None:\n",
    "            eps = self.eps\n",
    "\n",
    "        # Implement the epsilon-greedy policy here. We have `eps`\n",
    "        #  probability to choose a uniformly random action in action_space,\n",
    "        #  otherwise choose action that maximizes the values.\n",
    "        # Hint: Use the function of self.env.action_space to sample random\n",
    "        # action.\n",
    "        \n",
    "       # if np.random.uniform(0,1)< eps:\n",
    "       #     action=self.env.action_sample()\n",
    "       #     action=np.array(action)\n",
    "       # else:\n",
    "       #     action=np.argmax(values)   \n",
    "        action=self.env.action_sample()\n",
    "        return action\n",
    "    \n",
    "    def evaluate(self, num_episodes=50, *args, **kwargs):\n",
    "        \"\"\"Use the function you write to evaluate current policy.\n",
    "        Return the mean episode reward of 50 episodes.\"\"\"\n",
    "        env = UAVEnvironment(environment_config)\n",
    "        rewards =[]\n",
    "        \n",
    "        policy = lambda raw_state: self.compute_action(\n",
    "            self.process_state(raw_state), eps=0.0)\n",
    "        \n",
    "        rewards = []\n",
    "        for i in range(num_episodes):\n",
    "            obs = env.reset()\n",
    "            # all policy will return a direction and a speed\n",
    "            act = policy(obs)\n",
    "            ep_reward = 0\n",
    "            while True:\n",
    "                obs, reward, done = env.step(act)\n",
    "                act = policy(obs)\n",
    "                ep_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "                if render == True:\n",
    "                    clear_output(wait=True)\n",
    "                    env.print_attribute()\n",
    "                    print(\"Current Step: {}\".format(env.current_step))\n",
    "                    print(\"Policy choice direction: {}\".format(act))\n",
    "                    print(\"UAV current position x: {}, y: {}\".format(env.UAV_current_pos[0], env.UAV_current_pos[1]))\n",
    "                    print(\"Current step reward: {}, episodes rewards: {}\".format(reward, ep_reward))\n",
    "                    env.print_map()\n",
    "                    wait(sleep=0.2)\n",
    "            rewards.append(ep_reward)\n",
    "        return np.mean(rewards)\n",
    "        \n",
    "    \n",
    "    def train(self):\n",
    "        s = self.env.reset()\n",
    "        processed_s = self.process_state(s)\n",
    "        act = self.compute_action(processed_s)\n",
    "        stat = {\"loss\": []}\n",
    "\n",
    "        for t in range(self.max_episode_length):\n",
    "            next_state, reward, done = self.env.step(act)\n",
    "            print(next_state)\n",
    "            next_processed_s = self.process_state(next_state)\n",
    "\n",
    "            # Push the transition into memory.\n",
    "            self.memory.push(\n",
    "                (processed_s, act, reward, next_processed_s, done)\n",
    "            )\n",
    "\n",
    "            processed_s = next_processed_s\n",
    "            act = self.compute_action(next_processed_s)\n",
    "            self.step_since_update += 1\n",
    "            self.total_step += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            if t % self.config[\"learn_freq\"] != 0:\n",
    "                # It's not necessary to update in each step.\n",
    "                continue\n",
    "\n",
    "            if len(self.memory) < self.learn_start:\n",
    "                continue\n",
    "            elif len(self.memory) == self.learn_start:\n",
    "                print(\"Current memory contains {} transitions, \"\n",
    "                      \"start learning!\".format(self.learn_start))\n",
    "\n",
    "            batch = self.memory.sample(self.batch_size)\n",
    "\n",
    "            # Transform a batch of state / action / .. into a tensor.\n",
    "            state_batch = to_tensor(\n",
    "                np.stack([transition[0] for transition in batch])\n",
    "            )\n",
    "            #action_batch = to_tensor(\n",
    "            #    np.stack([transition[1] for transition in batch])\n",
    "            #)\n",
    "            #action_batch = np.stack([transition[1] for transition in batch])\n",
    "            #action_batch=torch.from_numpy(action_batch).type(torch.long)\n",
    "            action_batch = to_long_tensor(\n",
    "                np.stack([transition[1] for transition in batch])\n",
    "            )\n",
    "            reward_batch = to_tensor(\n",
    "                np.stack([transition[2] for transition in batch])\n",
    "            )\n",
    "            next_state_batch = torch.stack(\n",
    "                [transition[3] for transition in batch]\n",
    "            )\n",
    "            done_batch = to_tensor(\n",
    "                np.stack([transition[4] for transition in batch])\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Compute the values of Q in next state in batch.\n",
    "                # Hint: \n",
    "                #  1. Q_t_plus_one is the maximum value of Q values of possible\n",
    "                #     actions in next state. So the input to the network is \n",
    "                #     next_state_batch.\n",
    "                #  2. Q_t_plus_one is computed using the target network.\n",
    "                \n",
    "                next_state_values=self.target_network(next_state_batch)\n",
    "                Q_t_plus_one = next_state_values.max(1)[0].detach()\n",
    "                \n",
    "                \n",
    "                assert isinstance(Q_t_plus_one, torch.Tensor)\n",
    "                assert Q_t_plus_one.dim() == 1\n",
    "                \n",
    "                # Compute the target value of Q in batch.\n",
    "                # Hint: The Q target is simply r_t + gamma * Q_t+1 \n",
    "                #  IF the episode is not done at time t.\n",
    "                #  That is, the (gamma*Q_t+1) term should be masked out\n",
    "                #  if done_batch[t] is True.\n",
    "                #  A smart way to do so is: using (1-done_batch) as multiplier\n",
    "                Q_target = reward_batch+(1-done_batch)*Q_t_plus_one\n",
    "                \n",
    "                \n",
    "                #assert Q_target.shape == (self.batch_size,)\n",
    "            \n",
    "            # Collect the Q values in batch.\n",
    "            # Hint: Remember to call self.network.train()\n",
    "            #  before you get the Q value from self.network(state_batch),\n",
    "            #  otherwise the graident will not be recorded by pytorch.\n",
    "            self.network.train()\n",
    "            \n",
    "            state_action_values=self.network(state_batch)\n",
    "            \n",
    "            Q_t = torch.t(state_action_values).gather(0,action_batch)\n",
    "    \n",
    "            assert Q_t.shape == Q_target.shape\n",
    "\n",
    "            # Update the network\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss(input=Q_t, target=Q_target)\n",
    "            loss_value = loss.item()\n",
    "            stat['loss'].append(loss_value)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping. Uncomment next line\n",
    "            nn.utils.clip_grad_norm_(self.network.parameters(), self.clip_norm)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            self.network.eval()\n",
    "\n",
    "        if len(self.memory) >= self.learn_start and \\\n",
    "                self.step_since_update > self.target_update_freq:\n",
    "            print(\"{} steps has passed since last update. Now update the\"\n",
    "                  \" parameter of the behavior policy. Current step: {}\".format(\n",
    "                self.step_since_update, self.total_step\n",
    "            ))\n",
    "            self.step_since_update = 0\n",
    "            # Copy the weights of self.network to self.target_network.\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "            \n",
    "            self.target_network.eval()\n",
    "            \n",
    "        return {\"loss\": np.mean(stat[\"loss\"]), \"episode_len\": t}\n",
    "\n",
    "    def process_state(self, state):\n",
    "        state=np.array(state);\n",
    "        return torch.from_numpy(state).type(torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    \"\"\"Run the trainer and report progress, agnostic to the class of trainer\n",
    "    :param trainer_cls: A trainer class \n",
    "    :param config: A dict\n",
    "    :param reward_threshold: the reward threshold to break the training\n",
    "    :return: The trained trainer and a dataframe containing learning progress\n",
    "    \"\"\"\n",
    "    assert inspect.isclass(trainer_cls)\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    trainer = trainer_cls(config)\n",
    "    trainer.initialize_parameters()\n",
    "    config = trainer.config\n",
    "    start = now = time.time()\n",
    "    stats = []\n",
    "    for i in range(config['max_iteration'] + 1):\n",
    "        stat = trainer.train()\n",
    "        stats.append(stat or {})\n",
    "        if i % config['evaluate_interval'] == 0 or \\\n",
    "                i == config[\"max_iteration\"]:\n",
    "            reward = evaluate(trainer.policy,config, num_episodes=10)\n",
    "            print(\"({:.1f}s,+{:.1f}s)\\tIteration {}, current mean episode \"\n",
    "                  \"reward is {}. {}\".format(\n",
    "                time.time() - start, time.time() - now, i, reward,\n",
    "                {k: round(np.mean(v), 4) for k, v in\n",
    "                 stat.items()} if stat else \"\"))\n",
    "            now = time.time()\n",
    "        if reward_threshold is not None and reward > reward_threshold:\n",
    "            print(\"In {} iteration, current mean episode reward {:.3f} is \"\n",
    "                  \"greater than reward threshold {}. Congratulation! Now we \"\n",
    "                  \"exit the training process.\".format(\n",
    "                i, reward, reward_threshold))\n",
    "            break\n",
    "    return trainer, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAV position is: (0, 0)\n",
      "Users position are: [(927, 740), (702, 805), (784, 901), (926, 154), (266, 690), (650, 869), (926, 103), (893, 335), (586, 927), (173, 27)]\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 20)\n",
      "(0, 40)\n",
      "(20, 40)\n",
      "(20, 20)\n",
      "(0, 20)\n",
      "(0, 20)\n",
      "(0, 20)\n",
      "(20, 20)\n",
      "(40, 20)\n",
      "(40, 0)\n",
      "(40, 0)\n",
      "(40, 0)\n",
      "(20, 0)\n",
      "(20, 20)\n",
      "(0, 20)\n",
      "(0, 20)\n",
      "(0, 40)\n",
      "(20, 40)\n",
      "(40, 40)\n",
      "(40, 60)\n",
      "(60, 60)\n",
      "(60, 80)\n",
      "(60, 100)\n",
      "(80, 100)\n",
      "(80, 80)\n",
      "(80, 60)\n",
      "(80, 40)\n",
      "(80, 20)\n",
      "(100, 20)\n",
      "(100, 40)\n",
      "(100, 60)\n",
      "(100, 40)\n",
      "(120, 40)\n",
      "(140, 40)\n",
      "(160, 40)\n",
      "(160, 60)\n",
      "(160, 80)\n",
      "(160, 60)\n",
      "(160, 40)\n",
      "(180, 40)\n",
      "(180, 20)\n",
      "(160, 20)\n",
      "(180, 20)\n",
      "(180, 0)\n",
      "(160, 0)\n",
      "(180, 0)\n",
      "(200, 0)\n",
      "(180, 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': nan, 'episode_len': 49}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "new_config=merge_config(pytorch_config,dict(\n",
    "    max_iteration=2000,\n",
    "    evaluate_interval=100, \n",
    "    learning_rate=0.01,\n",
    "    clip_norm=10.0,\n",
    "    memory_size=50000,\n",
    "    learn_start=1000,\n",
    "    eps=0.1,\n",
    "    target_update_freq=1000,\n",
    "    batch_size=32,\n",
    "    max_episode_length=50,\n",
    "   ))\n",
    "trainer = DQNTrainer(new_config)\n",
    "trainer.initialize_parameters()\n",
    "trainer.env.print_locations()\n",
    "trainer.train()\n",
    "#run(trainer)\n",
    "#pytorch_trainer, pytorch_stat = run(DQNTrainer, new_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space: (0, 1, 2, 3), total_steps: 50, current_step: 49, random_seed: 20, map: {'width': 1000, 'length': 1000, 'height': 100}, UAV_speed: 20, UAV_initial_pos: (0, 0), UAV_current_pos: (0, 980), number_of_user: 10, users_pos: [(927, 740), (702, 805), (784, 901), (926, 154), (266, 690), (650, 869), (926, 103), (893, 335), (586, 927), (173, 27)], g0: 1e-05, B: 1000000, Pk: 0.1, noise: 1e-09\n",
      "Current Step: 49\n",
      "Policy choice direction: 0, speed: 20\n",
      "UAV current position x: 0, y: 980\n",
      "Current step reward: 0.027062911062308185, episodes rewards: 1.7602531124495604\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward is: 1.7864737588545252\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Reward is: {}\".format(evaluate(trainer.policy, config = value_iteration_config, num_episodes=1, render=True))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'render' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-f245b31d31e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-107-0e5e865580f2>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, num_episodes, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'render' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
