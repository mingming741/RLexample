{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "from utils import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from random import randint, choice\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAVEnvironment():\n",
    "    \"\"\"\n",
    "    Game environment for UAV test\n",
    "    \n",
    "    ---Map---\n",
    "    \n",
    "    y-axis(length)\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "     _______________________ x-axis(width)\n",
    "     \n",
    "    Hight is a fixed value\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        # Game config\n",
    "        self.action_space = (0, 1, 2, 3) # up, right, down, left, total 4 actions\n",
    "        self.total_steps = config[\"total_steps\"] # when the game end\n",
    "        self.current_step = 0\n",
    "        if config[\"is_random_env\"] == False:\n",
    "            self.random_seed = config[\"random_seed\"]\n",
    "            random.seed(self.random_seed)\n",
    "        \n",
    "        # Map config\n",
    "        self.map = dict(width=config[\"map\"][\"width\"], length=config[\"map\"][\"length\"], height=config[\"map\"][\"height\"])\n",
    "        self.UAV_speed = config[\"UAV_speed\"]\n",
    "        self.UAV_initial_pos = config[\"UAV_initial_pos\"] # a tuple\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        self.number_of_user = config[\"number_of_user\"]\n",
    "        self.users_pos = list()\n",
    "        self.UAV_path = [] # record the path of UAV\n",
    "        for i in range(0, self.number_of_user):\n",
    "            self.users_pos.append((randint(0, self.map[\"width\"]), randint(0, self.map[\"length\"])))\n",
    "        \n",
    "        # Wireless config\n",
    "        self.g0 = config[\"wireless_parameter\"][\"g0\"]\n",
    "        self.B = config[\"wireless_parameter\"][\"B\"]\n",
    "        self.Pk = config[\"wireless_parameter\"][\"Pk\"]\n",
    "        self.noise = config[\"wireless_parameter\"][\"noise\"]\n",
    "        \n",
    "    def get_reward(self, UAV_pos):\n",
    "        # One step Reward is define as the summation of all user's utility\n",
    "        reward = 0\n",
    "        for user_index in range(0, self.number_of_user):\n",
    "            gkm = self.g0 / (self.map[\"height\"] ** 2 + (UAV_pos[0] - self.users_pos[user_index][0]) ** 2 + (UAV_pos[1] - self.users_pos[user_index][1]) ** 2)\n",
    "            #user_utility = self.B * math.log(1 + self.Pk * gkm / self.noise, 2)\n",
    "            user_utility = (self.B/self.number_of_user)* math.log(1 + self.Pk * gkm / self.noise, 2)\n",
    "            reward = reward + user_utility\n",
    "        return reward / (10 ** 6) # Use Mkbps as signal basic unit\n",
    "    \n",
    "    def transition_dynamics(self, action, speed, state):\n",
    "        # given the action (direction), calculate the next state (UAV current position)\n",
    "        assert action in self.action_space\n",
    "        next_UAV_pos = list(state)\n",
    "        if action == 0:\n",
    "            # move up\n",
    "            next_UAV_pos[1] = min(next_UAV_pos[1] + speed, self.map[\"length\"])\n",
    "        if action == 1:\n",
    "            # move right\n",
    "            next_UAV_pos[0] = min(next_UAV_pos[0] + speed, self.map[\"width\"])\n",
    "        if action == 2:\n",
    "            # move down\n",
    "            next_UAV_pos[1] = max(next_UAV_pos[1] - speed, 0)\n",
    "        if action == 3:\n",
    "            # move left\n",
    "            next_UAV_pos[0] = max(next_UAV_pos[0] - speed, 0)\n",
    "        return tuple(next_UAV_pos)\n",
    "    \n",
    "    def get_transition(self):\n",
    "        # This function only works for model based, we are trying to disable this function to try more algorithm\n",
    "        # Return a table of transition, we assume UAV use fixed flying speed\n",
    "        \"\"\"\n",
    "        Structure:\n",
    "        transition[\n",
    "            x_0[\n",
    "                y_0[\n",
    "                    {next_state, reward}, # for action 1\n",
    "                    {next_state, reward}, # for action 2\n",
    "                    ...\n",
    "                    {next_state, reward}, # for action 20\n",
    "                ],\n",
    "                y_1*v[],\n",
    "                ...\n",
    "                y_h-1*v[]\n",
    "            ],\n",
    "            x_1*v[],\n",
    "            x_2*v[],\n",
    "            ...\n",
    "            x_w-1*v[]\n",
    "        ]\n",
    "        \"\"\"\n",
    "        transition = list()\n",
    "        for state_x in range(0, int(self.map[\"width\"] / self.UAV_speed) + 1):\n",
    "            transition.append(list())\n",
    "            for state_y in range(0, int(self.map[\"length\"] / self.UAV_speed) + 1):\n",
    "                transition[state_x].append(list())\n",
    "                for action in self.action_space:\n",
    "                    next_state = self.transition_dynamics(action, self.UAV_speed, (state_x * self.UAV_speed, state_y * self.UAV_speed))\n",
    "                    reward = self.get_reward(next_state)\n",
    "                    transition[state_x][state_y].append(dict(next_state=next_state,reward=reward))\n",
    "        return transition\n",
    "                    \n",
    "    def step(self, action, speed=-1):\n",
    "        # assume we use the max speed as the default speed, when come near to the opt-position, we can slow down the speed\n",
    "        if speed < 0 or speed >= self.UAV_speed:\n",
    "            speed = self.UAV_speed\n",
    "            \n",
    "        self.UAV_path.append(self.UAV_current_pos)\n",
    "        self.UAV_current_pos = self.transition_dynamics(action, speed, self.UAV_current_pos)\n",
    "        self.current_step = self.current_step + 1\n",
    "        done = False\n",
    "        if self.current_step == self.total_steps:\n",
    "            done =  True\n",
    "        return self.UAV_current_pos, self.get_reward(self.UAV_current_pos), done\n",
    "    \n",
    "    def action_sample(self):\n",
    "        return choice(self.action_space)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        return self.UAV_current_pos\n",
    "        \n",
    "    def print_attribute(self):\n",
    "        attrs = vars(self)\n",
    "        print(', '.join(\"%s: %s\" % item for item in attrs.items()))\n",
    "        \n",
    "    def print_locations(self):\n",
    "        print(\"UAV position is: {}\".format(self.UAV_current_pos))\n",
    "        print(\"Users position are: {}\".format(self.users_pos))\n",
    "        \n",
    "    def print_map(self):\n",
    "        x_list = [pos[0] for pos in self.users_pos]\n",
    "        y_list = [pos[1] for pos in self.users_pos]\n",
    "        x_list.append(self.UAV_current_pos[0])\n",
    "        y_list.append(self.UAV_current_pos[1])\n",
    "        for i in range(0, len(self.UAV_path)):\n",
    "            x_list.append(self.UAV_path[i][0])\n",
    "            y_list.append(self.UAV_path[i][1])\n",
    "        \n",
    "        colors = np.array([\"red\", \"green\", \"blue\"])\n",
    "        sizes = []\n",
    "        colors_map = []\n",
    "        for i in range(0, self.number_of_user):\n",
    "            sizes.append(25)\n",
    "            colors_map.append(1)\n",
    "        sizes.append(50)\n",
    "        colors_map.append(0)\n",
    "        for i in range(0, len(self.UAV_path)):\n",
    "            sizes.append(10)\n",
    "            colors_map.append(2)\n",
    "        for i in range(0, len(self.UAV_path) - 1):\n",
    "            x_values = [self.UAV_path[i][0], self.UAV_path[i+1][0]]\n",
    "            y_values = [self.UAV_path[i][1], self.UAV_path[i+1][1]]\n",
    "            plt.plot(x_values, y_values, 'b')\n",
    "        plt.scatter(x_list, y_list, c=colors[colors_map], s=sizes) \n",
    "        plt.axis([0, self.map[\"width\"], 0, self.map[\"length\"]])\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_config = dict(\n",
    "    total_steps = 50,\n",
    "    random_seed = 21,\n",
    "    is_random_env = False,\n",
    "    map=dict(\n",
    "        width=1000,\n",
    "        length=1000,\n",
    "        height=100\n",
    "    ),\n",
    "    number_of_user = 10,\n",
    "    UAV_speed = 50,\n",
    "    UAV_initial_pos = (0, 0),\n",
    "    wireless_parameter = dict(\n",
    "        g0 = 10 ** (-5),\n",
    "        B = 10 ** (6),\n",
    "        Pk = 0.1,\n",
    "        noise = 10 ** (-9)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(policy, config, num_episodes=1, render=False):\n",
    "    env = UAVEnvironment(config)\n",
    "    \n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        # all policy will return a direction and a speed\n",
    "        act_direction, act_speed = policy(obs)\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            obs, reward, done = env.step(act_direction, act_speed)\n",
    "            act_direction, act_speed = policy(obs)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            if render == True:\n",
    "                clear_output(wait=True)\n",
    "                env.print_attribute()\n",
    "                print(\"Current Step: {}\".format(env.current_step))\n",
    "                print(\"Policy choice direction: {}, speed: {}\".format(act_direction, act_speed))\n",
    "                print(\"UAV current position x: {}, y: {}\".format(env.UAV_current_pos[0], env.UAV_current_pos[1]))\n",
    "                print(\"Current step reward: {}, episodes rewards: {}\".format(reward, ep_reward))\n",
    "                env.print_map()\n",
    "                wait(sleep=0.2)\n",
    "        rewards.append(ep_reward)\n",
    "    return np.mean(rewards)\n",
    "\n",
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Class for all RL algorithm\n",
    "class UAVTrainer: \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env = UAVEnvironment(self.config)\n",
    "\n",
    "# Value Iteration, Tabular, transition dynamic is known, assume only use fixed speed to reduce action space\n",
    "class UAVTrainerValueIteration(UAVTrainer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.transitions = self.env.get_transition()\n",
    "        self.q_table = []\n",
    "        self.obs_dim = (int(self.env.map[\"width\"] / self.env.UAV_speed), int(self.env.map[\"length\"] / self.env.UAV_speed))\n",
    "        self.act_dim = len(self.env.action_space)\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        \n",
    "        for x in range(0, self.obs_dim[0]+1):\n",
    "            self.q_table.append(list())\n",
    "            for y in range(0, self.obs_dim[1]+1):\n",
    "                self.q_table[x].append(0)\n",
    "            \n",
    "    def get_transition(self, state, act):\n",
    "        transition = self.transitions[state[0]][state[1]][act]\n",
    "        return transition[\"next_state\"], transition[\"reward\"]\n",
    "    \n",
    "    def print_transitions(self):\n",
    "        print(\"Transition width {}, length {}, number of act {}\".format(len(self.transitions), len(self.transitions[0]), len(self.transitions[0][0])))\n",
    "        print(self.transitions)\n",
    "        \n",
    "    def print_table(self):\n",
    "        for j in range(len(self.q_table[0])-1, -1, -1):\n",
    "            for i in range(0, len(self.q_table)):\n",
    "                print(self.q_table[i][j], end =\" \")\n",
    "            print(\"\")\n",
    "            \n",
    "            \n",
    "    def copy_current_table(self):\n",
    "        old_table = []\n",
    "        for x in range(0, self.obs_dim[0]+1):\n",
    "            old_table.append(list())\n",
    "            for y in range(0, self.obs_dim[1]+1):\n",
    "                old_table[x].append(self.q_table[x][y])\n",
    "        return old_table\n",
    "\n",
    "    def update_value_function(self):\n",
    "        old_table = self.copy_current_table()\n",
    "        for state_x in range(self.obs_dim[0] + 1):\n",
    "            for state_y in range(self.obs_dim[1] + 1):\n",
    "                state_value = 0\n",
    "                state_action_values = [0 for i in range(0, self.act_dim)]\n",
    "\n",
    "                for act in range(self.act_dim):\n",
    "                    next_state, reward = self.get_transition((state_x, state_y), act)\n",
    "                    table_x = int(next_state[0] / self.env.UAV_speed)\n",
    "                    table_y = int(next_state[1] / self.env.UAV_speed)\n",
    "                    #print(table_x, table_y)\n",
    "                    state_action_values[act] = state_action_values[act] + reward + self.gamma * old_table[table_x][table_y]   \n",
    "                state_value = np.max(state_action_values)\n",
    "                self.q_table[state_x][state_y] = state_value\n",
    "                #print(\"Update x: {}, y: {} to value {}\".format(state_x, state_y, state_value))\n",
    "            \n",
    "    def train(self):\n",
    "        old_state_value_table = self.copy_current_table()\n",
    "        current_step = 0\n",
    "        while current_step < self.config['max_iteration']:  \n",
    "            current_step = current_step + 1\n",
    "            self.update_value_function()\n",
    "            if current_step % self.config[\"evaluate_interval\"] == 0:\n",
    "                print(\"Iteration {}, Mean Reward is: {}\".format(current_step, evaluate(self.policy, config = self.config, num_episodes=1, render=False)))\n",
    "                #print(\"Iteration {}, Mean Reward is: {}\".format(current_step, 0))\n",
    "                # check exist\n",
    "                stop = True\n",
    "                flag = 0\n",
    "                for x in range(self.obs_dim[0] + 1):\n",
    "                    for y in range(self.obs_dim[1] + 1):\n",
    "                        if abs(self.q_table[x][y] - old_state_value_table[x][y]) > self.config[\"return_threshold\"]:\n",
    "                            stop = False\n",
    "                            flag = 1\n",
    "                    if flag == 1:\n",
    "                        break\n",
    "                if stop == True:\n",
    "                    print(\"Train converge at i = {}\".format(current_step))\n",
    "                    current_step = self.config['max_iteration']\n",
    "                else:\n",
    "                    old_state_value_table = self.copy_current_table()\n",
    "\n",
    "    def policy(self, obs):\n",
    "        table_x = int(obs[0] / self.env.UAV_speed)\n",
    "        table_y = int(obs[1] / self.env.UAV_speed)\n",
    "        next_state_value_list = []\n",
    "        for act in range(0, self.act_dim):\n",
    "            next_state, reward = self.get_transition((table_x, table_y), act)\n",
    "            next_state_x = int(next_state[0] / self.env.UAV_speed)\n",
    "            next_state_y = int(next_state[1] / self.env.UAV_speed)\n",
    "            next_state_value_list.append(self.q_table[next_state_x][next_state_y])\n",
    "        act = np.argmax(next_state_value_list)\n",
    "        return act, self.env.UAV_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space: (0, 1, 2, 3), total_steps: 50, current_step: 49, random_seed: 21, map: {'width': 1000, 'length': 1000, 'height': 100}, UAV_speed: 50, UAV_initial_pos: (0, 0), UAV_current_pos: (200, 550), number_of_user: 10, users_pos: [(168, 428), (706, 428), (650, 288), (490, 863), (221, 811), (486, 827), (986, 524), (187, 517), (540, 241), (807, 3)], UAV_path: [(0, 0), (0, 50), (0, 100), (0, 150), (0, 200), (0, 250), (0, 300), (0, 350), (50, 350), (50, 400), (100, 400), (100, 450), (150, 450), (150, 500), (200, 500), (200, 550), (200, 500), (200, 550), (200, 500), (200, 550), (200, 500), (200, 550), (200, 500), (200, 550), (200, 500), (200, 550), (200, 500), (200, 550), (200, 500), (200, 550), (200, 500), (200, 550), (200, 500), (200, 550), (200, 500), (200, 550), (200, 500), (200, 550), (200, 500), (200, 550), (200, 500), (200, 550), (200, 500), (200, 550), (200, 500), (200, 550), (200, 500), (200, 550), (200, 500)], g0: 1e-05, B: 1000000, Pk: 0.1, noise: 1e-09\n",
      "Current Step: 49\n",
      "Policy choice direction: 2, speed: 50\n",
      "UAV current position x: 200, y: 550\n",
      "Current step reward: 0.023286666173113396, episodes rewards: 1.0466230129000464\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward is: 1.0734336387468804\n"
     ]
    }
   ],
   "source": [
    "value_iteration_config = merge_config(dict(\n",
    "    max_iteration=10000,\n",
    "    evaluate_interval=100,  # don't need to update policy each iteration\n",
    "    gamma=0.9,\n",
    "    return_threshold=1,\n",
    "), environment_config)\n",
    "trainer = UAVTrainerValueIteration(value_iteration_config)\n",
    "trainer.env.print_locations()\n",
    "trainer.train()\n",
    "\n",
    "print(\"Mean Reward is: {}\".format(evaluate(trainer.policy, config = value_iteration_config, num_episodes=1, render=True))) # Enable render=True can see the agent behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rest is we try to create more complex environment, with more action space and state, not finished yet, please ignore the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "class UAVEnvironmentComplex(UAVEnvironment):\n",
    "    \"\"\"\n",
    "    Complex UAV environment, action can be compose as (speed, direction), UAV position can be continous float number\n",
    "    \n",
    "    State = (self.UAV_current_pos, self.users_pos [list])\n",
    "    Action = (speed, direction)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(UAVEnvironmentComplex, self).__init__(config)\n",
    "        delattr(self, \"action_space\") \n",
    "\n",
    "\n",
    "    def transition_dynamics(self, action, state):\n",
    "        # action = (speed, direction), speed with 0 and self.UAV_speed, direction with (0, 2 * pi)\n",
    "        speed = action[0]\n",
    "        direction = action[1]\n",
    "        next_x = self.UAV_current_pos[0] + speed * math.cos(direction)\n",
    "        next_y = self.UAV_current_pos[1] + speed * math.sin(direction)\n",
    "        next_x = max(0, next_x)\n",
    "        next_x = min(self.map[\"width\"], next_x)\n",
    "        next_y = max(0, next_y)\n",
    "        next_y = min(self.map[\"length\"], next_y)\n",
    "        return (next_x, next_y)\n",
    "  \n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        return (self.UAV_current_pos, self.users_pos)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # action = (speed, direction)\n",
    "        # This function return the state = (self.UAV_current_pos, self.users_pos [list])\n",
    "        speed = action[0]\n",
    "        speed = max(0, speed)\n",
    "        speed = min(self.UAV_speed, speed)\n",
    "        \n",
    "        standarded_action = (speed, action[1])\n",
    "        self.UAV_current_pos = self.transition_dynamics(standarded_action, self.UAV_current_pos)\n",
    "        self.current_step = self.current_step + 1\n",
    "        done = False\n",
    "        if self.current_step == self.total_steps:\n",
    "            done =  True\n",
    "        state = (self.UAV_current_pos, self.users_pos)\n",
    "        reward = self.get_reward(self.UAV_current_pos)\n",
    "        return state, reward, done\n",
    "    \n",
    "    def action_sample(self):\n",
    "        speed = random.uniform(0, 1) * self.UAV_speed\n",
    "        random_direction = math.pi * 2 * random.uniform(0, 1)\n",
    "        action = (speed, random_direction)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAV position is: (0, 0)\n",
      "Users position are: [(927, 740), (702, 805), (784, 901), (926, 154), (266, 690), (650, 869), (926, 103), (893, 335), (586, 927), (173, 27)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "myGame = UAVEnvironmentComplex(environment_config)\n",
    "myGame.print_locations()\n",
    "myGame.print_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_complex(policy, config, num_episodes=1, render=False):\n",
    "    env = UAVEnvironmentComplex(config)\n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        # all policy will return a direction and a speed\n",
    "        action = policy(obs)\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            obs, reward, done = env.step(action)\n",
    "            action = policy(obs)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            if render == True:\n",
    "                clear_output(wait=True)\n",
    "                env.print_locations()\n",
    "                print(\"Current Step: {}\".format(env.current_step))\n",
    "                print(\"Policy choice direction: {}, speed: {}\".format((action[1] * 180 / math.pi), action[0]))\n",
    "                print(\"Current step reward: {}, episodes rewards: {}\".format(reward, ep_reward))\n",
    "                env.print_map()\n",
    "                wait(sleep=0.2)\n",
    "        rewards.append(ep_reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Class for all RL algorithm\n",
    "class UAVComplexTrainer: \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env = UAVEnvironmentComplex(self.config)\n",
    "            \n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def compute_values(self, state):\n",
    "        pass\n",
    "    \n",
    "    def state_to_feature_vector(self, UAV_pos, User_pos_list):\n",
    "        # State (UAV_pos, Users_pos), assmue there is max_number_of_user which UAV can be server, to define the feature vector length\n",
    "        # Feature vector: a vector contain tuple of positions\n",
    "        capacity = self.config[\"max_number_of_user\"] + 1 # UAV + user\n",
    "        features = [0] * capacity * 2\n",
    "        features[0] = UAV_pos[0]\n",
    "        features[1] = UAV_pos[1]\n",
    "        for i in range(0, len(User_pos_list)):\n",
    "            features[2*i+2] = User_pos_list[i][0]\n",
    "            features[2*i+3] = User_pos_list[i][1]\n",
    "        return features\n",
    "\n",
    "    def policy(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAV position is: (97.45900719243718, 133.47713238386874)\n",
      "Users position are: [(927, 740), (702, 805), (784, 901), (926, 154), (266, 690), (650, 869), (926, 103), (893, 335), (586, 927), (173, 27)]\n",
      "Current Step: 49\n",
      "Policy choice direction: 304.95088960378666, speed: 20.253410541370233\n",
      "Current step reward: 0.07053718576216651, episodes rewards: 3.167014290895861\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward is: 3.2487254128836653\n"
     ]
    }
   ],
   "source": [
    "# Start from random policy\n",
    "class UAVComplexTrainerRandomPolicy(UAVComplexTrainer):\n",
    "    def __init__(self, config):\n",
    "        super(UAVComplexTrainerRandomPolicy, self).__init__(config)\n",
    "        \n",
    "    def policy(self, obs):\n",
    "        action = self.env.action_sample()\n",
    "        return action\n",
    "\n",
    "random_policy_config = environment_config\n",
    "trainer = UAVComplexTrainerRandomPolicy(random_policy_config)\n",
    "print(\"Mean Reward is: {}\".format(evaluate_complex(trainer.policy, config = random_policy_config, num_episodes=1, render=True))) # Enable render=True can see the agent behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def to_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        x = np.array(x) \n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.float32)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    return x\n",
    "\n",
    "\n",
    "class NetworkModel(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(NetworkModel, self).__init__()\n",
    "        self.network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(obs_dim,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,act_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.network(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Class for all RL algorithm\n",
    "class UAVComplexTrainerNN(UAVComplexTrainer): \n",
    "    def __init__(self, config):\n",
    "        super(UAVComplexTrainerNN, self).__init__(config)\n",
    "        self.max_episode_length = self.config[\"max_episode_length\"]\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.gamma = self.config[\"gamma\"]\n",
    "        self.model = NetworkModel((self.config[\"max_number_of_user\"] + 1) * 2, 2)\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def policy(self, state):\n",
    "        if np.random.uniform(0,1) <= self.config[\"eps\"]:\n",
    "            action = self.env.action_sample()\n",
    "        else:\n",
    "            feature = self.state_to_feature_vector(state[0], state[1])\n",
    "            model_input = to_tensor(feature).squeeze()\n",
    "            action = self.model(model_input)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UAVComplexTrainerNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-ec4a819669ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m ), environment_config)\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mNNTrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUAVComplexTrainerNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_function_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mean Reward is: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNNTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_function_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Enable render=True can see the agent behavior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'UAVComplexTrainerNN' is not defined"
     ]
    }
   ],
   "source": [
    "linear_function_config = merge_config(dict(\n",
    "    total_steps = 50,\n",
    "    number_of_user = 10,\n",
    "    max_number_of_user = 20,\n",
    "    map=dict(\n",
    "        width=1000,\n",
    "        length=1000,\n",
    "        height=100\n",
    "    ),\n",
    "    max_episode_length=10000,\n",
    "    eps=0.01,\n",
    "    gamma=0.9,\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "), environment_config)\n",
    "\n",
    "NNTrainer = UAVComplexTrainerNN(linear_function_config)\n",
    "print(\"Mean Reward is: {}\".format(evaluate_complex(NNTrainer.policy, config = linear_function_config, num_episodes=1, render=True))) # Enable render=True can see the agent behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "s = np.random.normal(mu, sigma, (10,3))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
