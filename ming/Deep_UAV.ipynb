{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import random\n",
    "from random import randint, choice\n",
    "import matplotlib.pyplot as plt\n",
    "from util import *\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAVEnvironment():\n",
    "    \"\"\"\n",
    "    Game environment for UAV test\n",
    "    \n",
    "    ---Map---\n",
    "    \n",
    "    y-axis(length)\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "     _______________________ x-axis(width)\n",
    "     \n",
    "    Hight is a fixed value\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        # Game config\n",
    "        self.action_space = (0, 1, 2, 3) # up, right, down, left, total 4 actions\n",
    "        self.total_steps = config[\"total_steps\"] # when the game end\n",
    "        self.current_step = 0\n",
    "        if config[\"is_random_env\"] == False:\n",
    "            self.random_seed = config[\"random_seed\"]\n",
    "            random.seed(self.random_seed)\n",
    "        \n",
    "        # Map config\n",
    "        self.map = dict(width=config[\"map\"][\"width\"], length=config[\"map\"][\"length\"], height=config[\"map\"][\"height\"])\n",
    "        self.UAV_speed = config[\"UAV_speed\"]\n",
    "        self.UAV_initial_pos = config[\"UAV_initial_pos\"] # a tuple\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        self.number_of_user = config[\"number_of_user\"]\n",
    "        self.users_pos = list()\n",
    "        for i in range(0, self.number_of_user):\n",
    "            self.users_pos.append((randint(0, self.map[\"width\"]), randint(0, self.map[\"length\"])))\n",
    "        \n",
    "        # Wireless config\n",
    "        self.g0 = config[\"wireless_parameter\"][\"g0\"]\n",
    "        self.B = config[\"wireless_parameter\"][\"B\"]\n",
    "        self.Pk = config[\"wireless_parameter\"][\"Pk\"]\n",
    "        self.noise = config[\"wireless_parameter\"][\"noise\"]\n",
    "        \n",
    "    def get_reward(self, UAV_pos):\n",
    "        # One step Reward is define as the summation of all user's utility\n",
    "        reward = 0\n",
    "        for user_index in range(0, self.number_of_user):\n",
    "            gkm = self.g0 / (self.map[\"height\"] ** 2 + (UAV_pos[0] - self.users_pos[user_index][0]) ** 2 + (UAV_pos[1] - self.users_pos[user_index][1]) ** 2)\n",
    "            user_utility = self.B * math.log(1 + self.Pk * gkm / self.noise, 2)\n",
    "            reward = reward + user_utility\n",
    "        return reward / (10 ** 6) # Use Mkbps as signal basic unit\n",
    "    \n",
    "    def transition_dynamics(self, action, speed, state):\n",
    "        # given the action (direction), calculate the next state (UAV current position)\n",
    "        assert action in self.action_space\n",
    "        next_UAV_pos = list(state)\n",
    "        if action == 0:\n",
    "            # move up\n",
    "            next_UAV_pos[1] = min(next_UAV_pos[1] + speed, self.map[\"length\"])\n",
    "        if action == 1:\n",
    "            # move right\n",
    "            next_UAV_pos[0] = min(next_UAV_pos[0] + speed, self.map[\"width\"])\n",
    "        if action == 2:\n",
    "            # move down\n",
    "            next_UAV_pos[1] = max(next_UAV_pos[1] - speed, 0)\n",
    "        if action == 3:\n",
    "            # move left\n",
    "            next_UAV_pos[0] = max(next_UAV_pos[0] - speed, 0)\n",
    "        return tuple(next_UAV_pos)\n",
    "    \n",
    "    def get_transition(self):\n",
    "        # This function only works for model based, we are trying to disable this function to try more algorithm\n",
    "        # Return a table of transition, we assume UAV use fixed flying speed\n",
    "        \"\"\"\n",
    "        Structure:\n",
    "        transition[\n",
    "            x_0[\n",
    "                y_0[\n",
    "                    {next_state, reward}, # for action 1\n",
    "                    {next_state, reward}, # for action 2\n",
    "                    ...\n",
    "                    {next_state, reward}, # for action 20\n",
    "                ],\n",
    "                y_1*v[],\n",
    "                ...\n",
    "                y_h-1*v[]\n",
    "            ],\n",
    "            x_1*v[],\n",
    "            x_2*v[],\n",
    "            ...\n",
    "            x_w-1*v[]\n",
    "        ]\n",
    "        \"\"\"\n",
    "        transition = list()\n",
    "        for state_x in range(0, int(self.map[\"width\"] / self.UAV_speed) + 1):\n",
    "            transition.append(list())\n",
    "            for state_y in range(0, int(self.map[\"length\"] / self.UAV_speed) + 1):\n",
    "                transition[state_x].append(list())\n",
    "                for action in self.action_space:\n",
    "                    next_state = self.transition_dynamics(action, self.UAV_speed, (state_x * self.UAV_speed, state_y * self.UAV_speed))\n",
    "                    reward = self.get_reward(next_state)\n",
    "                    transition[state_x][state_y].append(dict(next_state=next_state,reward=reward))\n",
    "        return transition\n",
    "                    \n",
    "    def step(self, action, speed=-1):\n",
    "        # assume we use the max speed as the default speed, when come near to the opt-position, we can slow down the speed\n",
    "        if speed < 0 or speed >= self.UAV_speed:\n",
    "            speed = self.UAV_speed\n",
    "            \n",
    "        self.UAV_current_pos = self.transition_dynamics(action, speed, self.UAV_current_pos)\n",
    "        self.current_step = self.current_step + 1\n",
    "        done = False\n",
    "        if self.current_step == self.total_steps:\n",
    "            done =  True\n",
    "        return self.UAV_current_pos, self.get_reward(self.UAV_current_pos), done\n",
    "    \n",
    "    def action_sample(self):\n",
    "        return choice(self.action_space)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        return self.UAV_current_pos\n",
    "        \n",
    "    def print_attribute(self):\n",
    "        attrs = vars(self)\n",
    "        print(', '.join(\"%s: %s\" % item for item in attrs.items()))\n",
    "        \n",
    "    def print_locations(self):\n",
    "        print(\"UAV position is: {}\".format(self.UAV_current_pos))\n",
    "        print(\"Users position are: {}\".format(self.users_pos))\n",
    "        \n",
    "    def print_map(self):\n",
    "        x_list = [pos[0] for pos in self.users_pos]\n",
    "        y_list = [pos[1] for pos in self.users_pos]\n",
    "        x_list.append(self.UAV_current_pos[0])\n",
    "        y_list.append(self.UAV_current_pos[1])\n",
    "        \n",
    "        colors = np.array([\"red\", \"green\"])\n",
    "        sizes = []\n",
    "        colors_map = []\n",
    "        for i in range(0, self.number_of_user):\n",
    "            sizes.append(25)\n",
    "            colors_map.append(1)\n",
    "        sizes.append(50)\n",
    "        colors_map.append(0)\n",
    "        plt.scatter(x_list, y_list, c=colors[colors_map], s=sizes) \n",
    "        plt.axis([0, self.map[\"width\"], 0, self.map[\"length\"]])\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_config = dict(\n",
    "    total_steps = 50,\n",
    "    random_seed = 0,\n",
    "    is_random_env = True,\n",
    "    map=dict(\n",
    "        width=1000,\n",
    "        length=1000,\n",
    "        height=100\n",
    "    ),\n",
    "    number_of_user = 10,\n",
    "    UAV_speed = 20,\n",
    "    UAV_initial_pos = (0, 0),\n",
    "    wireless_parameter = dict(\n",
    "        g0 = 10 ** (-5),\n",
    "        B = 10 ** (6),\n",
    "        Pk = 0.1,\n",
    "        noise = 10 ** (-9)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAV position is: (0, 0)\n",
      "Users position are: [(769, 500), (922, 202), (937, 711), (650, 932), (863, 142), (829, 55), (355, 720), (919, 371), (524, 185), (434, 149)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "myGame = UAVEnvironment(environment_config)\n",
    "myGame.print_locations()\n",
    "myGame.print_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(policy, config, num_episodes=1, render=False):\n",
    "    env = UAVEnvironment(config)\n",
    "    \n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        # all policy will return a direction and a speed\n",
    "        act_direction, act_speed = policy(obs)\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            obs, reward, done = env.step(act_direction, act_speed)\n",
    "            act_direction, act_speed = policy(obs)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            if render == True:\n",
    "                clear_output(wait=True)\n",
    "                env.print_attribute()\n",
    "                print(\"Current Step: {}\".format(env.current_step))\n",
    "                print(\"Policy choice direction: {}, speed: {}\".format(act_direction, act_speed))\n",
    "                print(\"UAV current position x: {}, y: {}\".format(env.UAV_current_pos[0], env.UAV_current_pos[1]))\n",
    "                print(\"Current step reward: {}, episodes rewards: {}\".format(reward, ep_reward))\n",
    "                env.print_map()\n",
    "                wait(sleep=0.2)\n",
    "        rewards.append(ep_reward)\n",
    "    return np.mean(rewards)\n",
    "\n",
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAV position is: (0, 0)\n",
      "Users position are: [(910, 248), (936, 339), (771, 394), (85, 553), (221, 526), (8, 13), (773, 291), (61, 643), (741, 908), (580, 365)]\n",
      "Mean Reward is: 2.2223227801873096\n"
     ]
    }
   ],
   "source": [
    "# Basic Class for all RL algorithm\n",
    "class UAVTrainer: \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env = UAVEnvironment(self.config)\n",
    "\n",
    "# Start from random policy\n",
    "class UAVTrainerRandomPolicy(UAVTrainer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "    def policy(self, obs):\n",
    "        max_speed = self.env.UAV_speed\n",
    "        return self.env.action_sample(), max_speed\n",
    "\n",
    "random_policy_config = environment_config\n",
    "trainer = UAVTrainerRandomPolicy(random_policy_config)\n",
    "trainer.env.print_locations()\n",
    "print(\"Mean Reward is: {}\".format(evaluate(trainer.policy, config = random_policy_config, num_episodes=1, render=False))) # Enable render=True can see the agent behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAV position is: (0, 0)\n",
      "Users position are: [(585, 33), (439, 494), (591, 15), (211, 473), (832, 503), (843, 284), (669, 830), (164, 35), (533, 501), (335, 77)]\n",
      "Iteration 100, Mean Reward is: 12.21220913177311\n",
      "Iteration 200, Mean Reward is: 12.21220913177311\n",
      "Train converge at i = 200\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Value Iteration, Tabular, transition dynamic is known, assume only use fixed speed to reduce action space\n",
    "class UAVTrainerValueIteration(UAVTrainer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.transitions = self.env.get_transition()\n",
    "        self.q_table = []\n",
    "        self.obs_dim = (int(self.env.map[\"width\"] / self.env.UAV_speed), int(self.env.map[\"length\"] / self.env.UAV_speed))\n",
    "        self.act_dim = len(self.env.action_space)\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        \n",
    "        for x in range(0, self.obs_dim[0]+1):\n",
    "            self.q_table.append(list())\n",
    "            for y in range(0, self.obs_dim[1]+1):\n",
    "                self.q_table[x].append(0)\n",
    "            \n",
    "    def get_transition(self, state, act):\n",
    "        transition = self.transitions[state[0]][state[1]][act]\n",
    "        return transition[\"next_state\"], transition[\"reward\"]\n",
    "    \n",
    "    def print_transitions(self):\n",
    "        print(\"Transition width {}, length {}, number of act {}\".format(len(self.transitions), len(self.transitions[0]), len(self.transitions[0][0])))\n",
    "        print(self.transitions)\n",
    "        \n",
    "    def print_table(self):\n",
    "        for j in range(len(self.q_table[0])-1, -1, -1):\n",
    "            for i in range(0, len(self.q_table)):\n",
    "                print(self.q_table[i][j], end =\" \")\n",
    "            print(\"\")\n",
    "            \n",
    "            \n",
    "    def copy_current_table(self):\n",
    "        old_table = []\n",
    "        for x in range(0, self.obs_dim[0]+1):\n",
    "            old_table.append(list())\n",
    "            for y in range(0, self.obs_dim[1]+1):\n",
    "                old_table[x].append(self.q_table[x][y])\n",
    "        return old_table\n",
    "\n",
    "    def update_value_function(self):\n",
    "        old_table = self.copy_current_table()\n",
    "        for state_x in range(self.obs_dim[0] + 1):\n",
    "            for state_y in range(self.obs_dim[1] + 1):\n",
    "                state_value = 0\n",
    "                state_action_values = [0 for i in range(0, self.act_dim)]\n",
    "\n",
    "                for act in range(self.act_dim):\n",
    "                    next_state, reward = self.get_transition((state_x, state_y), act)\n",
    "                    table_x = int(next_state[0] / self.env.UAV_speed)\n",
    "                    table_y = int(next_state[1] / self.env.UAV_speed)\n",
    "                    #print(table_x, table_y)\n",
    "                    state_action_values[act] = state_action_values[act] + reward + self.gamma * old_table[table_x][table_y]   \n",
    "                state_value = np.max(state_action_values)\n",
    "                self.q_table[state_x][state_y] = state_value\n",
    "                #print(\"Update x: {}, y: {} to value {}\".format(state_x, state_y, state_value))\n",
    "            \n",
    "    def train(self):\n",
    "        old_state_value_table = self.copy_current_table()\n",
    "        current_step = 0\n",
    "        while current_step < self.config['max_iteration']:  \n",
    "            current_step = current_step + 1\n",
    "            self.update_value_function()\n",
    "            if current_step % self.config[\"evaluate_interval\"] == 0:\n",
    "                print(\"Iteration {}, Mean Reward is: {}\".format(current_step, evaluate(self.policy, config = self.config, num_episodes=1, render=False)))\n",
    "                #print(\"Iteration {}, Mean Reward is: {}\".format(current_step, 0))\n",
    "                # check exist\n",
    "                stop = True\n",
    "                flag = 0\n",
    "                for x in range(self.obs_dim[0] + 1):\n",
    "                    for y in range(self.obs_dim[1] + 1):\n",
    "                        if abs(self.q_table[x][y] - old_state_value_table[x][y]) > self.config[\"return_threshold\"]:\n",
    "                            stop = False\n",
    "                            flag = 1\n",
    "                    if flag == 1:\n",
    "                        break\n",
    "                if stop == True:\n",
    "                    print(\"Train converge at i = {}\".format(current_step))\n",
    "                    current_step = self.config['max_iteration']\n",
    "                else:\n",
    "                    old_state_value_table = self.copy_current_table()\n",
    "\n",
    "    def policy(self, obs):\n",
    "        table_x = int(obs[0] / self.env.UAV_speed)\n",
    "        table_y = int(obs[1] / self.env.UAV_speed)\n",
    "        next_state_value_list = []\n",
    "        for act in range(0, self.act_dim):\n",
    "            next_state, reward = self.get_transition((table_x, table_y), act)\n",
    "            next_state_x = int(next_state[0] / self.env.UAV_speed)\n",
    "            next_state_y = int(next_state[1] / self.env.UAV_speed)\n",
    "            next_state_value_list.append(self.q_table[next_state_x][next_state_y])\n",
    "        act = np.argmax(next_state_value_list)\n",
    "        return act, self.env.UAV_speed\n",
    "\n",
    "value_iteration_config = merge_config(dict(\n",
    "    max_iteration=10000,\n",
    "    evaluate_interval=100,  # don't need to update policy each iteration\n",
    "    gamma=0.9,\n",
    "    return_threshold=1,\n",
    "    random_seed = 10,\n",
    "    is_random_env = False\n",
    "), environment_config)\n",
    "trainer = UAVTrainerValueIteration(value_iteration_config)\n",
    "trainer.env.print_locations()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward is: 12.21220913177311\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Reward is: {}\".format(evaluate(trainer.policy, config = value_iteration_config, num_episodes=1, render=False))) # Enable render=True can see the agent behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "class UAVEnvironmentComplex(UAVEnvironment):\n",
    "    \"\"\"\n",
    "    Complex UAV environment, action can be compose as (speed, direction), UAV position can be continous float number\n",
    "    \n",
    "    State = (self.UAV_current_pos, self.users_pos [list])\n",
    "    Action = (speed, direction)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(UAVEnvironmentComplex, self).__init__(config)\n",
    "        delattr(self, \"action_space\") \n",
    "\n",
    "\n",
    "    def transition_dynamics(self, action, state):\n",
    "        # action = (speed, direction), speed with 0 and self.UAV_speed, direction with (0, 2 * pi)\n",
    "        speed = action[0]\n",
    "        direction = action[1]\n",
    "        next_x = self.UAV_current_pos[0] + speed * math.cos(direction)\n",
    "        next_y = self.UAV_current_pos[1] + speed * math.sin(direction)\n",
    "        next_x = max(0, next_x)\n",
    "        next_x = min(self.map[\"width\"], next_x)\n",
    "        next_y = max(0, next_y)\n",
    "        next_y = min(self.map[\"length\"], next_y)\n",
    "        return (next_x, next_y)\n",
    "  \n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        return (self.UAV_current_pos, self.users_pos)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # action = (speed, direction)\n",
    "        # This function return the state = (self.UAV_current_pos, self.users_pos [list])\n",
    "        speed = action[0]\n",
    "        speed = max(0, speed)\n",
    "        speed = min(self.UAV_speed, speed)\n",
    "        \n",
    "        standarded_action = (speed, action[1])\n",
    "        self.UAV_current_pos = self.transition_dynamics(standarded_action, self.UAV_current_pos)\n",
    "        self.current_step = self.current_step + 1\n",
    "        done = False\n",
    "        if self.current_step == self.total_steps:\n",
    "            done =  True\n",
    "        state = (self.UAV_current_pos, self.users_pos)\n",
    "        reward = self.get_reward(self.UAV_current_pos)\n",
    "        return state, reward, done\n",
    "    \n",
    "    def action_sample(self):\n",
    "        speed = random.uniform(0, 1) * self.UAV_speed\n",
    "        random_direction = math.pi * 2 * random.uniform(0, 1)\n",
    "        action = (speed, random_direction)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAV position is: (0, 0)\n",
      "Users position are: [(469, 86), (907, 661), (809, 302), (742, 270), (471, 830), (442, 842), (806, 67), (993, 93), (526, 367), (501, 452)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "myGame = UAVEnvironmentComplex(environment_config)\n",
    "myGame.print_locations()\n",
    "myGame.print_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_complex(policy, config, num_episodes=1, render=False):\n",
    "    env = UAVEnvironmentComplex(config)\n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        # all policy will return a direction and a speed\n",
    "        action = policy(obs)\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            obs, reward, done = env.step(action)\n",
    "            action = policy(obs)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            if render == True:\n",
    "                clear_output(wait=True)\n",
    "                env.print_locations()\n",
    "                print(\"Current Step: {}\".format(env.current_step))\n",
    "                print(\"Policy choice direction: {}, speed: {}\".format((action[1] * 180 / math.pi), action[0]))\n",
    "                print(\"Current step reward: {}, episodes rewards: {}\".format(reward, ep_reward))\n",
    "                env.print_map()\n",
    "                wait(sleep=0.2)\n",
    "        rewards.append(ep_reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Class for all RL algorithm\n",
    "class UAVComplexTrainer: \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env = UAVEnvironmentComplex(self.config)\n",
    "            \n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def compute_values(self, state):\n",
    "        pass\n",
    "    \n",
    "    def state_to_feature_vector(self, UAV_pos, User_pos_list):\n",
    "        # State (UAV_pos, Users_pos), assmue there is max_number_of_user which UAV can be server, to define the feature vector length\n",
    "        # Feature vector: a vector contain tuple of positions\n",
    "        capacity = self.config[\"max_number_of_user\"] + 1 # UAV + user\n",
    "        features = [0] * capacity * 2\n",
    "        features[0] = UAV_pos[0]\n",
    "        features[1] = UAV_pos[1]\n",
    "        for i in range(0, len(User_pos_list)):\n",
    "            features[2*i+2] = User_pos_list[i][0]\n",
    "            features[2*i+3] = User_pos_list[i][1]\n",
    "        return features\n",
    "\n",
    "    def policy(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward is: 1.4088875274428188\n"
     ]
    }
   ],
   "source": [
    "# Start from random policy\n",
    "class UAVComplexTrainerRandomPolicy(UAVComplexTrainer):\n",
    "    def __init__(self, config):\n",
    "        super(UAVComplexTrainerRandomPolicy, self).__init__(config)\n",
    "        \n",
    "    def policy(self, obs):\n",
    "        action = self.env.action_sample()\n",
    "        return action\n",
    "\n",
    "random_policy_config = environment_config\n",
    "trainer = UAVComplexTrainerRandomPolicy(random_policy_config)\n",
    "print(\"Mean Reward is: {}\".format(evaluate_complex(trainer.policy, config = random_policy_config, num_episodes=1, render=False))) # Enable render=True can see the agent behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def to_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        x = np.array(x) \n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.float32)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    return x\n",
    "\n",
    "\n",
    "class NetworkModel(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(NetworkModel, self).__init__()\n",
    "        self.network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(obs_dim,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,act_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.network(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Class for all RL algorithm\n",
    "class UAVComplexTrainerNN(UAVComplexTrainer): \n",
    "    def __init__(self, config):\n",
    "        super(UAVComplexTrainerNN, self).__init__(config)\n",
    "        self.max_episode_length = self.config[\"max_episode_length\"]\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.gamma = self.config[\"gamma\"]\n",
    "        self.model = NetworkModel((self.config[\"max_number_of_user\"] + 1) * 2, 2)\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def policy(self, state):\n",
    "        if np.random.uniform(0,1) <= self.config[\"eps\"]:\n",
    "            action = self.env.action_sample()\n",
    "        else:\n",
    "            feature = self.state_to_feature_vector(state[0], state[1])\n",
    "            model_input = to_tensor(feature).squeeze()\n",
    "            action = self.model(model_input)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAV position is: (0, 0)\n",
      "Users position are: [(980, 997), (467, 466), (450, 615), (181, 405), (284, 890), (621, 555), (414, 715), (977, 257), (398, 996), (167, 657)]\n",
      "Current Step: 49\n",
      "Policy choice direction: -2333.401123046875, speed: 3.400959014892578\n",
      "Current step reward: 0.02484581793656819, episodes rewards: 1.2174450788918412\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward is: 1.2422908968284094\n"
     ]
    }
   ],
   "source": [
    "linear_function_config = merge_config(dict(\n",
    "    total_steps = 50,\n",
    "    number_of_user = 10,\n",
    "    max_number_of_user = 20,\n",
    "    map=dict(\n",
    "        width=1000,\n",
    "        length=1000,\n",
    "        height=100\n",
    "    ),\n",
    "    max_episode_length=10000,\n",
    "    eps=0.01,\n",
    "    gamma=0.9,\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "), environment_config)\n",
    "\n",
    "NNTrainer = UAVComplexTrainerNN(linear_function_config)\n",
    "print(\"Mean Reward is: {}\".format(evaluate_complex(NNTrainer.policy, config = linear_function_config, num_episodes=1, render=True))) # Enable render=True can see the agent behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "s = np.random.normal(mu, sigma, (10,3))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
